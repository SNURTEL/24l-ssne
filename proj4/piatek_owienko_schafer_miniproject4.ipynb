{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SSNE Miniproject 4\n",
    "### 318703 Tomasz Owienko\n",
    "### 318718 Anna Schäfer\n",
    "### Grupa piątek"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e11a7bd6121478"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:11:28.257582928Z",
     "start_time": "2024-05-12T15:11:25.341457468Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Callable, Optional\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch import Tensor\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS, STEP_OUTPUT\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "data": {
      "text/plain": "123"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 123\n",
    "pl.seed_everything(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:11:28.265952667Z",
     "start_time": "2024-05-12T15:11:28.259035226Z"
    }
   },
   "id": "77b016a920782844",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.set_float32_matmul_precision('medium')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:11:28.311290361Z",
     "start_time": "2024-05-12T15:11:28.267939733Z"
    }
   },
   "id": "1d9a0540a7f6c891",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ImagesDataModule(pl.LightningDataModule):\n",
    "    class FastDataset(Dataset):\n",
    "        def __init__(self, data, labels, num_classes):\n",
    "            self.dataset = data\n",
    "            self.labels = labels\n",
    "            self.number_classes = num_classes\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.dataset[index], self.labels[index]\n",
    "\n",
    "    def __init__(self, path: str, transform: Callable[[Any], torch.Tensor], *, val_fraction: float,\n",
    "                 test_fraction: float, in_memory=False):\n",
    "        super().__init__()\n",
    "        assert 0 <= val_fraction + test_fraction <= 1\n",
    "        assert val_fraction * test_fraction >= 0\n",
    "\n",
    "        self.image_folder = ImageFolder(path, transform=transform)\n",
    "        self.dataset: ImagesDataModule.FastDataset | None = None\n",
    "        self._val_fraction = val_fraction\n",
    "        self._test_fraction = test_fraction\n",
    "        self._in_memory = in_memory\n",
    "\n",
    "        self._train = self._val = self._test = None\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        if self._in_memory:\n",
    "            loader = DataLoader(self.image_folder, batch_size=len(self.image_folder))\n",
    "            data = next(iter(loader))\n",
    "            dataset = ImagesDataModule.FastDataset(data[0], data[1], num_classes=len(self.image_folder.classes))\n",
    "        else:\n",
    "            dataset = self.image_folder\n",
    "\n",
    "        val_size = int(len(dataset) * self._val_fraction)\n",
    "        test_size = int(len(dataset) * self._test_fraction)\n",
    "        train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "        self._train, self._val, self._test = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self._train, batch_size=2048, shuffle=True, num_workers=12 if not self._in_memory else 0, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self._val, batch_size=1024, shuffle=False, num_workers=12 if not self._in_memory else 0, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self._test, batch_size=64, shuffle=False, num_workers=8 if not self._in_memory else 0, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:11:28.336170033Z",
     "start_time": "2024-05-12T15:11:28.286830058Z"
    }
   },
   "id": "7b994e620275aa56"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# region unet-original\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        self.down4 = (Down(512, 1024))\n",
    "        self.up1 = (Up(1024, 512))\n",
    "        self.up2 = (Up(512, 256))\n",
    "        self.up3 = (Up(256, 128))\n",
    "        self.up4 = (Up(128, 64))\n",
    "        self.outc = (OutConv(64, n_channels))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:11:28.336506461Z",
     "start_time": "2024-05-12T15:11:28.327516281Z"
    }
   },
   "id": "6912dc3083ed8d72"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# region pl-module\n",
    "\n",
    "class DiffusionModel(pl.LightningModule):\n",
    "    def __init__(self, unet: nn.Module, betas_min, betas_max, n_steps, *, device=device):\n",
    "        super().__init__()\n",
    "        self._device = device\n",
    "        self.unet = unet\n",
    "        self._frechet = FrechetInceptionDistance().to(device)\n",
    "        self._loss = torch.nn.MSELoss()\n",
    "        \n",
    "        self.n_steps = n_steps\n",
    "        \n",
    "        self.beta_t = ((betas_max - betas_min) * torch.arange(0, n_steps + 1, dtype=torch.float32) / n_steps + betas_min).to(device)\n",
    "        self.sqrt_beta_t = (torch.sqrt(self.beta_t)).to(device)\n",
    "        self.alpha_t = (1 - self.beta_t).to(device)\n",
    "        self.log_alpha_t = torch.log(self.alpha_t).to(device)\n",
    "        self.alphabar_t = torch.cumsum(self.log_alpha_t, dim=0).exp().to(device)\n",
    "    \n",
    "        self.sqrtab = torch.sqrt(self.alphabar_t).to(device)\n",
    "        self.oneover_sqrta = (1 / torch.sqrt(self.alpha_t)).to(device)\n",
    "    \n",
    "        self.sqrtmab = torch.sqrt(1 - self.alphabar_t).to(device)\n",
    "        self.mab_over_sqrtmab = ((1 - self.alpha_t) / self.sqrtmab).to(device)\n",
    "\n",
    "    def forward(self, x) -> Any:\n",
    "        timestep = torch.randint(1, self.n_steps, (x.shape[0],)).to(\n",
    "            x.device\n",
    "        )  # t ~ Uniform(0, n_T)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "\n",
    "        x_noisy = (\n",
    "            self.sqrtab[timestep, None, None, None] * x\n",
    "            + self.sqrtmab[timestep, None, None, None] * noise\n",
    "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
    "        \n",
    "        noise_pred = self.unet(x_noisy, timestep / self.n_steps)\n",
    "\n",
    "        return self._loss(noise, noise_pred)\n",
    "    \n",
    "    def generate(self, n_images: int, size) -> torch.Tensor:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(n_images, *size).to(self.device)  # x_T ~ N(0, 1)\n",
    "    \n",
    "            # This samples accordingly to Algorithm 2. It is exactly the same logic.\n",
    "            for i in range(self.n_steps, 0, -1):\n",
    "                z = torch.randn(n_images, *size).to(self.device) if i > 1 else 0\n",
    "                eps = self.unet(x, i / self.n_steps)\n",
    "                x = (\n",
    "                    self.oneover_sqrta[i] * (x - eps * self.mab_over_sqrtmab[i])\n",
    "                    + self.sqrt_beta_t[i] * z\n",
    "                )\n",
    "                \n",
    "            return x\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        \n",
    "        loss = self.forward(x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx) -> Optional[STEP_OUTPUT]:\n",
    "        xh = self.generate(64, (3, 32, 32))\n",
    "        grid = torchvision.utils.make_grid(xh, nrow=4)\n",
    "        torchvision.utils.save_image(grid, f\"./generated/sample_{self.current_epoch}.png\")\n",
    "        gridn = torchvision.utils.make_grid(xh, nrow=4, normalize=True, value_range=(-1, 1))\n",
    "        torchvision.utils.save_image(gridn, f\"./generated/sample_{self.current_epoch}_n.png\")\n",
    "        \n",
    "        # n_steps = 40\n",
    "        # images, _ = batch\n",
    "        # distance = self.calc_frechet(images=images, n_steps=n_steps)\n",
    "        # self.log(f\"val_frechet\", distance)\n",
    "        # return distance\n",
    "    \n",
    "    def calc_frechet(self, images, n_steps):\n",
    "        \"\"\"Expecting input to be normalized [-1, 1]\"\"\"\n",
    "        generated = self.generate(n_images=len(images)).to(self._device)\n",
    "        self._frechet.update(self._denormalize(images), real=True)\n",
    "        self._frechet.update(self._denormalize(generated), real=False)\n",
    "        return self._frechet.compute()\n",
    "    \n",
    "    # def generate(self, n_images, n_steps):\n",
    "    #     \"\"\"Denormalized, (N, 3, H, W) of torch.uint8\"\"\"\n",
    "    #     return self._denormalize(self._generate(n_images=n_images, n_steps=n_steps))\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def _denormalize(images):\n",
    "    #     \"\"\"Transform from [-1, 1] to [0, 255] of type torch.uint8\"\"\"\n",
    "    #     return images.add(1).mul(127.5).clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:11:28.336627772Z",
     "start_time": "2024-05-12T15:11:28.327883836Z"
    }
   },
   "id": "3ce18c0886b66520",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5,), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dm = ImagesDataModule('data/trafic_32', transform, val_fraction=0.005, test_fraction=0.)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:11:34.344085247Z",
     "start_time": "2024-05-12T15:11:28.328203158Z"
    }
   },
   "id": "54ab308b81a9eb30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                     | Params\n",
      "------------------------------------------------------\n",
      "0 | unet     | UNet                     | 31.0 M\n",
      "1 | _frechet | FrechetInceptionDistance | 23.9 M\n",
      "2 | _loss    | MSELoss                  | 0     \n",
      "------------------------------------------------------\n",
      "31.0 M    Trainable params\n",
      "23.9 M    Non-trainable params\n",
      "54.9 M    Total params\n",
      "219.555   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6272413ade0f4612821a8c869c9d6fcf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27f5702530a540e6aa8dc8920619a682"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15aca3f92616452ab366b7dea989e220"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eec235b7d1a349fba436bd235507f054"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fc62e32b32b448f9404dbf7b6fae1f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "939f04ac97264472b32e105514a4f6ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2376cf744bf54b3abe73171cff2f4050"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CHECKPOINT: str | None = 'lightning_logs/version_0/checkpoints/checkpoint-epoch=47-val_frechet=253.98.ckpt'\n",
    "CHECKPOINT: str | None = None\n",
    "\n",
    "if CHECKPOINT:\n",
    "    raise NotImplementedError(\"TODO rewrite this for the DDPM-like model\")\n",
    "    model = DiffusionModel.load_from_checkpoint(\n",
    "        CHECKPOINT,\n",
    "        unet = UNet(3)\n",
    "    ).to(device)\n",
    "else:\n",
    "    model = DiffusionModel(UNet(3), betas_min=1e-4, betas_max=0.02, n_steps=1000).to(device)\n",
    "\n",
    "# best_frechet_callback = ModelCheckpoint(\n",
    "#     save_top_k=3,\n",
    "#     monitor=\"val_frechet\",\n",
    "#     mode=\"min\",\n",
    "#     filename=\"checkpoint-{epoch:02d}-{train_loss:.2f}-{val_frechet:.2f}\",\n",
    "# )\n",
    "\n",
    "last_epoch_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"epoch\",\n",
    "    mode=\"max\",\n",
    "    filename=\"checkpoint_last-{epoch:04d}-{train_loss:.5f}\",\n",
    "    # filename=\"checkpoint_last-{epoch:02d}-{train_loss:.2f}-{val_frechet:.2f}\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200, \n",
    "    callbacks=[\n",
    "        # best_frechet_callback,\n",
    "        last_epoch_callback\n",
    "    ], \n",
    "    log_every_n_steps=16,  # Dataset size / n batches with batch size 2048\n",
    "    check_val_every_n_epoch=1\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=dm, ckpt_path=CHECKPOINT)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-12T15:11:34.347494362Z"
    }
   },
   "id": "b7167a6f534aae75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NOTE: for some reason (spent 3h figuring out why), if training was resumed from a checkpoint, \n",
    "# the model has to be re-loaded before evaluation - in other case, results are essentially garbage\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "LOGS_PATH = Path(\"lightning_logs\")\n",
    "CHECKPOINTS_PATH = max(LOGS_PATH.glob(\"./*\"), key=lambda x: int(x.name.split(\"_\")[-1])) / \"checkpoints\"\n",
    "print(f\"Using checkpoints from {CHECKPOINTS_PATH} ({len([f for f in CHECKPOINTS_PATH.glob('./*')])} checkpoints)\")\n",
    "\n",
    "# best_frechet_checkpoint = min((f for f in CHECKPOINTS_PATH.glob('./*') if not f.stem.startswith(\"checkpoint_last\")),  key=lambda x: float(x.stem.split('=')[-1]))\n",
    "last_epoch_checkpoint = max((f for f in CHECKPOINTS_PATH.glob('./*') if f.stem.startswith(\"checkpoint_last\")), key=lambda x: int(x.stem.split('-')[1].split('=')[-1]))\n",
    "# print(f\"Best Frechet: {best_frechet_checkpoint}\")\n",
    "print(f\"Last epoch: {last_epoch_checkpoint}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1d42ad40346f3ce5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "\n",
    "for path, name, ax in zip(\n",
    "        (\n",
    "                # best_frechet_checkpoint, \n",
    "                last_epoch_checkpoint,\n",
    "        ),\n",
    "        (\n",
    "                # \"Best Frechet\", \n",
    "                \"Last epoch\",\n",
    "        ),\n",
    "    [axes]\n",
    "):\n",
    "    m = DiffusionModel.load_from_checkpoint(path, unet=UNet(3), betas_min=1e-4, betas_max=0.02, n_steps=1000).to(device)\n",
    "    \n",
    "    preds = m.generate(64, (3, 32, 32)).cpu()\n",
    "    ax.set_title(f\"{name}\\n{path}\")\n",
    "    ax.imshow((torchvision.utils.make_grid(preds, normalize=True, value_range=(-1, 1)).permute(1, 2, 0)))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c014ccb88bb4c65e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test arbitrary checkpoints here\n",
    "\n",
    "# ckpt: str | None =  \"saved_runs/broken_ddpm/checkpoints/checkpoint_last-epoch=149-train_loss=0.02.ckpt\"\n",
    "# \n",
    "# def _denormalize(images):\n",
    "#     normalized = images.clone()\n",
    "#     for i in range(len(normalized)):\n",
    "#         normalized[i] -= torch.min(normalized[i])\n",
    "#         normalized[i] *= 255 / torch.max(normalized[i])\n",
    "#     return normalized.to(torch.uint8)\n",
    "# \n",
    "# if ckpt:\n",
    "#     m1 = DiffusionModel.load_from_checkpoint(ckpt, unet=UNet(3, n_steps=1000, time_emb_dim=100), betas_min=1e-4, betas_max=0.02, n_steps=1000)\n",
    "#     preds = m1._generate1(8).cpu()\n",
    "#     print(preds.mean(dim=(2,3)))\n",
    "#     preds = _denormalize(preds)\n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "#     ax.imshow((torchvision.utils.make_grid(preds).permute(1, 2, 0)))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6a34a1115d62fb67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ff528cb8fd191723",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5250e032a5124387"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
