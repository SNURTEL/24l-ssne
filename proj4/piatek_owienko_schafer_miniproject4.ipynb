{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SSNE Miniproject 4\n",
    "### 318703 Tomasz Owienko\n",
    "### 318718 Anna Schäfer\n",
    "### Grupa piątek"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e11a7bd6121478"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-11T20:56:34.181330504Z",
     "start_time": "2024-05-11T20:56:31.870646854Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Callable, Optional\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch import Tensor\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS, STEP_OUTPUT\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "data": {
      "text/plain": "123"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 123\n",
    "pl.seed_everything(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T20:56:34.187250567Z",
     "start_time": "2024-05-11T20:56:34.180412764Z"
    }
   },
   "id": "77b016a920782844",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.set_float32_matmul_precision('medium')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T20:56:34.187705060Z",
     "start_time": "2024-05-11T20:56:34.181082197Z"
    }
   },
   "id": "1d9a0540a7f6c891",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ImagesDataModule(pl.LightningDataModule):\n",
    "    class FastDataset(Dataset):\n",
    "        def __init__(self, data, labels, num_classes):\n",
    "            self.dataset = data\n",
    "            self.labels = labels\n",
    "            self.number_classes = num_classes\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.dataset[index], self.labels[index]\n",
    "\n",
    "    def __init__(self, path: str, transform: Callable[[Any], torch.Tensor], *, val_fraction: float,\n",
    "                 test_fraction: float, in_memory=False):\n",
    "        super().__init__()\n",
    "        assert 0 <= val_fraction + test_fraction <= 1\n",
    "        assert val_fraction * test_fraction >= 0\n",
    "\n",
    "        self.image_folder = ImageFolder(path, transform=transform)\n",
    "        self.dataset: ImagesDataModule.FastDataset | None = None\n",
    "        self._val_fraction = val_fraction\n",
    "        self._test_fraction = test_fraction\n",
    "        self._in_memory = in_memory\n",
    "\n",
    "        self._train = self._val = self._test = None\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        if self._in_memory:\n",
    "            loader = DataLoader(self.image_folder, batch_size=len(self.image_folder))\n",
    "            data = next(iter(loader))\n",
    "            dataset = ImagesDataModule.FastDataset(data[0], data[1], num_classes=len(self.image_folder.classes))\n",
    "        else:\n",
    "            dataset = self.image_folder\n",
    "\n",
    "        val_size = int(len(dataset) * self._val_fraction)\n",
    "        test_size = int(len(dataset) * self._test_fraction)\n",
    "        train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "        self._train, self._val, self._test = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self._train, batch_size=2048, shuffle=True, num_workers=12 if not self._in_memory else 0, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self._val, batch_size=1024, shuffle=False, num_workers=12 if not self._in_memory else 0, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self._test, batch_size=64, shuffle=False, num_workers=8 if not self._in_memory else 0, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T20:56:34.189010830Z",
     "start_time": "2024-05-11T20:56:34.181575101Z"
    }
   },
   "id": "7b994e620275aa56"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# region unet-original\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "def sinusoidal_embedding(n, d):\n",
    "    # Returns the standard positional embedding\n",
    "    embedding = torch.zeros(n, d)\n",
    "    wk = torch.tensor([1 / 10_000 ** (2 * j / d) for j in range(d)])\n",
    "    wk = wk.reshape((1, d))\n",
    "    t = torch.arange(n).reshape((n, 1))\n",
    "    embedding[:,::2] = torch.sin(t * wk[:,::2])\n",
    "    embedding[:,1::2] = torch.cos(t * wk[:,::2])\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_steps, time_emb_dim, bilinear=False,):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "        # Sinusoidal embedding\n",
    "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
    "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
    "        self.time_embed.requires_grad_(False)\n",
    "\n",
    "        self.te0d = self._make_te(time_emb_dim, n_channels)\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.te1d = self._make_te(time_emb_dim, 64)\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.te2d = self._make_te(time_emb_dim, 128)\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.te3d = self._make_te(time_emb_dim, 256)\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.te4d = self._make_te(time_emb_dim, 512)\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.te1u = self._make_te(time_emb_dim, 1024)\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.te2u = self._make_te(time_emb_dim, 512)\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.te3u = self._make_te(time_emb_dim, 256)\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.te4u = self._make_te(time_emb_dim, 128)\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_channels))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        n = len(x)\n",
    "        t = self.time_embed(t)\n",
    "        x1 = self.inc(x + self.te0d(t).reshape(n, -1, 1, 1))\n",
    "        x2 = self.down1(x1 + self.te1d(t).reshape(n, -1, 1, 1))\n",
    "        x3 = self.down2(x2 + self.te2d(t).reshape(n, -1, 1, 1))\n",
    "        x4 = self.down3(x3 + self.te3d(t).reshape(n, -1, 1, 1))\n",
    "        x5 = self.down4(x4 + self.te4d(t).reshape(n, -1, 1, 1))\n",
    "        x = self.up1(x5 + self.te1u(t).reshape(n, -1, 1, 1), x4)\n",
    "        x = self.up2(x + self.te2u(t).reshape(n, -1, 1, 1), x3)\n",
    "        x = self.up3(x + self.te3u(t).reshape(n, -1, 1, 1), x2)\n",
    "        x = self.up4(x + self.te4u(t).reshape(n, -1, 1, 1), x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def _make_te(self, dim_in, dim_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(dim_in, dim_out),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim_out, dim_out)\n",
    "        )\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T20:56:34.189453749Z",
     "start_time": "2024-05-11T20:56:34.181908918Z"
    }
   },
   "id": "6912dc3083ed8d72"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# region pl-module\n",
    "\n",
    "class DiffusionModel(pl.LightningModule):\n",
    "    def __init__(self, unet: nn.Module, betas_min, betas_max, n_steps, *, device=device):\n",
    "        super().__init__()\n",
    "        self._device = device\n",
    "        self.unet = unet\n",
    "        self.betas = torch.linspace(betas_min, betas_max, n_steps).to(device)\n",
    "        self.n_steps = n_steps\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) for i in range(len(self.alphas))]).to(device)  # TODO cumprod\n",
    "        self._frechet = FrechetInceptionDistance().to(device)\n",
    "        \n",
    "    def corrupt(self, x, t, noise=None):\n",
    "        n_samples, _, _, _ = x.shape\n",
    "        a_bar = self.alpha_bars[t]\n",
    "        noise = noise if noise is not None else torch.randn(*x.shape).to(self.device)\n",
    "        corrupted = a_bar.sqrt() * x + (1 - a_bar).sqrt() * noise\n",
    "        return corrupted\n",
    "    \n",
    "    def forward(self, x, t, noise=None) -> Any:\n",
    "        return self.corrupt(x, t, noise)\n",
    "    \n",
    "    def backward1(self, x, t):\n",
    "        return self.unet(x, t)\n",
    "    \n",
    "    def generate1(self, n_images):\n",
    "        return self._denormalize1(self._generate1(n_images))\n",
    "    \n",
    "    def _generate1(self, n_images):\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(n_images, 3, 32, 32).to(device)\n",
    "            for t in reversed(range(self.n_steps)):\n",
    "                time_tensor = (torch.ones(n_images, 1) * t).to(device).long()\n",
    "                eta_theta = self.backward1(x, time_tensor)\n",
    "                \n",
    "                alpha_t = self.alphas[t]\n",
    "                alpha_t_bar = self.alpha_bars[t]\n",
    "                x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)\n",
    "                \n",
    "                if t > 0:\n",
    "                    z = torch.randn(n_images, 3, 32, 32).to(device)\n",
    "    \n",
    "                    # Option 1: sigma_t squared = beta_t\n",
    "                    beta_t = self.betas[t]\n",
    "                    sigma_t = beta_t.sqrt()\n",
    "    \n",
    "                    # Option 2: sigma_t squared = beta_tilda_t\n",
    "                    # prev_alpha_t_bar = ddpm.alpha_bars[t-1] if t > 0 else ddpm.alphas[0]\n",
    "                    # beta_tilda_t = ((1 - prev_alpha_t_bar)/(1 - alpha_t_bar)) * beta_t\n",
    "                    # sigma_t = beta_tilda_t.sqrt()\n",
    "    \n",
    "                    # Adding some more noise like in Langevin Dynamics fashion\n",
    "                    x = x + sigma_t * z\n",
    "                    \n",
    "            return x\n",
    "            \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        n = len(x)\n",
    "        \n",
    "        noise = torch.randn_like(x).to(device)\n",
    "        t = torch.randint(0, self.n_steps, (n,)).to(device)\n",
    "        noisy_x = self(x, t, noise)\n",
    "        eta_theta = self.backward1(noisy_x, t.reshape(n, -1))\n",
    "        loss = F.mse_loss(eta_theta, noise)\n",
    "        \n",
    "        # noise_amount = torch.rand(x.shape[0]).to(device)\n",
    "        # noisy_x = self.corrupt(x, noise_amount)\n",
    "        # x_hat = self(noisy_x)\n",
    "        # loss = F.mse_loss(x_hat, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "    \n",
    "    # def validation_step(self, batch, batch_idx) -> Optional[STEP_OUTPUT]:\n",
    "    #     n_steps = 40\n",
    "    #     images, _ = batch\n",
    "    #     distance = self.calc_frechet(images=images, n_steps=n_steps)\n",
    "    #     self.log(f\"val_frechet\", distance)\n",
    "    #     return distance\n",
    "    \n",
    "    def calc_frechet(self, images, n_steps):\n",
    "        \"\"\"Expecting input to be normalized [-1, 1]\"\"\"\n",
    "        generated = self.generate1(n_images=len(images)).to(self._device)\n",
    "        self._frechet.update(self._denormalize(images), real=True)\n",
    "        self._frechet.update(self._denormalize(generated), real=False)\n",
    "        return self._frechet.compute()\n",
    "    \n",
    "    def generate(self, n_images, n_steps):\n",
    "        \"\"\"Denormalized, (N, 3, H, W) of torch.uint8\"\"\"\n",
    "        return self._denormalize(self._generate(n_images=n_images, n_steps=n_steps))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _denormalize(images):\n",
    "        \"\"\"Transform from [-1, 1] to [0, 255] of type torch.uint8\"\"\"\n",
    "        return images.add(1).mul(127.5).clamp(0, 255).to(torch.uint8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _denormalize1(images):\n",
    "        normalized = images.clone()\n",
    "        for i in range(len(normalized)):\n",
    "            normalized[i] -= torch.min(normalized[i])\n",
    "            normalized[i] *= 255 / torch.max(normalized[i])\n",
    "        return normalized.to(torch.uint8)\n",
    "        \n",
    "    \n",
    "    def _generate(self, n_images, n_steps):        \n",
    "        \"\"\"Normalized, (N, 3, H, W) of torch.float32\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = 2 * (torch.rand(n_images, 3, 32, 32) - 1).to(self._device)\n",
    "            for i in range(n_steps):\n",
    "                pred = self(x)\n",
    "                mix_factor = 1 / (n_steps - i)\n",
    "                x = x * (1 - mix_factor) + pred * mix_factor\n",
    "            return x\n",
    "\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T20:56:34.199286351Z",
     "start_time": "2024-05-11T20:56:34.182517594Z"
    }
   },
   "id": "3ce18c0886b66520",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5,), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dm = ImagesDataModule('data/trafic_32', transform, val_fraction=0.005, test_fraction=0.)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T20:56:34.645685042Z",
     "start_time": "2024-05-11T20:56:34.198407376Z"
    }
   },
   "id": "54ab308b81a9eb30"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                     | Params\n",
      "------------------------------------------------------\n",
      "0 | unet     | UNet                     | 33.2 M\n",
      "1 | _frechet | FrechetInceptionDistance | 23.9 M\n",
      "------------------------------------------------------\n",
      "33.1 M    Trainable params\n",
      "24.0 M    Non-trainable params\n",
      "57.0 M    Total params\n",
      "228.094   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60522c7f2a6c43c7937327c4169fea2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# CHECKPOINT: str | None = 'lightning_logs/version_0/checkpoints/checkpoint-epoch=47-val_frechet=253.98.ckpt'\n",
    "CHECKPOINT: str | None = None\n",
    "\n",
    "if CHECKPOINT:\n",
    "    raise NotImplementedError(\"TODO rewrite this for the DDPM-like model\")\n",
    "    model = DiffusionModel.load_from_checkpoint(\n",
    "        CHECKPOINT,\n",
    "        unet = UNet(3)\n",
    "    ).to(device)\n",
    "else:\n",
    "    model = DiffusionModel(UNet(3, n_steps=1000, time_emb_dim=100), betas_min=1e-4, betas_max=0.02, n_steps=1000).to(device)\n",
    "\n",
    "# best_frechet_callback = ModelCheckpoint(\n",
    "#     save_top_k=3,\n",
    "#     monitor=\"val_frechet\",\n",
    "#     mode=\"min\",\n",
    "#     filename=\"checkpoint-{epoch:02d}-{train_loss:.2f}-{val_frechet:.2f}\",\n",
    "# )\n",
    "\n",
    "last_epoch_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"epoch\",\n",
    "    mode=\"max\",\n",
    "    filename=\"checkpoint_last-{epoch:02d}-{train_loss:.2f}\",\n",
    "    # filename=\"checkpoint_last-{epoch:02d}-{train_loss:.2f}-{val_frechet:.2f}\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2, \n",
    "    callbacks=[\n",
    "        # best_frechet_callback,\n",
    "        last_epoch_callback\n",
    "    ], \n",
    "    log_every_n_steps=16,  # Dataset size / n batches with batch size 2048\n",
    "    # check_val_every_n_epoch=8\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=dm, ckpt_path=CHECKPOINT)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T18:38:05.618552151Z",
     "start_time": "2024-05-11T18:32:52.801874783Z"
    }
   },
   "id": "b7167a6f534aae75"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoints from lightning_logs/version_1/checkpoints (1 checkpoints)\n",
      "Last epoch: lightning_logs/version_1/checkpoints/checkpoint_last-epoch=149-train_loss=0.03.ckpt\n"
     ]
    }
   ],
   "source": [
    "# NOTE: for some reason (spent 3h figuring out why), if training was resumed from a checkpoint, \n",
    "# the model has to be re-loaded before evaluation - in other case, results are essentially garbage\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "LOGS_PATH = Path(\"lightning_logs\")\n",
    "CHECKPOINTS_PATH = max(LOGS_PATH.glob(\"./*\"), key=lambda x: int(x.name.split(\"_\")[-1])) / \"checkpoints\"\n",
    "print(f\"Using checkpoints from {CHECKPOINTS_PATH} ({len([f for f in CHECKPOINTS_PATH.glob('./*')])} checkpoints)\")\n",
    "\n",
    "# best_frechet_checkpoint = min((f for f in CHECKPOINTS_PATH.glob('./*') if not f.stem.startswith(\"checkpoint_last\")),  key=lambda x: float(x.stem.split('=')[-1]))\n",
    "last_epoch_checkpoint = max((f for f in CHECKPOINTS_PATH.glob('./*') if f.stem.startswith(\"checkpoint_last\")), key=lambda x: int(x.stem.split('-')[1].split('=')[-1]))\n",
    "# print(f\"Best Frechet: {best_frechet_checkpoint}\")\n",
    "print(f\"Last epoch: {last_epoch_checkpoint}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T20:56:39.671868595Z",
     "start_time": "2024-05-11T20:56:39.600634610Z"
    }
   },
   "id": "1d42ad40346f3ce5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DiffusionModel:\n\tsize mismatch for unet.time_embed.weight: copying a param with shape torch.Size([200, 100]) from checkpoint, the shape in current model is torch.Size([1000, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 16\u001B[0m\n\u001B[1;32m      3\u001B[0m fig, axes \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m path, name, ax \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[1;32m      6\u001B[0m         (\n\u001B[1;32m      7\u001B[0m                 \u001B[38;5;66;03m# best_frechet_checkpoint, \u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m     axes\n\u001B[1;32m     15\u001B[0m ):\n\u001B[0;32m---> 16\u001B[0m     m \u001B[38;5;241m=\u001B[39m DiffusionModel\u001B[38;5;241m.\u001B[39mload_from_checkpoint(path, unet\u001B[38;5;241m=\u001B[39mUNet(\u001B[38;5;241m3\u001B[39m, n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, time_emb_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m), betas_min\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m, betas_max\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.02\u001B[39m, n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     18\u001B[0m     preds \u001B[38;5;241m=\u001B[39m m\u001B[38;5;241m.\u001B[39mgenerate1(\u001B[38;5;241m16\u001B[39m)\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[1;32m     19\u001B[0m     ax\u001B[38;5;241m.\u001B[39mset_title(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1520\u001B[0m, in \u001B[0;36mLightningModule.load_from_checkpoint\u001B[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001B[0m\n\u001B[1;32m   1440\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m   1441\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_from_checkpoint\u001B[39m(\n\u001B[1;32m   1442\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1447\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m   1448\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Self:\n\u001B[1;32m   1449\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1450\u001B[0m \u001B[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001B[39;00m\n\u001B[1;32m   1451\u001B[0m \u001B[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1518\u001B[0m \u001B[38;5;124;03m        y_hat = pretrained_model(x)\u001B[39;00m\n\u001B[1;32m   1519\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1520\u001B[0m     loaded \u001B[38;5;241m=\u001B[39m _load_from_checkpoint(\n\u001B[1;32m   1521\u001B[0m         \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   1522\u001B[0m         checkpoint_path,\n\u001B[1;32m   1523\u001B[0m         map_location,\n\u001B[1;32m   1524\u001B[0m         hparams_file,\n\u001B[1;32m   1525\u001B[0m         strict,\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   1527\u001B[0m     )\n\u001B[1;32m   1528\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Self, loaded)\n",
      "File \u001B[0;32m~/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:89\u001B[0m, in \u001B[0;36m_load_from_checkpoint\u001B[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001B[0m\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _load_state(\u001B[38;5;28mcls\u001B[39m, checkpoint, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\u001B[38;5;28mcls\u001B[39m, pl\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[0;32m---> 89\u001B[0m     storage \u001B[38;5;241m=\u001B[39m _load_state(\u001B[38;5;28mcls\u001B[39m, checkpoint, strict\u001B[38;5;241m=\u001B[39mstrict, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     90\u001B[0m     state_dict \u001B[38;5;241m=\u001B[39m checkpoint[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m state_dict:\n",
      "File \u001B[0;32m~/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:154\u001B[0m, in \u001B[0;36m_load_state\u001B[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;66;03m# load the state_dict on the model automatically\u001B[39;00m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m strict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m keys \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mload_state_dict(checkpoint[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m], strict\u001B[38;5;241m=\u001B[39mstrict)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m strict:\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m keys\u001B[38;5;241m.\u001B[39mmissing_keys:\n",
      "File \u001B[0;32m~/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2148\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2149\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2150\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   2152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2153\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2154\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   2155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for DiffusionModel:\n\tsize mismatch for unet.time_embed.weight: copying a param with shape torch.Size([200, 100]) from checkpoint, the shape in current model is torch.Size([1000, 100])."
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdM0lEQVR4nO3df2zXhZ348VdpaavetYswKwoy2OlkI3OjBEZJz5ynXdC48MdFFi+Cnkuu2XaInN5gXGQYk2ZbZjK3wX6BZgl6REXPPzhn/9gUxWwnV5ZlkLgIt8JWJMXYos4y4P39wy+91Bbl8+HTHy99PJLPH337ftMXG+9Xnp/PBz6tKoqiCACABCaN9wAAAGdLuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGmUHC7PPfdc3HjjjXHJJZdEVVVVPPnkk+97zbPPPhvNzc1RX18fs2fPjh/+8IflzAokZW8AlVJyuLz55ptx1VVXxfe///2zOv/AgQNx/fXXR2tra3R1dcXXv/71WLlyZTz++OMlDwvkZG8AlVJ1Lj9ksaqqKp544olYunTpGc/52te+Fk899VTs27dv8Fh7e3v85je/iRdffLHcbw0kZW8A56JmtL/Biy++GG1tbUOOff7zn4/NmzfHX/7yl5g8efKwawYGBmJgYGDw61OnTsVrr70WU6ZMiaqqqtEeGXiXoiji2LFjcckll8SkSaP/V+PsDfhgGI3dMerhcvjw4WhqahpyrKmpKU6cOBG9vb0xbdq0Ydd0dHTEhg0bRns0oEQHDx6M6dOnj/r3sTfgg6WSu2PUwyUihj3bOf3u1JmeBa1duzZWr149+HVfX19cdtllcfDgwWhoaBi9QYER9ff3x4wZM+Kv//qvx+x72huQ32jsjlEPl4svvjgOHz485NiRI0eipqYmpkyZMuI1dXV1UVdXN+x4Q0ODBQTjaKzecrE34IOlkrtj1N+sXrRoUXR2dg459swzz8T8+fNHfJ8awN4AzqTkcHnjjTdiz549sWfPnoh4558t7tmzJ7q7uyPinZdrly9fPnh+e3t7/OEPf4jVq1fHvn37YsuWLbF58+a46667KvM7ACY8ewOomKJEv/jFL4qIGPZYsWJFURRFsWLFiuLqq68ecs0vf/nL4rOf/WxRW1tbfOxjHys2bdpU0vfs6+srIqLo6+srdVygAs71HrQ34MNpNO7Dc/ocl7HS398fjY2N0dfX571qGAcZ78GMM8MHzWjch35WEQCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKRRVrhs3LgxZs2aFfX19dHc3Bw7d+58z/O3bt0aV111VZx//vkxbdq0uO222+Lo0aNlDQzkZG8AlVByuGzbti1WrVoV69ati66urmhtbY0lS5ZEd3f3iOc///zzsXz58rj99tvjd7/7XTz66KPx3//93/GlL33pnIcHcrA3gIopSrRgwYKivb19yLErr7yyWLNmzYjnf/vb3y5mz5495NgDDzxQTJ8+/ay/Z19fXxERRV9fX6njAhVwrvegvQEfTqNxH5b0isvx48dj9+7d0dbWNuR4W1tb7Nq1a8RrWlpa4tChQ7Fjx44oiiJeffXVeOyxx+KGG2444/cZGBiI/v7+IQ8gJ3sDqKSSwqW3tzdOnjwZTU1NQ443NTXF4cOHR7ympaUltm7dGsuWLYva2tq4+OKL4yMf+Uh873vfO+P36ejoiMbGxsHHjBkzShkTmEDsDaCSyvrLuVVVVUO+Lopi2LHT9u7dGytXrox77rkndu/eHU8//XQcOHAg2tvbz/jrr127Nvr6+gYfBw8eLGdMYAKxN4BKqCnl5KlTp0Z1dfWwZ0lHjhwZ9mzqtI6Ojli8eHHcfffdERHx6U9/Oi644IJobW2N++67L6ZNmzbsmrq6uqirqytlNGCCsjeASirpFZfa2tpobm6Ozs7OIcc7OzujpaVlxGveeuutmDRp6Leprq6OiHeecQEfbPYGUEklv1W0evXq+OlPfxpbtmyJffv2xZ133hnd3d2DL+GuXbs2li9fPnj+jTfeGNu3b49NmzbF/v3744UXXoiVK1fGggUL4pJLLqnc7wSYsOwNoFJKeqsoImLZsmVx9OjRuPfee6Onpyfmzp0bO3bsiJkzZ0ZERE9Pz5DPZrj11lvj2LFj8f3vfz/+9V//NT7ykY/ENddcE9/85jcr97sAJjR7A6iUqiLB6679/f3R2NgYfX190dDQMN7jwIdOxnsw48zwQTMa96GfVQQApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABplBUuGzdujFmzZkV9fX00NzfHzp073/P8gYGBWLduXcycOTPq6uri4x//eGzZsqWsgYGc7A2gEmpKvWDbtm2xatWq2LhxYyxevDh+9KMfxZIlS2Lv3r1x2WWXjXjNTTfdFK+++mps3rw5/uZv/iaOHDkSJ06cOOfhgRzsDaBSqoqiKEq5YOHChTFv3rzYtGnT4LE5c+bE0qVLo6OjY9j5Tz/9dHzxi1+M/fv3x4UXXljWkP39/dHY2Bh9fX3R0NBQ1q8BlO9c70F7Az6cRuM+LOmtouPHj8fu3bujra1tyPG2trbYtWvXiNc89dRTMX/+/PjWt74Vl156aVxxxRVx1113xZ///Oczfp+BgYHo7+8f8gBysjeASirpraLe3t44efJkNDU1DTne1NQUhw8fHvGa/fv3x/PPPx/19fXxxBNPRG9vb3z5y1+O11577YzvV3d0dMSGDRtKGQ2YoOwNoJLK+su5VVVVQ74uimLYsdNOnToVVVVVsXXr1liwYEFcf/31cf/998dDDz10xmdPa9eujb6+vsHHwYMHyxkTmEDsDaASSnrFZerUqVFdXT3sWdKRI0eGPZs6bdq0aXHppZdGY2Pj4LE5c+ZEURRx6NChuPzyy4ddU1dXF3V1daWMBkxQ9gZQSSW94lJbWxvNzc3R2dk55HhnZ2e0tLSMeM3ixYvjT3/6U7zxxhuDx15++eWYNGlSTJ8+vYyRgUzsDaCSSn6raPXq1fHTn/40tmzZEvv27Ys777wzuru7o729PSLeebl2+fLlg+fffPPNMWXKlLjtttti79698dxzz8Xdd98d//RP/xTnnXde5X4nwIRlbwCVUvLnuCxbtiyOHj0a9957b/T09MTcuXNjx44dMXPmzIiI6Onpie7u7sHz/+qv/io6OzvjX/7lX2L+/PkxZcqUuOmmm+K+++6r3O8CmNDsDaBSSv4cl/Hg8xhgfGW8BzPODB804/45LgAA40m4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSKCtcNm7cGLNmzYr6+vpobm6OnTt3ntV1L7zwQtTU1MRnPvOZcr4tkJi9AVRCyeGybdu2WLVqVaxbty66urqitbU1lixZEt3d3e95XV9fXyxfvjz+/u//vuxhgZzsDaBSqoqiKEq5YOHChTFv3rzYtGnT4LE5c+bE0qVLo6Oj44zXffGLX4zLL788qqur48knn4w9e/ac8dyBgYEYGBgY/Lq/vz9mzJgRfX190dDQUMq4QAX09/dHY2Nj2fegvQEfTue6O0ZS0isux48fj927d0dbW9uQ421tbbFr164zXvfggw/GK6+8EuvXrz+r79PR0RGNjY2DjxkzZpQyJjCB2BtAJZUULr29vXHy5MloamoacrypqSkOHz484jW///3vY82aNbF169aoqak5q++zdu3a6OvrG3wcPHiwlDGBCcTeACrp7DbCu1RVVQ35uiiKYcciIk6ePBk333xzbNiwIa644oqz/vXr6uqirq6unNGACcreACqhpHCZOnVqVFdXD3uWdOTIkWHPpiIijh07Fi+99FJ0dXXFV7/61YiIOHXqVBRFETU1NfHMM8/ENddccw7jAxOdvQFUUklvFdXW1kZzc3N0dnYOOd7Z2RktLS3Dzm9oaIjf/va3sWfPnsFHe3t7fOITn4g9e/bEwoULz216YMKzN4BKKvmtotWrV8ctt9wS8+fPj0WLFsWPf/zj6O7ujvb29oh4533mP/7xj/Gzn/0sJk2aFHPnzh1y/UUXXRT19fXDjgMfXPYGUCklh8uyZcvi6NGjce+990ZPT0/MnTs3duzYETNnzoyIiJ6envf9bAbgw8XeACql5M9xGQ+j8e/AgbOX8R7MODN80Iz757gAAIwn4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASKOscNm4cWPMmjUr6uvro7m5OXbu3HnGc7dv3x7XXXddfPSjH42GhoZYtGhR/PznPy97YCAnewOohJLDZdu2bbFq1apYt25ddHV1RWtrayxZsiS6u7tHPP+5556L6667Lnbs2BG7d++Ov/u7v4sbb7wxurq6znl4IAd7A6iUqqIoilIuWLhwYcybNy82bdo0eGzOnDmxdOnS6OjoOKtf41Of+lQsW7Ys7rnnnhH/+8DAQAwMDAx+3d/fHzNmzIi+vr5oaGgoZVygAvr7+6OxsbHse9DegA+nc90dIynpFZfjx4/H7t27o62tbcjxtra22LVr11n9GqdOnYpjx47FhRdeeMZzOjo6orGxcfAxY8aMUsYEJhB7A6ikksKlt7c3Tp48GU1NTUOONzU1xeHDh8/q1/jOd74Tb775Ztx0001nPGft2rXR19c3+Dh48GApYwITiL0BVFJNORdVVVUN+booimHHRvLII4/EN77xjfjP//zPuOiii854Xl1dXdTV1ZUzGjBB2RtAJZQULlOnTo3q6uphz5KOHDky7NnUu23bti1uv/32ePTRR+Paa68tfVIgJXsDqKSS3iqqra2N5ubm6OzsHHK8s7MzWlpaznjdI488Erfeems8/PDDccMNN5Q3KZCSvQFUUslvFa1evTpuueWWmD9/fixatCh+/OMfR3d3d7S3t0fEO+8z//GPf4yf/exnEfHO8lm+fHl897vfjc997nODz7rOO++8aGxsrOBvBZio7A2gUkoOl2XLlsXRo0fj3nvvjZ6enpg7d27s2LEjZs6cGRERPT09Qz6b4Uc/+lGcOHEivvKVr8RXvvKVweMrVqyIhx566Nx/B8CEZ28AlVLy57iMh9H4d+DA2ct4D2acGT5oxv1zXAAAxpNwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGsIFAEhDuAAAaQgXACAN4QIApCFcAIA0hAsAkIZwAQDSEC4AQBrCBQBIQ7gAAGkIFwAgDeECAKQhXACANIQLAJCGcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkUVa4bNy4MWbNmhX19fXR3NwcO3fufM/zn3322Whubo76+vqYPXt2/PCHPyxrWCAvewOohJLDZdu2bbFq1apYt25ddHV1RWtrayxZsiS6u7tHPP/AgQNx/fXXR2tra3R1dcXXv/71WLlyZTz++OPnPDyQg70BVEpVURRFKRcsXLgw5s2bF5s2bRo8NmfOnFi6dGl0dHQMO/9rX/taPPXUU7Fv377BY+3t7fGb3/wmXnzxxRG/x8DAQAwMDAx+3dfXF5dddlkcPHgwGhoaShkXqID+/v6YMWNGvP7669HY2Fjy9fYGfDid6+4YUVGCgYGBorq6uti+ffuQ4ytXriz+9m//dsRrWltbi5UrVw45tn379qKmpqY4fvz4iNesX7++iAgPD48J9njllVdKWRn2hoeHRxFR3u44k5ooQW9vb5w8eTKampqGHG9qaorDhw+PeM3hw4dHPP/EiRPR29sb06ZNG3bN2rVrY/Xq1YNfv/766zFz5szo7u6uXLGNstOVmenZnpnHRsaZT796ceGFF5Z8rb1x9jL+2YjIObeZx8a57I4zKSlcTquqqhrydVEUw4693/kjHT+trq4u6urqhh1vbGxM83/WaQ0NDWYeA2YeG5Mmlf8PEe2Ns5fxz0ZEzrnNPDbOZXcM+7VKOXnq1KlRXV097FnSkSNHhj07Ou3iiy8e8fyampqYMmVKieMC2dgbQCWVFC61tbXR3NwcnZ2dQ453dnZGS0vLiNcsWrRo2PnPPPNMzJ8/PyZPnlziuEA29gZQUaX+pZj/+I//KCZPnlxs3ry52Lt3b7Fq1ariggsuKP73f/+3KIqiWLNmTXHLLbcMnr9///7i/PPPL+68885i7969xebNm4vJkycXjz322Fl/z7fffrtYv3598fbbb5c67rgx89gw89g415ntjbOTceaiyDm3mcfGaMxccrgURVH84Ac/KGbOnFnU1tYW8+bNK5599tnB/7ZixYri6quvHnL+L3/5y+Kzn/1sUVtbW3zsYx8rNm3adE5DA/nYG0AllPw5LgAA48XPKgIA0hAuAEAawgUASEO4AABpTJhwyfgj70uZefv27XHdddfFRz/60WhoaIhFixbFz3/+8zGc9h2l/u982gsvvBA1NTXxmc98ZnQHHEGpMw8MDMS6deti5syZUVdXFx//+Mdjy5YtYzTtO0qdeevWrXHVVVfF+eefH9OmTYvbbrstjh49OkbTRjz33HNx4403xiWXXBJVVVXx5JNPvu812e7BiHwz2xvly7g3InLtjnHbG+P9z5qK4v8+4+EnP/lJsXfv3uKOO+4oLrjgguIPf/jDiOef/oyHO+64o9i7d2/xk5/8pOTPeBjrme+4447im9/8ZvHrX/+6ePnll4u1a9cWkydPLv7nf/5nws582uuvv17Mnj27aGtrK6666qqxGfb/K2fmL3zhC8XChQuLzs7O4sCBA8WvfvWr4oUXXpiwM+/cubOYNGlS8d3vfrfYv39/sXPnzuJTn/pUsXTp0jGbeceOHcW6deuKxx9/vIiI4oknnnjP8zPegxlntjfKk3FvFEW+3TFee2NChMuCBQuK9vb2IceuvPLKYs2aNSOe/2//9m/FlVdeOeTYP//zPxef+9znRm3Gdyt15pF88pOfLDZs2FDp0c6o3JmXLVtW/Pu//3uxfv36MV9Apc78X//1X0VjY2Nx9OjRsRhvRKXO/O1vf7uYPXv2kGMPPPBAMX369FGb8b2czQLKeA9mnHkk9sb7y7g3iiL37hjLvTHubxUdP348du/eHW1tbUOOt7W1xa5du0a85sUXXxx2/uc///l46aWX4i9/+cuozXpaOTO/26lTp+LYsWMV/YmZ76XcmR988MF45ZVXYv369aM94jDlzPzUU0/F/Pnz41vf+lZceumlccUVV8Rdd90Vf/7zn8di5LJmbmlpiUOHDsWOHTuiKIp49dVX47HHHosbbrhhLEYuS8Z7MOPM72ZvvL+MeyPiw7E7KnUPlvXToStprH7kfSWVM/O7fec734k333wzbrrpptEYcZhyZv79738fa9asiZ07d0ZNzdj/USln5v3798fzzz8f9fX18cQTT0Rvb298+ctfjtdee21M3q8uZ+aWlpbYunVrLFu2LN5+++04ceJEfOELX4jvfe97oz5vuTLegxlnfjd74/1l3BsRH47dUal7cNxfcTlttH/k/WgodebTHnnkkfjGN74R27Zti4suumi0xhvR2c588uTJuPnmm2PDhg1xxRVXjNV4Iyrlf+dTp05FVVVVbN26NRYsWBDXX3993H///fHQQw+N6bOnUmbeu3dvrFy5Mu65557YvXt3PP3003HgwIFob28fi1HLlvEezDjzafZGaTLujYgP/u6oxD047q+4ZPyR9+XMfNq2bdvi9ttvj0cffTSuvfba0RxziFJnPnbsWLz00kvR1dUVX/3qVyPinZu7KIqoqamJZ555Jq655poJNXNExLRp0+LSSy+NxsbGwWNz5syJoiji0KFDcfnll0+4mTs6OmLx4sVx9913R0TEpz/96bjggguitbU17rvvvlF/JaAcGe/BjDOfZm+M3swR4783Ij4cu6NS9+C4v+KS8UfelzNzxDvPmG699dZ4+OGHx/w9yFJnbmhoiN/+9rexZ8+ewUd7e3t84hOfiD179sTChQsn3MwREYsXL44//elP8cYbbwwee/nll2PSpEkxffr0UZ03oryZ33rrrZg0aeitWF1dHRH/92xkosl4D2acOcLeGO2ZI8Z/b0R8OHZHxe7Bkv4q7ygZjx95P9YzP/zww0VNTU3xgx/8oOjp6Rl8vP766xN25ncbj38dUOrMx44dK6ZPn178wz/8Q/G73/2uePbZZ4vLL7+8+NKXvjRhZ37wwQeLmpqaYuPGjcUrr7xSPP/888X8+fOLBQsWjNnMx44dK7q6uoqurq4iIor777+/6OrqGvxnmB+EezDjzPZGeTLujXLmHu/dMV57Y0KES1Hk/JH3pcx89dVXFxEx7LFixYoJO/O7jccCKorSZ963b19x7bXXFuedd14xffr0YvXq1cVbb701oWd+4IEHik9+8pPFeeedV0ybNq34x3/8x+LQoUNjNu8vfvGL9/zz+UG4B4si38z2Rvky7o2iyLU7xmtvVBXFBHw9CQBgBOP+d1wAAM6WcAEA0hAuAEAawgUASEO4AABpCBcAIA3hAgCkIVwAgDSECwCQhnABANIQLgBAGv8P6EDt3Xw6P4gAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "for path, name, ax in zip(\n",
    "        (\n",
    "                # best_frechet_checkpoint, \n",
    "                last_epoch_checkpoint,\n",
    "        ),\n",
    "        (\n",
    "                # \"Best Frechet\", \n",
    "                \"Last epoch\",\n",
    "        ),\n",
    "    axes\n",
    "):\n",
    "    m = DiffusionModel.load_from_checkpoint(path, unet=UNet(3, n_steps=1000, time_emb_dim=100), betas_min=1e-4, betas_max=0.02, n_steps=1000).to(device)\n",
    "    \n",
    "    preds = m.generate1(16).cpu()\n",
    "    ax.set_title(f\"{name}\\n{path}\")\n",
    "    ax.imshow((torchvision.utils.make_grid(preds).permute(1, 2, 0)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T20:56:41.696957815Z",
     "start_time": "2024-05-11T20:56:39.643065500Z"
    }
   },
   "id": "c014ccb88bb4c65e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test arbitrary checkpoints here\n",
    "\n",
    "ckpt: str | None =  \"lightning_logs/version_2/checkpoints/checkpoint_last-epoch=55-train_loss=0.04-val_frechet=389.36.ckpt\"\n",
    "\n",
    "def _denormalize(images):\n",
    "    normalized = images.clone()\n",
    "    for i in range(len(normalized)):\n",
    "        normalized[i] -= torch.min(normalized[i])\n",
    "        normalized[i] *= 255 / torch.max(normalized[i])\n",
    "    return normalized.to(torch.uint8)\n",
    "\n",
    "if ckpt:\n",
    "    m1 = DiffusionModel.load_from_checkpoint(ckpt, unet=UNet(3, n_steps=1000, time_emb_dim=100), betas_min=1e-4, betas_max=0.02, n_steps=1000)\n",
    "    preds = m1._generate1(8).cpu()\n",
    "    preds = _denormalize(preds)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "    ax.imshow((torchvision.utils.make_grid(preds).permute(1, 2, 0)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T20:56:41.699512131Z",
     "start_time": "2024-05-11T20:56:41.698569963Z"
    }
   },
   "id": "6a34a1115d62fb67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-11T20:56:41.700188885Z"
    }
   },
   "id": "ff528cb8fd191723",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-11T20:56:41.702833725Z"
    }
   },
   "id": "5250e032a5124387"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
