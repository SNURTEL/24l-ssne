{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from torch.optim import Adam\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from typing import Literal, Callable, Any\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "from torch import Tensor\n",
    "from pytorch_lightning.utilities.types import TRAIN_DATALOADERS\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    accuracy_score, \n",
    "    recall_score,\n",
    "    precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 123\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>location not palace excellent hotel booke dthe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>respite definitely not place stay looking ultr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stunning truly memorable spot right beach nusa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>solid business hotel near embassy stayed hotel...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nice place make sure lock money warning money ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16387</th>\n",
       "      <td>great base explore new york stayed 4 nights en...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16388</th>\n",
       "      <td>wonderful advert paris wonderful introduction ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16389</th>\n",
       "      <td>ideal relaxing holdiay rachel jay green liverp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16390</th>\n",
       "      <td>watch food, husband went resort 4 nights chris...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16391</th>\n",
       "      <td>fantastic hotel central barcelona family just ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16392 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rating\n",
       "0      location not palace excellent hotel booke dthe...       4\n",
       "1      respite definitely not place stay looking ultr...       3\n",
       "2      stunning truly memorable spot right beach nusa...       4\n",
       "3      solid business hotel near embassy stayed hotel...       3\n",
       "4      nice place make sure lock money warning money ...       3\n",
       "...                                                  ...     ...\n",
       "16387  great base explore new york stayed 4 nights en...       4\n",
       "16388  wonderful advert paris wonderful introduction ...       4\n",
       "16389  ideal relaxing holdiay rachel jay green liverp...       3\n",
       "16390  watch food, husband went resort 4 nights chris...       2\n",
       "16391  fantastic hotel central barcelona family just ...       4\n",
       "\n",
       "[16392 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/train_data.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Klasyfikacja na podstawie reprezentacji TFIDF\n",
    "\n",
    "Szybkie przeuczenie nawet po dodaniu regularyzacji i ważenia klas w funkcji straty, bardzo słabe wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        review\n",
       "rating        \n",
       "0         1137\n",
       "1         1434\n",
       "2         1747\n",
       "3         4831\n",
       "4         7243"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = df_train.groupby(\"rating\").count()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    df_train.review, df_train.rating, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFIDFClassifier(\n",
       "  (lin1): Linear(in_features=10000, out_features=4000, bias=True)\n",
       "  (act1): LeakyReLU(negative_slope=0.01)\n",
       "  (lin2): Linear(in_features=4000, out_features=1000, bias=True)\n",
       "  (act2): LeakyReLU(negative_slope=0.01)\n",
       "  (lin3): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (act3): LeakyReLU(negative_slope=0.01)\n",
       "  (lin4): Linear(in_features=500, out_features=50, bias=True)\n",
       "  (act4): LeakyReLU(negative_slope=0.01)\n",
       "  (lin5): Linear(in_features=50, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TFIDFClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TFIDFClassifier, self).__init__()\n",
    "        self.lin1 = nn.Linear(10000, 4000)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.lin2 = nn.Linear(4000, 1000)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.lin3 = nn.Linear(1000, 500)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.lin4 = nn.Linear(500, 50)\n",
    "        self.act4 = nn.LeakyReLU()\n",
    "        self.lin5 = nn.Linear(50, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.lin4(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.lin5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "tfidf_model = TFIDFClassifier().to(device)\n",
    "tfidf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_train = CountVectorizer()\n",
    "x_vectorized_train = count_vectorizer_train.fit_transform(x_train.values)\n",
    "tfidf_transformer_train = TfidfTransformer()\n",
    "x_transformed_train = tfidf_transformer_train.fit_transform(x_vectorized_train)[\n",
    "    :, :10000\n",
    "]\n",
    "\n",
    "count_vectorizer_val = CountVectorizer()\n",
    "x_vectorized_val = count_vectorizer_val.fit_transform(x_val.values)\n",
    "tfidf_transformer_val = TfidfTransformer()\n",
    "x_transformed_val = tfidf_transformer_val.fit_transform(x_vectorized_val)[:, :10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(x_transformed_train.todense()).to(torch.float),\n",
    "    torch.tensor(y_train.values),\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.from_numpy(x_transformed_val.todense()).to(torch.float),\n",
    "    torch.tensor(y_val.values),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # *********#\n",
    "    for imgs, labels in data_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        output = model(imgs)\n",
    "        pred = output.max(1, keepdim=True)[1]  # get the index of the max logit\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 1.61 val_acc: 0.107\n",
      "Epoch 1 loss 1.61 val_acc: 0.107\n",
      "Epoch 2 loss 1.61 val_acc: 0.441\n",
      "Epoch 3 loss 1.61 val_acc: 0.441\n",
      "Epoch 4 loss 1.61 val_acc: 0.441\n",
      "Epoch 5 loss 1.61 val_acc: 0.441\n",
      "Epoch 6 loss 1.61 val_acc: 0.293\n",
      "Epoch 7 loss 1.61 val_acc: 0.441\n",
      "Epoch 8 loss 1.61 val_acc: 0.441\n",
      "Epoch 9 loss 1.61 val_acc: 0.441\n",
      "Final Training Accuracy: 0.44217218137254904\n",
      "Final Validation Accuracy: 0.44129307715767\n",
      "Max Validation Accuracy: 0.44129307715767\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor(1 / class_counts.values).squeeze().to(torch.float).to(device)\n",
    ")\n",
    "optimizer = Adam(tfidf_model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "\n",
    "iters = []\n",
    "losses = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for n in range(10):\n",
    "    epoch_losses = []\n",
    "    for x, labels in iter(train_loader):\n",
    "        x, labels = x.to(device), labels.to(device)\n",
    "        tfidf_model.train()\n",
    "        out = tfidf_model(x).squeeze()\n",
    "\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        epoch_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss_mean = np.array(epoch_losses).mean()\n",
    "    iters.append(n)\n",
    "    losses.append(loss_mean)\n",
    "    test_acc = get_accuracy(tfidf_model, val_loader)\n",
    "    print(f\"Epoch {n} loss {loss_mean:.3} val_acc: {test_acc:.3}\")\n",
    "    train_acc.append(\n",
    "        get_accuracy(tfidf_model, train_loader)\n",
    "    )  # compute training accuracy\n",
    "    val_acc.append(test_acc)  # compute validation accuracy\n",
    "\n",
    "\n",
    "print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
    "print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n",
    "print(\"Max Validation Accuracy: {}\".format(max(val_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Klasyfikacja za pomocą modelu LLM\n",
    "\n",
    "Model można pobrać za pomocą komendy: `huggingface-cli download TheBloke/Llama-2-13B-Chat-GGUF llama-2-13b-chat.Q4_K_M.gguf --local-dir LLM --local-dir-use-symlinks False`\n",
    "Sprawdziliśmy podejście zero-shot na modelu Llama 2 13B. Bez przyuczania model radził sobie zdecydowanie lepiej niż pierwsze podejście, jednak nadal wyniki nie był bardzo wysoki. W używaniu LLMów bardzo ważne jest stworzenie odpowiedniego promptu oraz odpowiednie parsowanie odpowiedzi LLMa (pomimo uszczegółowienia, że Llama ma odpowiadać jedynie cyfrą przydzielonych gwiazdek, ma ona tendencję do odpowiadania pełnymi zdaniami wraz z uzasadnieniem - czasem w odpowiedziach nawet wyszczególnia oceny dla danych aspektów pobytu po podaniu ogólnej oceny). Co ciekawe większy model Llamy (dokładniej wersja 2 70B) wcale nie poradził sobie lepiej od wersji 2 13B, a odpowiadał zdecydowanie dłużej."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T17:31:01.868359Z",
     "start_time": "2024-06-16T17:31:01.425029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:51:18.027127Z",
     "start_time": "2024-06-16T16:51:14.423765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm_2_13b = LlamaCpp(model_path='LLM/llama-2-13b-chat.Q4_K_M.gguf',\n",
    "               n_gpu_layers=-1,\n",
    "               n_batch=1024,   # batch size for prompt processing, <=n_ctx,\n",
    "               n_ctx=2048,     # context length, increase for longer input/output\n",
    "               f16_kv=True)   # half-precision for key/value cache"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from LLM/llama-2-13b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 7.33 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3080 Ti, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.37 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    87.89 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  7412.96 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 1024\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   204.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    14.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T17:31:37.026936Z",
     "start_time": "2024-06-16T17:31:04.076753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm_2_70b = LlamaCpp(model_path='LLM/llama-2-70b-chat.Q4_K_M.gguf',\n",
    "               # n_gpu_layers=-1,\n",
    "               n_batch=512,   # batch size for prompt processing, <=n_ctx,\n",
    "               n_ctx=1024,     # context length, increase for longer input/output\n",
    "               f16_kv=True)   # half-precision for key/value cache"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 723 tensors from LLM/llama-2-70b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_K:  441 tensors\n",
      "llama_model_loader: - type q5_K:   40 tensors\n",
      "llama_model_loader: - type q6_K:   81 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 68.98 B\n",
      "llm_load_print_meta: model size       = 38.58 GiB (4.80 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3080 Ti, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.37 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 39503.23 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   320.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  320.00 MiB, K (f16):  160.00 MiB, V (f16):  160.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   433.75 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    18.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 884\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T17:31:37.039344Z",
     "start_time": "2024-06-16T17:31:37.029604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Parser04(BaseOutputParser):\n",
    "    pos: str\n",
    "\n",
    "    def __init__(self, pos: str = 'yes'):\n",
    "        super().__init__(pos=pos)\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        match = re.search(r'\\d+', text)\n",
    "        if match:\n",
    "            if match.group() in ['1', '2', '3', '4', '5']:\n",
    "                return int(match.group()) - 1\n",
    "            else:\n",
    "                return 4\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "parser04 = Parser04()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T17:31:37.049825Z",
     "start_time": "2024-06-16T17:31:37.041216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful assistant who judge what star rating from 1 to 5\n",
    "the hotel deserves based on review which will be given to you.\n",
    "ONLY return a number from [1, 5] and nothing more.\n",
    "<</SYS>> \n",
    "{text} [/INST]\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T17:32:40.174097Z",
     "start_time": "2024-06-16T17:32:40.170028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_prediction(topic, texts, template, parser, llm=llm_2_70b):\n",
    "    predictions = []\n",
    "    cls_zero = PromptTemplate.from_template(template) | llm | parser\n",
    "    for text in texts:\n",
    "        pred_zero = cls_zero.invoke({'topic': topic, 'text': text})\n",
    "        predictions.append(pred_zero)\n",
    "    return predictions"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T17:32:42.461177Z",
     "start_time": "2024-06-16T17:32:42.457961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def confusion_matrix_plot(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T17:28:07.735019Z",
     "start_time": "2024-06-16T16:51:24.495590Z"
    }
   },
   "cell_type": "code",
   "source": "answers_2_13b = make_prediction('Hotel reviews', df_train['review'].tolist()[:1000], template, parser04. llm_2_13b)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.09 ms /     3 runs   (    0.36 ms per token,  2744.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1313.36 ms /   142 tokens (    9.25 ms per token,   108.12 tokens per second)\n",
      "llama_print_timings:        eval time =      37.38 ms /     2 runs   (   18.69 ms per token,    53.51 tokens per second)\n",
      "llama_print_timings:       total time =    1360.80 ms /   144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      97.42 ms /   256 runs   (    0.38 ms per token,  2627.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     195.28 ms /   267 tokens (    0.73 ms per token,  1367.25 tokens per second)\n",
      "llama_print_timings:        eval time =    4541.36 ms /   255 runs   (   17.81 ms per token,    56.15 tokens per second)\n",
      "llama_print_timings:       total time =    5732.52 ms /   522 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.56 ms /    17 runs   (    0.39 ms per token,  2589.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.66 ms /    90 tokens (    1.25 ms per token,   798.89 tokens per second)\n",
      "llama_print_timings:        eval time =     264.31 ms /    16 runs   (   16.52 ms per token,    60.53 tokens per second)\n",
      "llama_print_timings:       total time =     433.23 ms /   106 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.57 ms /   121 runs   (    0.38 ms per token,  2598.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.60 ms /    89 tokens (    1.21 ms per token,   827.15 tokens per second)\n",
      "llama_print_timings:        eval time =    2075.43 ms /   120 runs   (   17.30 ms per token,    57.82 tokens per second)\n",
      "llama_print_timings:       total time =    2616.47 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.40 ms /    19 runs   (    0.39 ms per token,  2569.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     111.19 ms /    73 tokens (    1.52 ms per token,   656.52 tokens per second)\n",
      "llama_print_timings:        eval time =     329.83 ms /    18 runs   (   18.32 ms per token,    54.57 tokens per second)\n",
      "llama_print_timings:       total time =     509.48 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.72 ms /   114 runs   (    0.38 ms per token,  2607.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     111.91 ms /   101 tokens (    1.11 ms per token,   902.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1925.39 ms /   113 runs   (   17.04 ms per token,    58.69 tokens per second)\n",
      "llama_print_timings:       total time =    2457.08 ms /   214 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.52 ms /   108 runs   (    0.37 ms per token,  2732.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.76 ms /   171 tokens (    0.89 ms per token,  1119.38 tokens per second)\n",
      "llama_print_timings:        eval time =    1849.73 ms /   107 runs   (   17.29 ms per token,    57.85 tokens per second)\n",
      "llama_print_timings:       total time =    2367.46 ms /   278 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.17 ms /   123 runs   (    0.37 ms per token,  2722.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.96 ms /   232 tokens (    0.67 ms per token,  1497.17 tokens per second)\n",
      "llama_print_timings:        eval time =    2114.37 ms /   122 runs   (   17.33 ms per token,    57.70 tokens per second)\n",
      "llama_print_timings:       total time =    2685.67 ms /   354 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.84 ms /   111 runs   (    0.37 ms per token,  2717.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.61 ms /   188 tokens (    0.82 ms per token,  1215.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1870.83 ms /   110 runs   (   17.01 ms per token,    58.80 tokens per second)\n",
      "llama_print_timings:       total time =    2399.31 ms /   298 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.74 ms /    97 runs   (    0.37 ms per token,  2714.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     113.43 ms /    88 tokens (    1.29 ms per token,   775.80 tokens per second)\n",
      "llama_print_timings:        eval time =    1594.56 ms /    96 runs   (   16.61 ms per token,    60.20 tokens per second)\n",
      "llama_print_timings:       total time =    2036.55 ms /   184 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      77.26 ms /   215 runs   (    0.36 ms per token,  2782.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.96 ms /   253 tokens (    0.62 ms per token,  1611.86 tokens per second)\n",
      "llama_print_timings:        eval time =    3706.10 ms /   214 runs   (   17.32 ms per token,    57.74 tokens per second)\n",
      "llama_print_timings:       total time =    4613.38 ms /   467 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.04 ms /    21 runs   (    0.38 ms per token,  2611.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     115.56 ms /    87 tokens (    1.33 ms per token,   752.88 tokens per second)\n",
      "llama_print_timings:        eval time =     350.91 ms /    20 runs   (   17.55 ms per token,    56.99 tokens per second)\n",
      "llama_print_timings:       total time =     538.20 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.44 ms /    23 runs   (    0.37 ms per token,  2724.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.55 ms /   192 tokens (    0.83 ms per token,  1203.36 tokens per second)\n",
      "llama_print_timings:        eval time =     396.84 ms /    22 runs   (   18.04 ms per token,    55.44 tokens per second)\n",
      "llama_print_timings:       total time =     634.16 ms /   214 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      68.11 ms /   183 runs   (    0.37 ms per token,  2686.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.55 ms /   336 tokens (    0.62 ms per token,  1603.41 tokens per second)\n",
      "llama_print_timings:        eval time =    3257.93 ms /   182 runs   (   17.90 ms per token,    55.86 tokens per second)\n",
      "llama_print_timings:       total time =    4129.32 ms /   518 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.67 ms /   110 runs   (    0.38 ms per token,  2640.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.55 ms /    71 tokens (    1.54 ms per token,   648.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1847.71 ms /   109 runs   (   16.95 ms per token,    58.99 tokens per second)\n",
      "llama_print_timings:       total time =    2338.32 ms /   180 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.20 ms /    17 runs   (    0.36 ms per token,  2743.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.36 ms /    50 tokens (    2.17 ms per token,   461.42 tokens per second)\n",
      "llama_print_timings:        eval time =     286.16 ms /    16 runs   (   17.88 ms per token,    55.91 tokens per second)\n",
      "llama_print_timings:       total time =     455.51 ms /    66 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.21 ms /    17 runs   (    0.37 ms per token,  2735.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.89 ms /    33 tokens (    3.42 ms per token,   292.32 tokens per second)\n",
      "llama_print_timings:        eval time =     286.92 ms /    16 runs   (   17.93 ms per token,    55.76 tokens per second)\n",
      "llama_print_timings:       total time =     459.48 ms /    49 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.15 ms /   124 runs   (    0.37 ms per token,  2686.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.06 ms /    45 tokens (    2.40 ms per token,   416.42 tokens per second)\n",
      "llama_print_timings:        eval time =    2007.95 ms /   123 runs   (   16.32 ms per token,    61.26 tokens per second)\n",
      "llama_print_timings:       total time =    2537.04 ms /   168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.34 ms /    22 runs   (    0.38 ms per token,  2637.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =      99.37 ms /    44 tokens (    2.26 ms per token,   442.79 tokens per second)\n",
      "llama_print_timings:        eval time =     367.09 ms /    21 runs   (   17.48 ms per token,    57.21 tokens per second)\n",
      "llama_print_timings:       total time =     542.07 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      72.38 ms /   200 runs   (    0.36 ms per token,  2763.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     212.30 ms /   346 tokens (    0.61 ms per token,  1629.80 tokens per second)\n",
      "llama_print_timings:        eval time =    3528.42 ms /   199 runs   (   17.73 ms per token,    56.40 tokens per second)\n",
      "llama_print_timings:       total time =    4439.10 ms /   545 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.04 ms /   129 runs   (    0.38 ms per token,  2630.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.42 ms /   210 tokens (    0.67 ms per token,  1495.56 tokens per second)\n",
      "llama_print_timings:        eval time =    2218.05 ms /   128 runs   (   17.33 ms per token,    57.71 tokens per second)\n",
      "llama_print_timings:       total time =    2804.27 ms /   338 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.34 ms /    19 runs   (    0.39 ms per token,  2589.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     115.73 ms /    90 tokens (    1.29 ms per token,   777.68 tokens per second)\n",
      "llama_print_timings:        eval time =     320.07 ms /    18 runs   (   17.78 ms per token,    56.24 tokens per second)\n",
      "llama_print_timings:       total time =     500.82 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     3 runs   (    0.37 ms per token,  2676.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.18 ms /    93 tokens (    1.25 ms per token,   800.45 tokens per second)\n",
      "llama_print_timings:        eval time =      39.31 ms /     2 runs   (   19.66 ms per token,    50.88 tokens per second)\n",
      "llama_print_timings:       total time =     165.18 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.22 ms /   117 runs   (    0.37 ms per token,  2707.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.60 ms /   224 tokens (    0.71 ms per token,  1412.34 tokens per second)\n",
      "llama_print_timings:        eval time =    2069.64 ms /   116 runs   (   17.84 ms per token,    56.05 tokens per second)\n",
      "llama_print_timings:       total time =    2639.32 ms /   340 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      51.92 ms /   138 runs   (    0.38 ms per token,  2657.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.13 ms /   180 tokens (    0.90 ms per token,  1117.11 tokens per second)\n",
      "llama_print_timings:        eval time =    2481.46 ms /   137 runs   (   18.11 ms per token,    55.21 tokens per second)\n",
      "llama_print_timings:       total time =    3151.59 ms /   317 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2624.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.26 ms /    82 tokens (    1.33 ms per token,   750.49 tokens per second)\n",
      "llama_print_timings:        eval time =      38.95 ms /     2 runs   (   19.47 ms per token,    51.35 tokens per second)\n",
      "llama_print_timings:       total time =     159.01 ms /    84 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.30 ms /    94 runs   (    0.38 ms per token,  2663.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.39 ms /   142 tokens (    1.05 ms per token,   950.53 tokens per second)\n",
      "llama_print_timings:        eval time =    1569.82 ms /    93 runs   (   16.88 ms per token,    59.24 tokens per second)\n",
      "llama_print_timings:       total time =    2049.28 ms /   235 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.10 ms /     3 runs   (    0.37 ms per token,  2727.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.42 ms /    49 tokens (    2.23 ms per token,   447.82 tokens per second)\n",
      "llama_print_timings:        eval time =      36.39 ms /     2 runs   (   18.19 ms per token,    54.96 tokens per second)\n",
      "llama_print_timings:       total time =     156.14 ms /    51 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      95.04 ms /   253 runs   (    0.38 ms per token,  2662.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     396.38 ms /   591 tokens (    0.67 ms per token,  1491.00 tokens per second)\n",
      "llama_print_timings:        eval time =    4698.68 ms /   252 runs   (   18.65 ms per token,    53.63 tokens per second)\n",
      "llama_print_timings:       total time =    6102.71 ms /   843 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.94 ms /   116 runs   (    0.38 ms per token,  2639.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     205.55 ms /   274 tokens (    0.75 ms per token,  1333.00 tokens per second)\n",
      "llama_print_timings:        eval time =    2022.22 ms /   115 runs   (   17.58 ms per token,    56.87 tokens per second)\n",
      "llama_print_timings:       total time =    2680.90 ms /   389 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      20.70 ms /    55 runs   (    0.38 ms per token,  2656.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.01 ms /    86 tokens (    1.40 ms per token,   716.58 tokens per second)\n",
      "llama_print_timings:        eval time =     940.76 ms /    54 runs   (   17.42 ms per token,    57.40 tokens per second)\n",
      "llama_print_timings:       total time =    1257.99 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2599.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.85 ms /   144 tokens (    1.01 ms per token,   994.13 tokens per second)\n",
      "llama_print_timings:        eval time =      36.97 ms /     2 runs   (   18.49 ms per token,    54.10 tokens per second)\n",
      "llama_print_timings:       total time =     192.75 ms /   146 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.63 ms /    99 runs   (    0.36 ms per token,  2778.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.89 ms /    96 tokens (    1.20 ms per token,   835.56 tokens per second)\n",
      "llama_print_timings:        eval time =    1808.27 ms /    98 runs   (   18.45 ms per token,    54.20 tokens per second)\n",
      "llama_print_timings:       total time =    2282.46 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.71 ms /    21 runs   (    0.32 ms per token,  3128.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.83 ms /   166 tokens (    0.91 ms per token,  1093.34 tokens per second)\n",
      "llama_print_timings:        eval time =     365.15 ms /    20 runs   (   18.26 ms per token,    54.77 tokens per second)\n",
      "llama_print_timings:       total time =     589.70 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2601.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.98 ms /    98 tokens (    1.17 ms per token,   852.33 tokens per second)\n",
      "llama_print_timings:        eval time =      38.07 ms /     2 runs   (   19.03 ms per token,    52.54 tokens per second)\n",
      "llama_print_timings:       total time =     164.55 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.47 ms /   115 runs   (    0.37 ms per token,  2707.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.91 ms /   197 tokens (    0.81 ms per token,  1231.90 tokens per second)\n",
      "llama_print_timings:        eval time =    2096.32 ms /   114 runs   (   18.39 ms per token,    54.38 tokens per second)\n",
      "llama_print_timings:       total time =    2673.31 ms /   311 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      29.16 ms /    79 runs   (    0.37 ms per token,  2709.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.14 ms /   150 tokens (    0.99 ms per token,  1005.75 tokens per second)\n",
      "llama_print_timings:        eval time =    1364.22 ms /    78 runs   (   17.49 ms per token,    57.18 tokens per second)\n",
      "llama_print_timings:       total time =    1799.31 ms /   228 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.37 ms /    95 runs   (    0.37 ms per token,  2685.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     117.39 ms /   115 tokens (    1.02 ms per token,   979.68 tokens per second)\n",
      "llama_print_timings:        eval time =    1627.83 ms /    94 runs   (   17.32 ms per token,    57.75 tokens per second)\n",
      "llama_print_timings:       total time =    2078.35 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.78 ms /   130 runs   (    0.41 ms per token,  2462.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.25 ms /   167 tokens (    0.85 ms per token,  1182.27 tokens per second)\n",
      "llama_print_timings:        eval time =    2482.97 ms /   129 runs   (   19.25 ms per token,    51.95 tokens per second)\n",
      "llama_print_timings:       total time =    3189.13 ms /   296 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      64.20 ms /   173 runs   (    0.37 ms per token,  2694.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.43 ms /   134 tokens (    1.14 ms per token,   879.09 tokens per second)\n",
      "llama_print_timings:        eval time =    3070.72 ms /   172 runs   (   17.85 ms per token,    56.01 tokens per second)\n",
      "llama_print_timings:       total time =    3855.35 ms /   306 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.73 ms /    21 runs   (    0.37 ms per token,  2717.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.53 ms /    37 tokens (    3.15 ms per token,   317.51 tokens per second)\n",
      "llama_print_timings:        eval time =     368.97 ms /    20 runs   (   18.45 ms per token,    54.20 tokens per second)\n",
      "llama_print_timings:       total time =     560.73 ms /    57 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      90.74 ms /   248 runs   (    0.37 ms per token,  2732.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     568.82 ms /   931 tokens (    0.61 ms per token,  1636.72 tokens per second)\n",
      "llama_print_timings:        eval time =    4581.36 ms /   247 runs   (   18.55 ms per token,    53.91 tokens per second)\n",
      "llama_print_timings:       total time =    6068.50 ms /  1178 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.71 ms /    96 runs   (    0.39 ms per token,  2545.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.34 ms /   213 tokens (    0.76 ms per token,  1320.19 tokens per second)\n",
      "llama_print_timings:        eval time =    1681.68 ms /    95 runs   (   17.70 ms per token,    56.49 tokens per second)\n",
      "llama_print_timings:       total time =    2208.26 ms /   308 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      19.37 ms /    51 runs   (    0.38 ms per token,  2633.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.49 ms /   116 tokens (    1.06 ms per token,   947.04 tokens per second)\n",
      "llama_print_timings:        eval time =     859.01 ms /    50 runs   (   17.18 ms per token,    58.21 tokens per second)\n",
      "llama_print_timings:       total time =    1161.88 ms /   166 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      22.76 ms /    60 runs   (    0.38 ms per token,  2636.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.16 ms /   211 tokens (    0.74 ms per token,  1351.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1068.62 ms /    59 runs   (   18.11 ms per token,    55.21 tokens per second)\n",
      "llama_print_timings:       total time =    1436.16 ms /   270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.71 ms /    76 runs   (    0.38 ms per token,  2647.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     150.76 ms /   168 tokens (    0.90 ms per token,  1114.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1352.56 ms /    75 runs   (   18.03 ms per token,    55.45 tokens per second)\n",
      "llama_print_timings:       total time =    1773.16 ms /   243 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      59.45 ms /   161 runs   (    0.37 ms per token,  2708.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     200.42 ms /   258 tokens (    0.78 ms per token,  1287.32 tokens per second)\n",
      "llama_print_timings:        eval time =    2851.46 ms /   160 runs   (   17.82 ms per token,    56.11 tokens per second)\n",
      "llama_print_timings:       total time =    3633.86 ms /   418 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.38 ms /   110 runs   (    0.37 ms per token,  2723.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.96 ms /   162 tokens (    0.94 ms per token,  1059.09 tokens per second)\n",
      "llama_print_timings:        eval time =    1921.99 ms /   109 runs   (   17.63 ms per token,    56.71 tokens per second)\n",
      "llama_print_timings:       total time =    2461.68 ms /   271 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.13 ms /    21 runs   (    0.39 ms per token,  2582.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     127.06 ms /   113 tokens (    1.12 ms per token,   889.38 tokens per second)\n",
      "llama_print_timings:        eval time =     370.94 ms /    20 runs   (   18.55 ms per token,    53.92 tokens per second)\n",
      "llama_print_timings:       total time =     576.31 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.98 ms /    89 runs   (    0.37 ms per token,  2698.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.10 ms /    87 tokens (    1.31 ms per token,   762.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1541.04 ms /    88 runs   (   17.51 ms per token,    57.10 tokens per second)\n",
      "llama_print_timings:       total time =    1969.42 ms /   175 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     3 runs   (    0.39 ms per token,  2595.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.90 ms /   124 tokens (    0.97 ms per token,  1034.18 tokens per second)\n",
      "llama_print_timings:        eval time =      36.87 ms /     2 runs   (   18.44 ms per token,    54.24 tokens per second)\n",
      "llama_print_timings:       total time =     168.05 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.06 ms /     3 runs   (    0.35 ms per token,  2824.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.66 ms /    36 tokens (    3.05 ms per token,   328.28 tokens per second)\n",
      "llama_print_timings:        eval time =      36.72 ms /     2 runs   (   18.36 ms per token,    54.47 tokens per second)\n",
      "llama_print_timings:       total time =     156.90 ms /    38 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.76 ms /    82 runs   (    0.38 ms per token,  2665.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.96 ms /    91 tokens (    1.24 ms per token,   805.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1362.03 ms /    81 runs   (   16.82 ms per token,    59.47 tokens per second)\n",
      "llama_print_timings:       total time =    1769.57 ms /   172 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      78.64 ms /   210 runs   (    0.37 ms per token,  2670.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.00 ms /   322 tokens (    0.68 ms per token,  1470.33 tokens per second)\n",
      "llama_print_timings:        eval time =    3860.00 ms /   209 runs   (   18.47 ms per token,    54.15 tokens per second)\n",
      "llama_print_timings:       total time =    4879.94 ms /   531 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.13 ms /   122 runs   (    0.41 ms per token,  2433.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.76 ms /   144 tokens (    0.93 ms per token,  1076.59 tokens per second)\n",
      "llama_print_timings:        eval time =    2272.90 ms /   121 runs   (   18.78 ms per token,    53.24 tokens per second)\n",
      "llama_print_timings:       total time =    2950.88 ms /   265 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.19 ms /    84 runs   (    0.38 ms per token,  2609.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.53 ms /   103 tokens (    1.27 ms per token,   789.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1462.07 ms /    83 runs   (   17.62 ms per token,    56.77 tokens per second)\n",
      "llama_print_timings:       total time =    1891.81 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.75 ms /   106 runs   (    0.37 ms per token,  2735.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.81 ms /   124 tokens (    1.01 ms per token,   993.47 tokens per second)\n",
      "llama_print_timings:        eval time =    1876.06 ms /   105 runs   (   17.87 ms per token,    55.97 tokens per second)\n",
      "llama_print_timings:       total time =    2380.95 ms /   229 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.47 ms /   105 runs   (    0.37 ms per token,  2729.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.60 ms /   235 tokens (    0.64 ms per token,  1570.88 tokens per second)\n",
      "llama_print_timings:        eval time =    1878.53 ms /   104 runs   (   18.06 ms per token,    55.36 tokens per second)\n",
      "llama_print_timings:       total time =    2400.29 ms /   339 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.21 ms /    21 runs   (    0.39 ms per token,  2556.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.52 ms /   245 tokens (    0.62 ms per token,  1616.92 tokens per second)\n",
      "llama_print_timings:        eval time =     367.05 ms /    20 runs   (   18.35 ms per token,    54.49 tokens per second)\n",
      "llama_print_timings:       total time =     603.85 ms /   265 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.78 ms /    93 runs   (    0.37 ms per token,  2673.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.73 ms /   177 tokens (    0.91 ms per token,  1094.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1639.19 ms /    92 runs   (   17.82 ms per token,    56.13 tokens per second)\n",
      "llama_print_timings:       total time =    2129.81 ms /   269 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.81 ms /   102 runs   (    0.37 ms per token,  2697.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.77 ms /    69 tokens (    1.53 ms per token,   652.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1747.71 ms /   101 runs   (   17.30 ms per token,    57.79 tokens per second)\n",
      "llama_print_timings:       total time =    2211.42 ms /   170 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.14 ms /    21 runs   (    0.39 ms per token,  2578.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     245.62 ms /   383 tokens (    0.64 ms per token,  1559.33 tokens per second)\n",
      "llama_print_timings:        eval time =     356.12 ms /    20 runs   (   17.81 ms per token,    56.16 tokens per second)\n",
      "llama_print_timings:       total time =     676.12 ms /   403 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.33 ms /   109 runs   (    0.38 ms per token,  2637.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.15 ms /   104 tokens (    1.10 ms per token,   911.09 tokens per second)\n",
      "llama_print_timings:        eval time =    1843.23 ms /   108 runs   (   17.07 ms per token,    58.59 tokens per second)\n",
      "llama_print_timings:       total time =    2344.33 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.34 ms /   130 runs   (    0.37 ms per token,  2689.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     146.53 ms /   162 tokens (    0.90 ms per token,  1105.57 tokens per second)\n",
      "llama_print_timings:        eval time =    2298.62 ms /   129 runs   (   17.82 ms per token,    56.12 tokens per second)\n",
      "llama_print_timings:       total time =    2905.23 ms /   291 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.64 ms /    87 runs   (    0.38 ms per token,  2665.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.57 ms /   115 tokens (    1.07 ms per token,   930.63 tokens per second)\n",
      "llama_print_timings:        eval time =    1503.64 ms /    86 runs   (   17.48 ms per token,    57.19 tokens per second)\n",
      "llama_print_timings:       total time =    1941.29 ms /   201 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     3 runs   (    0.37 ms per token,  2680.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.95 ms /    43 tokens (    2.72 ms per token,   367.68 tokens per second)\n",
      "llama_print_timings:        eval time =      38.67 ms /     2 runs   (   19.34 ms per token,    51.71 tokens per second)\n",
      "llama_print_timings:       total time =     169.06 ms /    45 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.49 ms /   129 runs   (    0.39 ms per token,  2555.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.82 ms /   192 tokens (    0.87 ms per token,  1150.95 tokens per second)\n",
      "llama_print_timings:        eval time =    2306.49 ms /   128 runs   (   18.02 ms per token,    55.50 tokens per second)\n",
      "llama_print_timings:       total time =    2992.36 ms /   320 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.48 ms /   129 runs   (    0.37 ms per token,  2716.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.43 ms /   366 tokens (    0.59 ms per token,  1698.93 tokens per second)\n",
      "llama_print_timings:        eval time =    2289.21 ms /   128 runs   (   17.88 ms per token,    55.91 tokens per second)\n",
      "llama_print_timings:       total time =    2961.60 ms /   494 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.98 ms /   108 runs   (    0.38 ms per token,  2635.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     169.46 ms /   207 tokens (    0.82 ms per token,  1221.53 tokens per second)\n",
      "llama_print_timings:        eval time =    1979.84 ms /   107 runs   (   18.50 ms per token,    54.04 tokens per second)\n",
      "llama_print_timings:       total time =    2544.98 ms /   314 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.80 ms /   138 runs   (    0.37 ms per token,  2716.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.95 ms /   175 tokens (    0.87 ms per token,  1144.14 tokens per second)\n",
      "llama_print_timings:        eval time =    2315.17 ms /   137 runs   (   16.90 ms per token,    59.18 tokens per second)\n",
      "llama_print_timings:       total time =    2961.20 ms /   312 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.60 ms /   109 runs   (    0.37 ms per token,  2684.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     117.09 ms /    89 tokens (    1.32 ms per token,   760.11 tokens per second)\n",
      "llama_print_timings:        eval time =    1892.71 ms /   108 runs   (   17.53 ms per token,    57.06 tokens per second)\n",
      "llama_print_timings:       total time =    2398.42 ms /   197 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.27 ms /   127 runs   (    0.37 ms per token,  2686.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.83 ms /   107 tokens (    1.03 ms per token,   974.26 tokens per second)\n",
      "llama_print_timings:        eval time =    2203.39 ms /   126 runs   (   17.49 ms per token,    57.18 tokens per second)\n",
      "llama_print_timings:       total time =    2759.75 ms /   233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.11 ms /     3 runs   (    0.37 ms per token,  2690.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.17 ms /    69 tokens (    1.73 ms per token,   579.02 tokens per second)\n",
      "llama_print_timings:        eval time =      37.59 ms /     2 runs   (   18.80 ms per token,    53.20 tokens per second)\n",
      "llama_print_timings:       total time =     167.38 ms /    71 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      12.14 ms /    34 runs   (    0.36 ms per token,  2799.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     180.05 ms /    31 tokens (    5.81 ms per token,   172.17 tokens per second)\n",
      "llama_print_timings:        eval time =     572.15 ms /    33 runs   (   17.34 ms per token,    57.68 tokens per second)\n",
      "llama_print_timings:       total time =     866.33 ms /    64 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.56 ms /   132 runs   (    0.38 ms per token,  2610.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     220.70 ms /   297 tokens (    0.74 ms per token,  1345.72 tokens per second)\n",
      "llama_print_timings:        eval time =    2324.41 ms /   131 runs   (   17.74 ms per token,    56.36 tokens per second)\n",
      "llama_print_timings:       total time =    3012.54 ms /   428 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.63 ms /    21 runs   (    0.32 ms per token,  3166.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.90 ms /   244 tokens (    0.66 ms per token,  1516.50 tokens per second)\n",
      "llama_print_timings:        eval time =     368.50 ms /    20 runs   (   18.42 ms per token,    54.27 tokens per second)\n",
      "llama_print_timings:       total time =     603.23 ms /   264 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.45 ms /   111 runs   (    0.37 ms per token,  2677.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.32 ms /   102 tokens (    1.14 ms per token,   876.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1956.18 ms /   110 runs   (   17.78 ms per token,    56.23 tokens per second)\n",
      "llama_print_timings:       total time =    2461.33 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.65 ms /   115 runs   (    0.36 ms per token,  2761.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.68 ms /   130 tokens (    1.12 ms per token,   892.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1970.09 ms /   114 runs   (   17.28 ms per token,    57.87 tokens per second)\n",
      "llama_print_timings:       total time =    2522.49 ms /   244 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.36 ms /   130 runs   (    0.36 ms per token,  2745.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     150.95 ms /   237 tokens (    0.64 ms per token,  1570.08 tokens per second)\n",
      "llama_print_timings:        eval time =    2223.55 ms /   129 runs   (   17.24 ms per token,    58.02 tokens per second)\n",
      "llama_print_timings:       total time =    2839.72 ms /   366 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.55 ms /   121 runs   (    0.37 ms per token,  2715.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.88 ms /   246 tokens (    0.68 ms per token,  1474.15 tokens per second)\n",
      "llama_print_timings:        eval time =    2159.47 ms /   120 runs   (   18.00 ms per token,    55.57 tokens per second)\n",
      "llama_print_timings:       total time =    2763.04 ms /   366 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.41 ms /   109 runs   (    0.37 ms per token,  2697.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.32 ms /   135 tokens (    1.02 ms per token,   976.00 tokens per second)\n",
      "llama_print_timings:        eval time =    1871.80 ms /   108 runs   (   17.33 ms per token,    57.70 tokens per second)\n",
      "llama_print_timings:       total time =    2394.37 ms /   243 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.80 ms /   118 runs   (    0.36 ms per token,  2756.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.55 ms /    61 tokens (    1.81 ms per token,   551.80 tokens per second)\n",
      "llama_print_timings:        eval time =    2027.45 ms /   117 runs   (   17.33 ms per token,    57.71 tokens per second)\n",
      "llama_print_timings:       total time =    2549.92 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      55.76 ms /   150 runs   (    0.37 ms per token,  2690.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.02 ms /   199 tokens (    0.84 ms per token,  1184.38 tokens per second)\n",
      "llama_print_timings:        eval time =    2679.33 ms /   149 runs   (   17.98 ms per token,    55.61 tokens per second)\n",
      "llama_print_timings:       total time =    3386.14 ms /   348 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      85.27 ms /   229 runs   (    0.37 ms per token,  2685.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     238.87 ms /   374 tokens (    0.64 ms per token,  1565.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4177.39 ms /   228 runs   (   18.32 ms per token,    54.58 tokens per second)\n",
      "llama_print_timings:       total time =    5279.05 ms /   602 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.84 ms /   120 runs   (    0.37 ms per token,  2676.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.35 ms /    69 tokens (    1.63 ms per token,   614.15 tokens per second)\n",
      "llama_print_timings:        eval time =    2073.54 ms /   119 runs   (   17.42 ms per token,    57.39 tokens per second)\n",
      "llama_print_timings:       total time =    2622.18 ms /   188 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2633.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     117.37 ms /    96 tokens (    1.22 ms per token,   817.96 tokens per second)\n",
      "llama_print_timings:        eval time =      37.53 ms /     2 runs   (   18.77 ms per token,    53.29 tokens per second)\n",
      "llama_print_timings:       total time =     165.39 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2544.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.26 ms /    69 tokens (    1.60 ms per token,   625.78 tokens per second)\n",
      "llama_print_timings:        eval time =      34.85 ms /     2 runs   (   17.42 ms per token,    57.40 tokens per second)\n",
      "llama_print_timings:       total time =     157.17 ms /    71 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.94 ms /    21 runs   (    0.38 ms per token,  2644.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.63 ms /   192 tokens (    0.87 ms per token,  1152.27 tokens per second)\n",
      "llama_print_timings:        eval time =     360.86 ms /    20 runs   (   18.04 ms per token,    55.42 tokens per second)\n",
      "llama_print_timings:       total time =     600.05 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.10 ms /    71 runs   (    0.38 ms per token,  2619.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.93 ms /    49 tokens (    2.24 ms per token,   445.74 tokens per second)\n",
      "llama_print_timings:        eval time =    1228.55 ms /    70 runs   (   17.55 ms per token,    56.98 tokens per second)\n",
      "llama_print_timings:       total time =    1587.35 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.74 ms /   115 runs   (    0.36 ms per token,  2754.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.72 ms /   134 tokens (    1.11 ms per token,   900.99 tokens per second)\n",
      "llama_print_timings:        eval time =    2026.69 ms /   114 runs   (   17.78 ms per token,    56.25 tokens per second)\n",
      "llama_print_timings:       total time =    2581.95 ms /   248 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.01 ms /   126 runs   (    0.37 ms per token,  2680.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.43 ms /   163 tokens (    0.98 ms per token,  1016.05 tokens per second)\n",
      "llama_print_timings:        eval time =    2250.15 ms /   125 runs   (   18.00 ms per token,    55.55 tokens per second)\n",
      "llama_print_timings:       total time =    2865.21 ms /   288 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     3 runs   (    0.37 ms per token,  2678.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.19 ms /    63 tokens (    1.72 ms per token,   582.30 tokens per second)\n",
      "llama_print_timings:        eval time =      38.58 ms /     2 runs   (   19.29 ms per token,    51.84 tokens per second)\n",
      "llama_print_timings:       total time =     157.71 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      18.87 ms /    50 runs   (    0.38 ms per token,  2650.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     153.05 ms /   154 tokens (    0.99 ms per token,  1006.22 tokens per second)\n",
      "llama_print_timings:        eval time =     879.28 ms /    49 runs   (   17.94 ms per token,    55.73 tokens per second)\n",
      "llama_print_timings:       total time =    1203.96 ms /   203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2615.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.62 ms /   101 tokens (    1.05 ms per token,   956.21 tokens per second)\n",
      "llama_print_timings:        eval time =      31.06 ms /     2 runs   (   15.53 ms per token,    64.40 tokens per second)\n",
      "llama_print_timings:       total time =     148.09 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.04 ms /    21 runs   (    0.38 ms per token,  2611.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.17 ms /   108 tokens (    1.17 ms per token,   855.97 tokens per second)\n",
      "llama_print_timings:        eval time =     364.04 ms /    20 runs   (   18.20 ms per token,    54.94 tokens per second)\n",
      "llama_print_timings:       total time =     563.33 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.50 ms /    21 runs   (    0.36 ms per token,  2801.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.21 ms /    86 tokens (    1.43 ms per token,   698.00 tokens per second)\n",
      "llama_print_timings:        eval time =     367.49 ms /    20 runs   (   18.37 ms per token,    54.42 tokens per second)\n",
      "llama_print_timings:       total time =     565.22 ms /   106 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      29.22 ms /    78 runs   (    0.37 ms per token,  2669.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     117.34 ms /    85 tokens (    1.38 ms per token,   724.40 tokens per second)\n",
      "llama_print_timings:        eval time =    1309.46 ms /    77 runs   (   17.01 ms per token,    58.80 tokens per second)\n",
      "llama_print_timings:       total time =    1700.62 ms /   162 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /     3 runs   (    0.38 ms per token,  2654.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     382.70 ms /   522 tokens (    0.73 ms per token,  1363.98 tokens per second)\n",
      "llama_print_timings:        eval time =      38.59 ms /     2 runs   (   19.29 ms per token,    51.83 tokens per second)\n",
      "llama_print_timings:       total time =     431.42 ms /   524 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.34 ms /   136 runs   (    0.36 ms per token,  2756.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.48 ms /   182 tokens (    0.89 ms per token,  1120.16 tokens per second)\n",
      "llama_print_timings:        eval time =    2383.30 ms /   135 runs   (   17.65 ms per token,    56.64 tokens per second)\n",
      "llama_print_timings:       total time =    3034.15 ms /   317 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.14 ms /    95 runs   (    0.37 ms per token,  2703.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.84 ms /   203 tokens (    0.77 ms per token,  1302.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1696.81 ms /    94 runs   (   18.05 ms per token,    55.40 tokens per second)\n",
      "llama_print_timings:       total time =    2185.44 ms /   297 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.22 ms /     3 runs   (    0.41 ms per token,  2454.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.94 ms /    55 tokens (    2.00 ms per token,   500.26 tokens per second)\n",
      "llama_print_timings:        eval time =      38.93 ms /     2 runs   (   19.47 ms per token,    51.37 tokens per second)\n",
      "llama_print_timings:       total time =     160.21 ms /    57 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      56.08 ms /   150 runs   (    0.37 ms per token,  2674.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.51 ms /   121 tokens (    1.03 ms per token,   971.83 tokens per second)\n",
      "llama_print_timings:        eval time =    2630.80 ms /   149 runs   (   17.66 ms per token,    56.64 tokens per second)\n",
      "llama_print_timings:       total time =    3286.13 ms /   270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.61 ms /    18 runs   (    0.37 ms per token,  2722.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.08 ms /    57 tokens (    1.88 ms per token,   532.29 tokens per second)\n",
      "llama_print_timings:        eval time =     307.13 ms /    17 runs   (   18.07 ms per token,    55.35 tokens per second)\n",
      "llama_print_timings:       total time =     477.95 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.87 ms /    86 runs   (    0.37 ms per token,  2698.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.09 ms /    92 tokens (    1.26 ms per token,   792.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1491.01 ms /    85 runs   (   17.54 ms per token,    57.01 tokens per second)\n",
      "llama_print_timings:       total time =    1907.84 ms /   177 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.66 ms /    85 runs   (    0.38 ms per token,  2602.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.99 ms /   195 tokens (    0.81 ms per token,  1242.09 tokens per second)\n",
      "llama_print_timings:        eval time =    1516.12 ms /    84 runs   (   18.05 ms per token,    55.40 tokens per second)\n",
      "llama_print_timings:       total time =    1971.78 ms /   279 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.41 ms /    22 runs   (    0.38 ms per token,  2616.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.39 ms /    65 tokens (    1.82 ms per token,   549.01 tokens per second)\n",
      "llama_print_timings:        eval time =     380.33 ms /    21 runs   (   18.11 ms per token,    55.22 tokens per second)\n",
      "llama_print_timings:       total time =     575.34 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.96 ms /    21 runs   (    0.38 ms per token,  2636.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.26 ms /    68 tokens (    1.78 ms per token,   560.77 tokens per second)\n",
      "llama_print_timings:        eval time =     363.48 ms /    20 runs   (   18.17 ms per token,    55.02 tokens per second)\n",
      "llama_print_timings:       total time =     560.27 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.58 ms /   108 runs   (    0.38 ms per token,  2661.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.77 ms /    88 tokens (    1.41 ms per token,   711.01 tokens per second)\n",
      "llama_print_timings:        eval time =    1822.04 ms /   107 runs   (   17.03 ms per token,    58.73 tokens per second)\n",
      "llama_print_timings:       total time =    2323.34 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      61.23 ms /   166 runs   (    0.37 ms per token,  2711.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     444.82 ms /   628 tokens (    0.71 ms per token,  1411.79 tokens per second)\n",
      "llama_print_timings:        eval time =    2982.35 ms /   165 runs   (   18.07 ms per token,    55.33 tokens per second)\n",
      "llama_print_timings:       total time =    4022.88 ms /   793 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.73 ms /    23 runs   (    0.38 ms per token,  2634.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.37 ms /    71 tokens (    1.55 ms per token,   643.28 tokens per second)\n",
      "llama_print_timings:        eval time =     407.52 ms /    22 runs   (   18.52 ms per token,    53.98 tokens per second)\n",
      "llama_print_timings:       total time =     609.24 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      22.11 ms /    58 runs   (    0.38 ms per token,  2623.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.99 ms /    56 tokens (    1.89 ms per token,   528.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1001.92 ms /    57 runs   (   17.58 ms per token,    56.89 tokens per second)\n",
      "llama_print_timings:       total time =    1312.24 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.35 ms /    17 runs   (    0.37 ms per token,  2675.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.90 ms /   112 tokens (    1.12 ms per token,   889.62 tokens per second)\n",
      "llama_print_timings:        eval time =     292.37 ms /    16 runs   (   18.27 ms per token,    54.73 tokens per second)\n",
      "llama_print_timings:       total time =     480.82 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      16.74 ms /    44 runs   (    0.38 ms per token,  2629.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.16 ms /    91 tokens (    1.25 ms per token,   797.16 tokens per second)\n",
      "llama_print_timings:        eval time =     764.20 ms /    43 runs   (   17.77 ms per token,    56.27 tokens per second)\n",
      "llama_print_timings:       total time =    1031.77 ms /   134 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.37 ms /   134 runs   (    0.37 ms per token,  2714.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.69 ms /   137 tokens (    1.06 ms per token,   940.39 tokens per second)\n",
      "llama_print_timings:        eval time =    2374.34 ms /   133 runs   (   17.85 ms per token,    56.02 tokens per second)\n",
      "llama_print_timings:       total time =    2996.46 ms /   270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.57 ms /    22 runs   (    0.39 ms per token,  2567.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     115.83 ms /   102 tokens (    1.14 ms per token,   880.64 tokens per second)\n",
      "llama_print_timings:        eval time =     364.82 ms /    21 runs   (   17.37 ms per token,    57.56 tokens per second)\n",
      "llama_print_timings:       total time =     560.67 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      56.41 ms /   153 runs   (    0.37 ms per token,  2712.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.10 ms /   301 tokens (    0.72 ms per token,  1392.87 tokens per second)\n",
      "llama_print_timings:        eval time =    2756.15 ms /   152 runs   (   18.13 ms per token,    55.15 tokens per second)\n",
      "llama_print_timings:       total time =    3522.52 ms /   453 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.68 ms /   135 runs   (    0.38 ms per token,  2663.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     212.33 ms /   280 tokens (    0.76 ms per token,  1318.73 tokens per second)\n",
      "llama_print_timings:        eval time =    2302.38 ms /   134 runs   (   17.18 ms per token,    58.20 tokens per second)\n",
      "llama_print_timings:       total time =    2995.79 ms /   414 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      66.34 ms /   176 runs   (    0.38 ms per token,  2652.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     220.23 ms /   344 tokens (    0.64 ms per token,  1561.98 tokens per second)\n",
      "llama_print_timings:        eval time =    3142.42 ms /   175 runs   (   17.96 ms per token,    55.69 tokens per second)\n",
      "llama_print_timings:       total time =    3993.60 ms /   519 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.44 ms /    20 runs   (    0.37 ms per token,  2687.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.17 ms /    93 tokens (    1.29 ms per token,   773.89 tokens per second)\n",
      "llama_print_timings:        eval time =     339.44 ms /    19 runs   (   17.87 ms per token,    55.97 tokens per second)\n",
      "llama_print_timings:       total time =     529.80 ms /   112 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.65 ms /    23 runs   (    0.38 ms per token,  2658.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =      99.92 ms /    56 tokens (    1.78 ms per token,   560.44 tokens per second)\n",
      "llama_print_timings:        eval time =     397.43 ms /    22 runs   (   18.07 ms per token,    55.36 tokens per second)\n",
      "llama_print_timings:       total time =     580.01 ms /    78 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      25.82 ms /    70 runs   (    0.37 ms per token,  2710.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     416.63 ms /   581 tokens (    0.72 ms per token,  1394.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1261.85 ms /    69 runs   (   18.29 ms per token,    54.68 tokens per second)\n",
      "llama_print_timings:       total time =    1923.19 ms /   650 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.16 ms /    92 runs   (    0.37 ms per token,  2693.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     113.57 ms /    50 tokens (    2.27 ms per token,   440.25 tokens per second)\n",
      "llama_print_timings:        eval time =    1603.44 ms /    91 runs   (   17.62 ms per token,    56.75 tokens per second)\n",
      "llama_print_timings:       total time =    2040.41 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2640.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.72 ms /    51 tokens (    2.13 ms per token,   469.11 tokens per second)\n",
      "llama_print_timings:        eval time =      36.69 ms /     2 runs   (   18.34 ms per token,    54.51 tokens per second)\n",
      "llama_print_timings:       total time =     156.54 ms /    53 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.92 ms /    22 runs   (    0.36 ms per token,  2777.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     104.33 ms /    49 tokens (    2.13 ms per token,   469.67 tokens per second)\n",
      "llama_print_timings:        eval time =     362.62 ms /    21 runs   (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:       total time =     542.21 ms /    70 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /     3 runs   (    0.38 ms per token,  2657.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.35 ms /    60 tokens (    1.81 ms per token,   553.78 tokens per second)\n",
      "llama_print_timings:        eval time =      37.42 ms /     2 runs   (   18.71 ms per token,    53.44 tokens per second)\n",
      "llama_print_timings:       total time =     156.61 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.45 ms /    97 runs   (    0.38 ms per token,  2661.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     164.28 ms /   164 tokens (    1.00 ms per token,   998.27 tokens per second)\n",
      "llama_print_timings:        eval time =    1692.16 ms /    96 runs   (   17.63 ms per token,    56.73 tokens per second)\n",
      "llama_print_timings:       total time =    2197.20 ms /   260 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.88 ms /   126 runs   (    0.37 ms per token,  2687.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.11 ms /   138 tokens (    1.04 ms per token,   957.58 tokens per second)\n",
      "llama_print_timings:        eval time =    2224.66 ms /   125 runs   (   17.80 ms per token,    56.19 tokens per second)\n",
      "llama_print_timings:       total time =    2821.67 ms /   263 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.85 ms /    23 runs   (    0.38 ms per token,  2598.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.05 ms /    79 tokens (    1.53 ms per token,   652.64 tokens per second)\n",
      "llama_print_timings:        eval time =     390.68 ms /    22 runs   (   17.76 ms per token,    56.31 tokens per second)\n",
      "llama_print_timings:       total time =     593.33 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2643.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.70 ms /   146 tokens (    0.96 ms per token,  1045.10 tokens per second)\n",
      "llama_print_timings:        eval time =      33.80 ms /     2 runs   (   16.90 ms per token,    59.17 tokens per second)\n",
      "llama_print_timings:       total time =     184.27 ms /   148 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.20 ms /   103 runs   (    0.37 ms per token,  2696.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.90 ms /    41 tokens (    2.63 ms per token,   379.99 tokens per second)\n",
      "llama_print_timings:        eval time =    1680.63 ms /   102 runs   (   16.48 ms per token,    60.69 tokens per second)\n",
      "llama_print_timings:       total time =    2142.88 ms /   143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.02 ms /    26 runs   (    0.39 ms per token,  2595.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     146.56 ms /   168 tokens (    0.87 ms per token,  1146.26 tokens per second)\n",
      "llama_print_timings:        eval time =     435.85 ms /    25 runs   (   17.43 ms per token,    57.36 tokens per second)\n",
      "llama_print_timings:       total time =     677.29 ms /   193 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.96 ms /    21 runs   (    0.38 ms per token,  2637.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.66 ms /    87 tokens (    1.55 ms per token,   646.10 tokens per second)\n",
      "llama_print_timings:        eval time =     400.26 ms /    20 runs   (   20.01 ms per token,    49.97 tokens per second)\n",
      "llama_print_timings:       total time =     610.16 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      26.98 ms /    78 runs   (    0.35 ms per token,  2890.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     113.96 ms /    72 tokens (    1.58 ms per token,   631.80 tokens per second)\n",
      "llama_print_timings:        eval time =    1362.01 ms /    77 runs   (   17.69 ms per token,    56.53 tokens per second)\n",
      "llama_print_timings:       total time =    1757.37 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.78 ms /    21 runs   (    0.37 ms per token,  2698.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.88 ms /   224 tokens (    0.77 ms per token,  1295.67 tokens per second)\n",
      "llama_print_timings:        eval time =     371.50 ms /    20 runs   (   18.57 ms per token,    53.84 tokens per second)\n",
      "llama_print_timings:       total time =     617.92 ms /   244 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.38 ms /    75 runs   (    0.38 ms per token,  2643.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.22 ms /   254 tokens (    0.71 ms per token,  1401.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1332.09 ms /    74 runs   (   18.00 ms per token,    55.55 tokens per second)\n",
      "llama_print_timings:       total time =    1775.95 ms /   328 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.61 ms /   126 runs   (    0.37 ms per token,  2703.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     143.18 ms /   143 tokens (    1.00 ms per token,   998.76 tokens per second)\n",
      "llama_print_timings:        eval time =    2230.46 ms /   125 runs   (   17.84 ms per token,    56.04 tokens per second)\n",
      "llama_print_timings:       total time =    2818.17 ms /   268 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      55.35 ms /   147 runs   (    0.38 ms per token,  2655.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     263.00 ms /   390 tokens (    0.67 ms per token,  1482.87 tokens per second)\n",
      "llama_print_timings:        eval time =    2677.01 ms /   146 runs   (   18.34 ms per token,    54.54 tokens per second)\n",
      "llama_print_timings:       total time =    3470.39 ms /   536 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.53 ms /   109 runs   (    0.37 ms per token,  2689.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     283.98 ms /   400 tokens (    0.71 ms per token,  1408.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1995.11 ms /   108 runs   (   18.47 ms per token,    54.13 tokens per second)\n",
      "llama_print_timings:       total time =    2669.27 ms /   508 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.50 ms /   108 runs   (    0.37 ms per token,  2666.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.77 ms /   176 tokens (    0.86 ms per token,  1159.63 tokens per second)\n",
      "llama_print_timings:        eval time =    1849.81 ms /   107 runs   (   17.29 ms per token,    57.84 tokens per second)\n",
      "llama_print_timings:       total time =    2379.10 ms /   283 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.09 ms /   105 runs   (    0.37 ms per token,  2686.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.30 ms /   169 tokens (    0.90 ms per token,  1109.65 tokens per second)\n",
      "llama_print_timings:        eval time =    1856.32 ms /   104 runs   (   17.85 ms per token,    56.02 tokens per second)\n",
      "llama_print_timings:       total time =    2378.02 ms /   273 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.87 ms /    90 runs   (    0.38 ms per token,  2657.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.52 ms /   176 tokens (    0.96 ms per token,  1044.41 tokens per second)\n",
      "llama_print_timings:        eval time =    1603.10 ms /    89 runs   (   18.01 ms per token,    55.52 tokens per second)\n",
      "llama_print_timings:       total time =    2086.89 ms /   265 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.81 ms /   106 runs   (    0.37 ms per token,  2731.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.34 ms /   150 tokens (    1.01 ms per token,   991.18 tokens per second)\n",
      "llama_print_timings:        eval time =    1842.30 ms /   105 runs   (   17.55 ms per token,    56.99 tokens per second)\n",
      "llama_print_timings:       total time =    2366.94 ms /   255 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.44 ms /    17 runs   (    0.38 ms per token,  2639.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.16 ms /   209 tokens (    0.78 ms per token,  1280.92 tokens per second)\n",
      "llama_print_timings:        eval time =     308.75 ms /    16 runs   (   19.30 ms per token,    51.82 tokens per second)\n",
      "llama_print_timings:       total time =     532.17 ms /   225 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.60 ms /   124 runs   (    0.36 ms per token,  2780.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.84 ms /   114 tokens (    1.10 ms per token,   913.16 tokens per second)\n",
      "llama_print_timings:        eval time =    2170.40 ms /   123 runs   (   17.65 ms per token,    56.67 tokens per second)\n",
      "llama_print_timings:       total time =    2731.40 ms /   237 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.80 ms /    91 runs   (    0.37 ms per token,  2692.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.78 ms /   175 tokens (    0.94 ms per token,  1068.53 tokens per second)\n",
      "llama_print_timings:        eval time =    1632.51 ms /    90 runs   (   18.14 ms per token,    55.13 tokens per second)\n",
      "llama_print_timings:       total time =    2117.49 ms /   265 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      88.05 ms /   239 runs   (    0.37 ms per token,  2714.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     465.10 ms /   675 tokens (    0.69 ms per token,  1451.30 tokens per second)\n",
      "llama_print_timings:        eval time =    4331.15 ms /   238 runs   (   18.20 ms per token,    54.95 tokens per second)\n",
      "llama_print_timings:       total time =    5675.85 ms /   913 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      24.38 ms /    64 runs   (    0.38 ms per token,  2625.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.00 ms /   100 tokens (    1.16 ms per token,   862.05 tokens per second)\n",
      "llama_print_timings:        eval time =    1095.52 ms /    63 runs   (   17.39 ms per token,    57.51 tokens per second)\n",
      "llama_print_timings:       total time =    1435.59 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     3 runs   (    0.39 ms per token,  2579.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.52 ms /    92 tokens (    1.24 ms per token,   803.38 tokens per second)\n",
      "llama_print_timings:        eval time =      37.96 ms /     2 runs   (   18.98 ms per token,    52.69 tokens per second)\n",
      "llama_print_timings:       total time =     163.22 ms /    94 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.92 ms /    28 runs   (    0.39 ms per token,  2564.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.66 ms /    88 tokens (    1.42 ms per token,   705.91 tokens per second)\n",
      "llama_print_timings:        eval time =     474.10 ms /    27 runs   (   17.56 ms per token,    56.95 tokens per second)\n",
      "llama_print_timings:       total time =     696.04 ms /   115 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.10 ms /     3 runs   (    0.37 ms per token,  2714.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     106.14 ms /    51 tokens (    2.08 ms per token,   480.47 tokens per second)\n",
      "llama_print_timings:        eval time =      36.52 ms /     2 runs   (   18.26 ms per token,    54.76 tokens per second)\n",
      "llama_print_timings:       total time =     153.66 ms /    53 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.95 ms /    99 runs   (    0.37 ms per token,  2679.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.37 ms /   209 tokens (    0.78 ms per token,  1279.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1766.44 ms /    98 runs   (   18.02 ms per token,    55.48 tokens per second)\n",
      "llama_print_timings:       total time =    2277.03 ms /   307 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.90 ms /    21 runs   (    0.38 ms per token,  2658.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     113.96 ms /    55 tokens (    2.07 ms per token,   482.63 tokens per second)\n",
      "llama_print_timings:        eval time =     362.37 ms /    20 runs   (   18.12 ms per token,    55.19 tokens per second)\n",
      "llama_print_timings:       total time =     553.95 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.66 ms /   134 runs   (    0.37 ms per token,  2698.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.45 ms /    77 tokens (    1.40 ms per token,   716.63 tokens per second)\n",
      "llama_print_timings:        eval time =    2306.36 ms /   133 runs   (   17.34 ms per token,    57.67 tokens per second)\n",
      "llama_print_timings:       total time =    2891.64 ms /   210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.38 ms /    92 runs   (    0.37 ms per token,  2675.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     249.66 ms /   327 tokens (    0.76 ms per token,  1309.78 tokens per second)\n",
      "llama_print_timings:        eval time =    1659.47 ms /    91 runs   (   18.24 ms per token,    54.84 tokens per second)\n",
      "llama_print_timings:       total time =    2230.99 ms /   418 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2631.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.33 ms /   139 tokens (    1.13 ms per token,   883.48 tokens per second)\n",
      "llama_print_timings:        eval time =      38.29 ms /     2 runs   (   19.14 ms per token,    52.24 tokens per second)\n",
      "llama_print_timings:       total time =     206.73 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.37 ms /    90 runs   (    0.38 ms per token,  2618.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     174.10 ms /   184 tokens (    0.95 ms per token,  1056.86 tokens per second)\n",
      "llama_print_timings:        eval time =    1616.85 ms /    89 runs   (   18.17 ms per token,    55.05 tokens per second)\n",
      "llama_print_timings:       total time =    2106.99 ms /   273 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.39 ms /   124 runs   (    0.37 ms per token,  2732.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.88 ms /   204 tokens (    0.83 ms per token,  1207.93 tokens per second)\n",
      "llama_print_timings:        eval time =    2132.87 ms /   123 runs   (   17.34 ms per token,    57.67 tokens per second)\n",
      "llama_print_timings:       total time =    2736.90 ms /   327 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      51.72 ms /   138 runs   (    0.37 ms per token,  2668.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     234.91 ms /   382 tokens (    0.61 ms per token,  1626.16 tokens per second)\n",
      "llama_print_timings:        eval time =    2456.88 ms /   137 runs   (   17.93 ms per token,    55.76 tokens per second)\n",
      "llama_print_timings:       total time =    3182.24 ms /   519 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.49 ms /    21 runs   (    0.40 ms per token,  2473.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.84 ms /    93 tokens (    1.18 ms per token,   846.69 tokens per second)\n",
      "llama_print_timings:        eval time =     371.14 ms /    20 runs   (   18.56 ms per token,    53.89 tokens per second)\n",
      "llama_print_timings:       total time =     557.57 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.18 ms /   146 runs   (    0.36 ms per token,  2797.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     231.88 ms /   363 tokens (    0.64 ms per token,  1565.44 tokens per second)\n",
      "llama_print_timings:        eval time =    2630.87 ms /   145 runs   (   18.14 ms per token,    55.11 tokens per second)\n",
      "llama_print_timings:       total time =    3380.71 ms /   508 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.21 ms /    22 runs   (    0.37 ms per token,  2678.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.90 ms /    55 tokens (    2.02 ms per token,   495.93 tokens per second)\n",
      "llama_print_timings:        eval time =     392.90 ms /    21 runs   (   18.71 ms per token,    53.45 tokens per second)\n",
      "llama_print_timings:       total time =     583.04 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.06 ms /     3 runs   (    0.35 ms per token,  2819.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.14 ms /    85 tokens (    1.40 ms per token,   713.42 tokens per second)\n",
      "llama_print_timings:        eval time =      38.48 ms /     2 runs   (   19.24 ms per token,    51.97 tokens per second)\n",
      "llama_print_timings:       total time =     167.73 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.16 ms /   100 runs   (    0.37 ms per token,  2690.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.70 ms /    74 tokens (    1.69 ms per token,   593.40 tokens per second)\n",
      "llama_print_timings:        eval time =    1726.18 ms /    99 runs   (   17.44 ms per token,    57.35 tokens per second)\n",
      "llama_print_timings:       total time =    2200.65 ms /   173 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      68.62 ms /   187 runs   (    0.37 ms per token,  2725.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     224.86 ms /   257 tokens (    0.87 ms per token,  1142.94 tokens per second)\n",
      "llama_print_timings:        eval time =    3337.68 ms /   186 runs   (   17.94 ms per token,    55.73 tokens per second)\n",
      "llama_print_timings:       total time =    4241.58 ms /   443 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      19.80 ms /    54 runs   (    0.37 ms per token,  2727.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     182.54 ms /    31 tokens (    5.89 ms per token,   169.83 tokens per second)\n",
      "llama_print_timings:        eval time =     919.57 ms /    53 runs   (   17.35 ms per token,    57.64 tokens per second)\n",
      "llama_print_timings:       total time =    1289.82 ms /    84 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.46 ms /    96 runs   (    0.37 ms per token,  2707.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.99 ms /   124 tokens (    0.97 ms per token,  1033.40 tokens per second)\n",
      "llama_print_timings:        eval time =    1613.30 ms /    95 runs   (   16.98 ms per token,    58.89 tokens per second)\n",
      "llama_print_timings:       total time =    2064.67 ms /   219 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2617.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.91 ms /   115 tokens (    1.09 ms per token,   920.66 tokens per second)\n",
      "llama_print_timings:        eval time =      38.53 ms /     2 runs   (   19.27 ms per token,    51.91 tokens per second)\n",
      "llama_print_timings:       total time =     173.61 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.65 ms /    76 runs   (    0.38 ms per token,  2652.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.90 ms /   174 tokens (    0.92 ms per token,  1081.40 tokens per second)\n",
      "llama_print_timings:        eval time =    1326.00 ms /    75 runs   (   17.68 ms per token,    56.56 tokens per second)\n",
      "llama_print_timings:       total time =    1749.29 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.44 ms /    17 runs   (    0.38 ms per token,  2641.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     100.76 ms /    52 tokens (    1.94 ms per token,   516.07 tokens per second)\n",
      "llama_print_timings:        eval time =     288.19 ms /    16 runs   (   18.01 ms per token,    55.52 tokens per second)\n",
      "llama_print_timings:       total time =     450.32 ms /    68 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.50 ms /   119 runs   (    0.37 ms per token,  2735.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.18 ms /   143 tokens (    1.09 ms per token,   921.52 tokens per second)\n",
      "llama_print_timings:        eval time =    2164.41 ms /   118 runs   (   18.34 ms per token,    54.52 tokens per second)\n",
      "llama_print_timings:       total time =    2755.45 ms /   261 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      12.83 ms /    33 runs   (    0.39 ms per token,  2571.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     252.53 ms /   337 tokens (    0.75 ms per token,  1334.47 tokens per second)\n",
      "llama_print_timings:        eval time =     578.52 ms /    32 runs   (   18.08 ms per token,    55.31 tokens per second)\n",
      "llama_print_timings:       total time =     946.99 ms /   369 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.24 ms /    84 runs   (    0.37 ms per token,  2689.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.36 ms /   177 tokens (    0.88 ms per token,  1132.00 tokens per second)\n",
      "llama_print_timings:        eval time =    1501.05 ms /    83 runs   (   18.08 ms per token,    55.29 tokens per second)\n",
      "llama_print_timings:       total time =    1951.47 ms /   260 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.19 ms /     3 runs   (    0.40 ms per token,  2516.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.00 ms /    94 tokens (    1.23 ms per token,   810.34 tokens per second)\n",
      "llama_print_timings:        eval time =      40.30 ms /     2 runs   (   20.15 ms per token,    49.63 tokens per second)\n",
      "llama_print_timings:       total time =     166.37 ms /    96 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.13 ms /   115 runs   (    0.37 ms per token,  2729.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.33 ms /    82 tokens (    1.53 ms per token,   654.27 tokens per second)\n",
      "llama_print_timings:        eval time =    1990.75 ms /   114 runs   (   17.46 ms per token,    57.26 tokens per second)\n",
      "llama_print_timings:       total time =    2515.49 ms /   196 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     3 runs   (    0.39 ms per token,  2597.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.41 ms /   104 tokens (    1.14 ms per token,   878.30 tokens per second)\n",
      "llama_print_timings:        eval time =      42.90 ms /     2 runs   (   21.45 ms per token,    46.62 tokens per second)\n",
      "llama_print_timings:       total time =     172.67 ms /   106 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2633.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     127.37 ms /   112 tokens (    1.14 ms per token,   879.35 tokens per second)\n",
      "llama_print_timings:        eval time =      39.43 ms /     2 runs   (   19.71 ms per token,    50.73 tokens per second)\n",
      "llama_print_timings:       total time =     178.15 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2553.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.19 ms /    87 tokens (    1.42 ms per token,   706.24 tokens per second)\n",
      "llama_print_timings:        eval time =      41.17 ms /     2 runs   (   20.58 ms per token,    48.58 tokens per second)\n",
      "llama_print_timings:       total time =     174.50 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      77.41 ms /   212 runs   (    0.37 ms per token,  2738.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     438.75 ms /   616 tokens (    0.71 ms per token,  1403.99 tokens per second)\n",
      "llama_print_timings:        eval time =    3895.65 ms /   211 runs   (   18.46 ms per token,    54.16 tokens per second)\n",
      "llama_print_timings:       total time =    5119.24 ms /   827 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.58 ms /    17 runs   (    0.39 ms per token,  2584.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.05 ms /   194 tokens (    0.80 ms per token,  1243.17 tokens per second)\n",
      "llama_print_timings:        eval time =     299.42 ms /    16 runs   (   18.71 ms per token,    53.44 tokens per second)\n",
      "llama_print_timings:       total time =     516.38 ms /   210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      87.71 ms /   238 runs   (    0.37 ms per token,  2713.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     288.41 ms /   437 tokens (    0.66 ms per token,  1515.23 tokens per second)\n",
      "llama_print_timings:        eval time =    4459.51 ms /   237 runs   (   18.82 ms per token,    53.14 tokens per second)\n",
      "llama_print_timings:       total time =    5634.02 ms /   674 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      94.21 ms /   256 runs   (    0.37 ms per token,  2717.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.33 ms /   480 tokens (    0.58 ms per token,  1718.39 tokens per second)\n",
      "llama_print_timings:        eval time =    4732.42 ms /   255 runs   (   18.56 ms per token,    53.88 tokens per second)\n",
      "llama_print_timings:       total time =    5956.16 ms /   735 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.48 ms /    17 runs   (    0.38 ms per token,  2622.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.09 ms /   120 tokens (    0.99 ms per token,  1007.65 tokens per second)\n",
      "llama_print_timings:        eval time =     291.70 ms /    16 runs   (   18.23 ms per token,    54.85 tokens per second)\n",
      "llama_print_timings:       total time =     472.71 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.09 ms /     3 runs   (    0.36 ms per token,  2739.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.21 ms /    85 tokens (    1.40 ms per token,   713.04 tokens per second)\n",
      "llama_print_timings:        eval time =      38.17 ms /     2 runs   (   19.08 ms per token,    52.40 tokens per second)\n",
      "llama_print_timings:       total time =     167.62 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.22 ms /    21 runs   (    0.39 ms per token,  2554.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     111.82 ms /    47 tokens (    2.38 ms per token,   420.31 tokens per second)\n",
      "llama_print_timings:        eval time =     368.87 ms /    20 runs   (   18.44 ms per token,    54.22 tokens per second)\n",
      "llama_print_timings:       total time =     563.71 ms /    67 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.52 ms /   119 runs   (    0.37 ms per token,  2734.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.43 ms /   122 tokens (    1.01 ms per token,   988.39 tokens per second)\n",
      "llama_print_timings:        eval time =    2049.66 ms /   118 runs   (   17.37 ms per token,    57.57 tokens per second)\n",
      "llama_print_timings:       total time =    2606.10 ms /   240 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2640.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.14 ms /    42 tokens (    3.12 ms per token,   320.28 tokens per second)\n",
      "llama_print_timings:        eval time =      47.44 ms /     2 runs   (   23.72 ms per token,    42.16 tokens per second)\n",
      "llama_print_timings:       total time =     190.11 ms /    44 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.74 ms /    89 runs   (    0.37 ms per token,  2718.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     136.51 ms /   104 tokens (    1.31 ms per token,   761.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1467.56 ms /    88 runs   (   16.68 ms per token,    59.96 tokens per second)\n",
      "llama_print_timings:       total time =    1911.65 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      70.88 ms /   192 runs   (    0.37 ms per token,  2708.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     153.05 ms /   221 tokens (    0.69 ms per token,  1443.97 tokens per second)\n",
      "llama_print_timings:        eval time =    3469.40 ms /   191 runs   (   18.16 ms per token,    55.05 tokens per second)\n",
      "llama_print_timings:       total time =    4326.88 ms /   412 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.48 ms /    81 runs   (    0.38 ms per token,  2657.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.21 ms /   172 tokens (    0.97 ms per token,  1034.82 tokens per second)\n",
      "llama_print_timings:        eval time =    1463.77 ms /    80 runs   (   18.30 ms per token,    54.65 tokens per second)\n",
      "llama_print_timings:       total time =    1922.89 ms /   252 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.52 ms /   101 runs   (    0.37 ms per token,  2691.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     153.31 ms /   196 tokens (    0.78 ms per token,  1278.50 tokens per second)\n",
      "llama_print_timings:        eval time =    1788.29 ms /   100 runs   (   17.88 ms per token,    55.92 tokens per second)\n",
      "llama_print_timings:       total time =    2297.43 ms /   296 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.95 ms /    83 runs   (    0.37 ms per token,  2681.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     104.94 ms /    37 tokens (    2.84 ms per token,   352.58 tokens per second)\n",
      "llama_print_timings:        eval time =    1418.94 ms /    82 runs   (   17.30 ms per token,    57.79 tokens per second)\n",
      "llama_print_timings:       total time =    1815.06 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.37 ms /   135 runs   (    0.36 ms per token,  2790.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.22 ms /   224 tokens (    0.74 ms per token,  1355.75 tokens per second)\n",
      "llama_print_timings:        eval time =    2434.07 ms /   134 runs   (   18.16 ms per token,    55.05 tokens per second)\n",
      "llama_print_timings:       total time =    3084.52 ms /   358 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.05 ms /    97 runs   (    0.36 ms per token,  2767.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.43 ms /    98 tokens (    1.15 ms per token,   871.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1629.08 ms /    96 runs   (   16.97 ms per token,    58.93 tokens per second)\n",
      "llama_print_timings:       total time =    2080.90 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.37 ms /    17 runs   (    0.37 ms per token,  2668.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.23 ms /    98 tokens (    1.21 ms per token,   828.92 tokens per second)\n",
      "llama_print_timings:        eval time =     308.03 ms /    16 runs   (   19.25 ms per token,    51.94 tokens per second)\n",
      "llama_print_timings:       total time =     487.28 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.73 ms /   106 runs   (    0.37 ms per token,  2736.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.26 ms /    80 tokens (    1.38 ms per token,   725.55 tokens per second)\n",
      "llama_print_timings:        eval time =    1851.38 ms /   105 runs   (   17.63 ms per token,    56.71 tokens per second)\n",
      "llama_print_timings:       total time =    2332.72 ms /   185 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.05 ms /    79 runs   (    0.38 ms per token,  2628.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.57 ms /   122 tokens (    1.02 ms per token,   979.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1376.00 ms /    78 runs   (   17.64 ms per token,    56.69 tokens per second)\n",
      "llama_print_timings:       total time =    1778.75 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.07 ms /    89 runs   (    0.37 ms per token,  2691.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     515.10 ms /   757 tokens (    0.68 ms per token,  1469.62 tokens per second)\n",
      "llama_print_timings:        eval time =    1690.94 ms /    88 runs   (   19.22 ms per token,    52.04 tokens per second)\n",
      "llama_print_timings:       total time =    2521.42 ms /   845 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.63 ms /    17 runs   (    0.39 ms per token,  2563.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     129.29 ms /   117 tokens (    1.11 ms per token,   904.91 tokens per second)\n",
      "llama_print_timings:        eval time =     293.57 ms /    16 runs   (   18.35 ms per token,    54.50 tokens per second)\n",
      "llama_print_timings:       total time =     485.85 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.24 ms /    91 runs   (    0.37 ms per token,  2737.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.08 ms /    91 tokens (    1.37 ms per token,   727.53 tokens per second)\n",
      "llama_print_timings:        eval time =    1591.90 ms /    90 runs   (   17.69 ms per token,    56.54 tokens per second)\n",
      "llama_print_timings:       total time =    2037.06 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.91 ms /    86 runs   (    0.37 ms per token,  2695.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.23 ms /   330 tokens (    0.66 ms per token,  1505.27 tokens per second)\n",
      "llama_print_timings:        eval time =    1554.97 ms /    85 runs   (   18.29 ms per token,    54.66 tokens per second)\n",
      "llama_print_timings:       total time =    2079.93 ms /   415 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.75 ms /    18 runs   (    0.43 ms per token,  2321.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.19 ms /    93 tokens (    1.27 ms per token,   786.86 tokens per second)\n",
      "llama_print_timings:        eval time =     300.22 ms /    17 runs   (   17.66 ms per token,    56.63 tokens per second)\n",
      "llama_print_timings:       total time =     495.63 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.05 ms /     3 runs   (    0.35 ms per token,  2857.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.61 ms /    93 tokens (    1.30 ms per token,   771.06 tokens per second)\n",
      "llama_print_timings:        eval time =      38.33 ms /     2 runs   (   19.16 ms per token,    52.18 tokens per second)\n",
      "llama_print_timings:       total time =     169.41 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      57.85 ms /   156 runs   (    0.37 ms per token,  2696.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     169.11 ms /   147 tokens (    1.15 ms per token,   869.26 tokens per second)\n",
      "llama_print_timings:        eval time =    2764.58 ms /   155 runs   (   17.84 ms per token,    56.07 tokens per second)\n",
      "llama_print_timings:       total time =    3498.47 ms /   302 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.40 ms /   139 runs   (    0.38 ms per token,  2652.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     153.56 ms /   203 tokens (    0.76 ms per token,  1321.96 tokens per second)\n",
      "llama_print_timings:        eval time =    2403.08 ms /   138 runs   (   17.41 ms per token,    57.43 tokens per second)\n",
      "llama_print_timings:       total time =    3049.02 ms /   341 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      67.82 ms /   185 runs   (    0.37 ms per token,  2727.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     490.93 ms /   758 tokens (    0.65 ms per token,  1544.01 tokens per second)\n",
      "llama_print_timings:        eval time =    3417.25 ms /   184 runs   (   18.57 ms per token,    53.84 tokens per second)\n",
      "llama_print_timings:       total time =    4578.99 ms /   942 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.17 ms /     3 runs   (    0.39 ms per token,  2566.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.68 ms /   112 tokens (    1.07 ms per token,   935.81 tokens per second)\n",
      "llama_print_timings:        eval time =      35.17 ms /     2 runs   (   17.59 ms per token,    56.86 tokens per second)\n",
      "llama_print_timings:       total time =     166.02 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      58.04 ms /   158 runs   (    0.37 ms per token,  2722.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.86 ms /   178 tokens (    0.91 ms per token,  1099.70 tokens per second)\n",
      "llama_print_timings:        eval time =    2828.33 ms /   157 runs   (   18.01 ms per token,    55.51 tokens per second)\n",
      "llama_print_timings:       total time =    3559.41 ms /   335 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.47 ms /    89 runs   (    0.36 ms per token,  2741.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     245.16 ms /   323 tokens (    0.76 ms per token,  1317.51 tokens per second)\n",
      "llama_print_timings:        eval time =    1580.64 ms /    88 runs   (   17.96 ms per token,    55.67 tokens per second)\n",
      "llama_print_timings:       total time =    2137.86 ms /   411 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      58.41 ms /   157 runs   (    0.37 ms per token,  2687.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.62 ms /   209 tokens (    0.78 ms per token,  1285.20 tokens per second)\n",
      "llama_print_timings:        eval time =    2836.35 ms /   156 runs   (   18.18 ms per token,    55.00 tokens per second)\n",
      "llama_print_timings:       total time =    3567.44 ms /   365 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.08 ms /     3 runs   (    0.36 ms per token,  2777.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.18 ms /    78 tokens (    1.54 ms per token,   649.03 tokens per second)\n",
      "llama_print_timings:        eval time =      37.20 ms /     2 runs   (   18.60 ms per token,    53.77 tokens per second)\n",
      "llama_print_timings:       total time =     168.16 ms /    80 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      21.73 ms /    59 runs   (    0.37 ms per token,  2714.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     101.09 ms /    59 tokens (    1.71 ms per token,   583.63 tokens per second)\n",
      "llama_print_timings:        eval time =     968.87 ms /    58 runs   (   16.70 ms per token,    59.86 tokens per second)\n",
      "llama_print_timings:       total time =    1273.71 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.99 ms /    29 runs   (    0.38 ms per token,  2638.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.44 ms /   109 tokens (    0.99 ms per token,  1005.21 tokens per second)\n",
      "llama_print_timings:        eval time =     489.82 ms /    28 runs   (   17.49 ms per token,    57.16 tokens per second)\n",
      "llama_print_timings:       total time =     700.35 ms /   137 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.15 ms /   100 runs   (    0.37 ms per token,  2691.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.83 ms /   191 tokens (    0.82 ms per token,  1225.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1804.39 ms /    99 runs   (   18.23 ms per token,    54.87 tokens per second)\n",
      "llama_print_timings:       total time =    2310.66 ms /   290 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.32 ms /   103 runs   (    0.36 ms per token,  2759.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.52 ms /   100 tokens (    1.19 ms per token,   843.76 tokens per second)\n",
      "llama_print_timings:        eval time =    1788.25 ms /   102 runs   (   17.53 ms per token,    57.04 tokens per second)\n",
      "llama_print_timings:       total time =    2272.35 ms /   202 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.11 ms /   118 runs   (    0.37 ms per token,  2737.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     164.06 ms /   185 tokens (    0.89 ms per token,  1127.63 tokens per second)\n",
      "llama_print_timings:        eval time =    2101.41 ms /   117 runs   (   17.96 ms per token,    55.68 tokens per second)\n",
      "llama_print_timings:       total time =    2682.92 ms /   302 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.47 ms /   110 runs   (    0.37 ms per token,  2717.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     177.30 ms /   211 tokens (    0.84 ms per token,  1190.11 tokens per second)\n",
      "llama_print_timings:        eval time =    1988.17 ms /   109 runs   (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:       total time =    2552.66 ms /   320 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.16 ms /    99 runs   (    0.37 ms per token,  2737.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     243.40 ms /   306 tokens (    0.80 ms per token,  1257.18 tokens per second)\n",
      "llama_print_timings:        eval time =    1787.91 ms /    98 runs   (   18.24 ms per token,    54.81 tokens per second)\n",
      "llama_print_timings:       total time =    2380.30 ms /   404 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      57.60 ms /   158 runs   (    0.36 ms per token,  2743.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.89 ms /   328 tokens (    0.66 ms per token,  1519.31 tokens per second)\n",
      "llama_print_timings:        eval time =    2795.24 ms /   157 runs   (   17.80 ms per token,    56.17 tokens per second)\n",
      "llama_print_timings:       total time =    3574.01 ms /   485 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.19 ms /     3 runs   (    0.40 ms per token,  2527.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.78 ms /   103 tokens (    1.11 ms per token,   897.40 tokens per second)\n",
      "llama_print_timings:        eval time =      37.80 ms /     2 runs   (   18.90 ms per token,    52.91 tokens per second)\n",
      "llama_print_timings:       total time =     164.45 ms /   105 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.38 ms /    17 runs   (    0.38 ms per token,  2662.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.96 ms /   136 tokens (    1.12 ms per token,   894.98 tokens per second)\n",
      "llama_print_timings:        eval time =     282.88 ms /    16 runs   (   17.68 ms per token,    56.56 tokens per second)\n",
      "llama_print_timings:       total time =     492.13 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.16 ms /    83 runs   (    0.36 ms per token,  2751.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.95 ms /   126 tokens (    0.98 ms per token,  1024.84 tokens per second)\n",
      "llama_print_timings:        eval time =    1448.57 ms /    82 runs   (   17.67 ms per token,    56.61 tokens per second)\n",
      "llama_print_timings:       total time =    1860.68 ms /   208 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2606.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     115.72 ms /    90 tokens (    1.29 ms per token,   777.73 tokens per second)\n",
      "llama_print_timings:        eval time =      35.57 ms /     2 runs   (   17.79 ms per token,    56.23 tokens per second)\n",
      "llama_print_timings:       total time =     162.09 ms /    92 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.83 ms /   106 runs   (    0.38 ms per token,  2661.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.62 ms /   236 tokens (    0.68 ms per token,  1478.47 tokens per second)\n",
      "llama_print_timings:        eval time =    1903.36 ms /   105 runs   (   18.13 ms per token,    55.17 tokens per second)\n",
      "llama_print_timings:       total time =    2440.53 ms /   341 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    17 runs   (    0.38 ms per token,  2605.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.46 ms /    46 tokens (    2.34 ms per token,   428.08 tokens per second)\n",
      "llama_print_timings:        eval time =     295.16 ms /    16 runs   (   18.45 ms per token,    54.21 tokens per second)\n",
      "llama_print_timings:       total time =     465.17 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.41 ms /   121 runs   (    0.38 ms per token,  2607.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     127.94 ms /    95 tokens (    1.35 ms per token,   742.53 tokens per second)\n",
      "llama_print_timings:        eval time =    2149.59 ms /   120 runs   (   17.91 ms per token,    55.82 tokens per second)\n",
      "llama_print_timings:       total time =    2718.06 ms /   215 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.41 ms /    98 runs   (    0.38 ms per token,  2619.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.16 ms /   103 tokens (    1.21 ms per token,   829.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1701.31 ms /    97 runs   (   17.54 ms per token,    57.01 tokens per second)\n",
      "llama_print_timings:       total time =    2171.04 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      51.07 ms /   130 runs   (    0.39 ms per token,  2545.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.15 ms /   143 tokens (    1.01 ms per token,   992.03 tokens per second)\n",
      "llama_print_timings:        eval time =    2427.20 ms /   129 runs   (   18.82 ms per token,    53.15 tokens per second)\n",
      "llama_print_timings:       total time =    3075.80 ms /   272 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.51 ms /   116 runs   (    0.38 ms per token,  2665.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.32 ms /   102 tokens (    1.18 ms per token,   847.73 tokens per second)\n",
      "llama_print_timings:        eval time =    2029.32 ms /   115 runs   (   17.65 ms per token,    56.67 tokens per second)\n",
      "llama_print_timings:       total time =    2581.44 ms /   217 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      66.78 ms /   181 runs   (    0.37 ms per token,  2710.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.45 ms /   205 tokens (    0.74 ms per token,  1344.74 tokens per second)\n",
      "llama_print_timings:        eval time =    3259.55 ms /   180 runs   (   18.11 ms per token,    55.22 tokens per second)\n",
      "llama_print_timings:       total time =    4085.25 ms /   385 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.74 ms /   107 runs   (    0.37 ms per token,  2692.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     104.72 ms /    64 tokens (    1.64 ms per token,   611.13 tokens per second)\n",
      "llama_print_timings:        eval time =    1842.21 ms /   106 runs   (   17.38 ms per token,    57.54 tokens per second)\n",
      "llama_print_timings:       total time =    2321.59 ms /   170 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.91 ms /    21 runs   (    0.38 ms per token,  2653.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     113.90 ms /    47 tokens (    2.42 ms per token,   412.65 tokens per second)\n",
      "llama_print_timings:        eval time =     351.02 ms /    20 runs   (   17.55 ms per token,    56.98 tokens per second)\n",
      "llama_print_timings:       total time =     540.01 ms /    67 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.91 ms /    21 runs   (    0.38 ms per token,  2655.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.40 ms /   121 tokens (    1.08 ms per token,   927.92 tokens per second)\n",
      "llama_print_timings:        eval time =     363.66 ms /    20 runs   (   18.18 ms per token,    55.00 tokens per second)\n",
      "llama_print_timings:       total time =     568.06 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.17 ms /   114 runs   (    0.36 ms per token,  2769.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.35 ms /   190 tokens (    0.83 ms per token,  1207.51 tokens per second)\n",
      "llama_print_timings:        eval time =    2026.66 ms /   113 runs   (   17.94 ms per token,    55.76 tokens per second)\n",
      "llama_print_timings:       total time =    2586.86 ms /   303 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.63 ms /    17 runs   (    0.39 ms per token,  2562.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.22 ms /    62 tokens (    1.84 ms per token,   542.81 tokens per second)\n",
      "llama_print_timings:        eval time =     287.74 ms /    16 runs   (   17.98 ms per token,    55.61 tokens per second)\n",
      "llama_print_timings:       total time =     463.16 ms /    78 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.42 ms /    17 runs   (    0.38 ms per token,  2647.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     101.68 ms /    55 tokens (    1.85 ms per token,   540.93 tokens per second)\n",
      "llama_print_timings:        eval time =     288.24 ms /    16 runs   (   18.02 ms per token,    55.51 tokens per second)\n",
      "llama_print_timings:       total time =     452.77 ms /    71 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      51.49 ms /   140 runs   (    0.37 ms per token,  2719.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     164.66 ms /   178 tokens (    0.93 ms per token,  1081.03 tokens per second)\n",
      "llama_print_timings:        eval time =    2518.02 ms /   139 runs   (   18.12 ms per token,    55.20 tokens per second)\n",
      "llama_print_timings:       total time =    3181.24 ms /   317 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.20 ms /    22 runs   (    0.37 ms per token,  2683.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.33 ms /    71 tokens (    1.71 ms per token,   585.18 tokens per second)\n",
      "llama_print_timings:        eval time =     403.19 ms /    21 runs   (   19.20 ms per token,    52.08 tokens per second)\n",
      "llama_print_timings:       total time =     601.79 ms /    92 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.57 ms /    23 runs   (    0.37 ms per token,  2683.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.84 ms /    43 tokens (    2.67 ms per token,   374.45 tokens per second)\n",
      "llama_print_timings:        eval time =     421.76 ms /    22 runs   (   19.17 ms per token,    52.16 tokens per second)\n",
      "llama_print_timings:       total time =     617.69 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    18 runs   (    0.36 ms per token,  2763.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.25 ms /   174 tokens (    0.99 ms per token,  1010.14 tokens per second)\n",
      "llama_print_timings:        eval time =     317.99 ms /    17 runs   (   18.71 ms per token,    53.46 tokens per second)\n",
      "llama_print_timings:       total time =     553.53 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.83 ms /   100 runs   (    0.38 ms per token,  2643.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     173.49 ms /   171 tokens (    1.01 ms per token,   985.64 tokens per second)\n",
      "llama_print_timings:        eval time =    1716.02 ms /    99 runs   (   17.33 ms per token,    57.69 tokens per second)\n",
      "llama_print_timings:       total time =    2238.49 ms /   270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.02 ms /    90 runs   (    0.37 ms per token,  2725.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     101.77 ms /    37 tokens (    2.75 ms per token,   363.58 tokens per second)\n",
      "llama_print_timings:        eval time =    1517.90 ms /    89 runs   (   17.06 ms per token,    58.63 tokens per second)\n",
      "llama_print_timings:       total time =    1933.12 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       0.93 ms /     3 runs   (    0.31 ms per token,  3229.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     167.71 ms /   184 tokens (    0.91 ms per token,  1097.13 tokens per second)\n",
      "llama_print_timings:        eval time =      38.78 ms /     2 runs   (   19.39 ms per token,    51.58 tokens per second)\n",
      "llama_print_timings:       total time =     216.28 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.54 ms /    17 runs   (    0.39 ms per token,  2597.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.81 ms /    60 tokens (    1.80 ms per token,   556.53 tokens per second)\n",
      "llama_print_timings:        eval time =     303.55 ms /    16 runs   (   18.97 ms per token,    52.71 tokens per second)\n",
      "llama_print_timings:       total time =     472.59 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.49 ms /    22 runs   (    0.39 ms per token,  2590.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.28 ms /   117 tokens (    1.16 ms per token,   864.85 tokens per second)\n",
      "llama_print_timings:        eval time =     392.62 ms /    21 runs   (   18.70 ms per token,    53.49 tokens per second)\n",
      "llama_print_timings:       total time =     607.78 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.62 ms /   110 runs   (    0.36 ms per token,  2776.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     180.40 ms /   192 tokens (    0.94 ms per token,  1064.28 tokens per second)\n",
      "llama_print_timings:        eval time =    1995.05 ms /   109 runs   (   18.30 ms per token,    54.64 tokens per second)\n",
      "llama_print_timings:       total time =    2557.93 ms /   301 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.52 ms /   122 runs   (    0.37 ms per token,  2680.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.93 ms /   112 tokens (    1.19 ms per token,   842.55 tokens per second)\n",
      "llama_print_timings:        eval time =    2141.89 ms /   121 runs   (   17.70 ms per token,    56.49 tokens per second)\n",
      "llama_print_timings:       total time =    2721.20 ms /   233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.34 ms /    17 runs   (    0.37 ms per token,  2683.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     167.67 ms /   191 tokens (    0.88 ms per token,  1139.12 tokens per second)\n",
      "llama_print_timings:        eval time =     293.35 ms /    16 runs   (   18.33 ms per token,    54.54 tokens per second)\n",
      "llama_print_timings:       total time =     523.10 ms /   207 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.53 ms /    17 runs   (    0.38 ms per token,  2603.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.47 ms /   103 tokens (    1.15 ms per token,   869.39 tokens per second)\n",
      "llama_print_timings:        eval time =     292.58 ms /    16 runs   (   18.29 ms per token,    54.69 tokens per second)\n",
      "llama_print_timings:       total time =     472.68 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.10 ms /     3 runs   (    0.37 ms per token,  2722.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.52 ms /    96 tokens (    1.28 ms per token,   783.57 tokens per second)\n",
      "llama_print_timings:        eval time =      37.76 ms /     2 runs   (   18.88 ms per token,    52.96 tokens per second)\n",
      "llama_print_timings:       total time =     170.71 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.57 ms /   109 runs   (    0.37 ms per token,  2686.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.52 ms /   154 tokens (    1.06 ms per token,   941.78 tokens per second)\n",
      "llama_print_timings:        eval time =    1917.83 ms /   108 runs   (   17.76 ms per token,    56.31 tokens per second)\n",
      "llama_print_timings:       total time =    2460.07 ms /   262 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.01 ms /   106 runs   (    0.38 ms per token,  2649.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.39 ms /   173 tokens (    0.97 ms per token,  1027.39 tokens per second)\n",
      "llama_print_timings:        eval time =    1898.48 ms /   105 runs   (   18.08 ms per token,    55.31 tokens per second)\n",
      "llama_print_timings:       total time =    2439.11 ms /   278 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.85 ms /    73 runs   (    0.38 ms per token,  2621.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     171.61 ms /   174 tokens (    0.99 ms per token,  1013.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1293.00 ms /    72 runs   (   17.96 ms per token,    55.68 tokens per second)\n",
      "llama_print_timings:       total time =    1717.77 ms /   246 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      74.81 ms /   201 runs   (    0.37 ms per token,  2686.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     246.27 ms /   353 tokens (    0.70 ms per token,  1433.40 tokens per second)\n",
      "llama_print_timings:        eval time =    3585.03 ms /   200 runs   (   17.93 ms per token,    55.79 tokens per second)\n",
      "llama_print_timings:       total time =    4554.96 ms /   553 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.99 ms /   132 runs   (    0.37 ms per token,  2694.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.21 ms /   154 tokens (    1.03 ms per token,   973.38 tokens per second)\n",
      "llama_print_timings:        eval time =    2390.39 ms /   131 runs   (   18.25 ms per token,    54.80 tokens per second)\n",
      "llama_print_timings:       total time =    3023.23 ms /   285 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      21.25 ms /    55 runs   (    0.39 ms per token,  2588.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.03 ms /    73 tokens (    1.69 ms per token,   593.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1002.73 ms /    54 runs   (   18.57 ms per token,    53.85 tokens per second)\n",
      "llama_print_timings:       total time =    1324.53 ms /   127 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.04 ms /   114 runs   (    0.38 ms per token,  2648.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.25 ms /    80 tokens (    1.45 ms per token,   688.16 tokens per second)\n",
      "llama_print_timings:        eval time =    2006.89 ms /   113 runs   (   17.76 ms per token,    56.31 tokens per second)\n",
      "llama_print_timings:       total time =    2522.46 ms /   193 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       0.92 ms /     3 runs   (    0.30 ms per token,  3278.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.11 ms /    98 tokens (    1.24 ms per token,   809.20 tokens per second)\n",
      "llama_print_timings:        eval time =      41.49 ms /     2 runs   (   20.75 ms per token,    48.20 tokens per second)\n",
      "llama_print_timings:       total time =     172.50 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.29 ms /   131 runs   (    0.37 ms per token,  2712.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     297.12 ms /   455 tokens (    0.65 ms per token,  1531.35 tokens per second)\n",
      "llama_print_timings:        eval time =    2446.83 ms /   130 runs   (   18.82 ms per token,    53.13 tokens per second)\n",
      "llama_print_timings:       total time =    3218.73 ms /   585 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.96 ms /    83 runs   (    0.37 ms per token,  2680.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.88 ms /   198 tokens (    0.80 ms per token,  1254.12 tokens per second)\n",
      "llama_print_timings:        eval time =    1448.19 ms /    82 runs   (   17.66 ms per token,    56.62 tokens per second)\n",
      "llama_print_timings:       total time =    1896.92 ms /   280 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.92 ms /   106 runs   (    0.38 ms per token,  2655.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.13 ms /   201 tokens (    0.84 ms per token,  1195.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1825.13 ms /   105 runs   (   17.38 ms per token,    57.53 tokens per second)\n",
      "llama_print_timings:       total time =    2365.64 ms /   306 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.79 ms /   106 runs   (    0.37 ms per token,  2733.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.05 ms /   201 tokens (    0.75 ms per token,  1330.69 tokens per second)\n",
      "llama_print_timings:        eval time =    1875.91 ms /   105 runs   (   17.87 ms per token,    55.97 tokens per second)\n",
      "llama_print_timings:       total time =    2401.52 ms /   306 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.37 ms /   120 runs   (    0.36 ms per token,  2767.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.29 ms /    92 tokens (    1.29 ms per token,   777.74 tokens per second)\n",
      "llama_print_timings:        eval time =    2127.66 ms /   119 runs   (   17.88 ms per token,    55.93 tokens per second)\n",
      "llama_print_timings:       total time =    2671.82 ms /   211 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.21 ms /    17 runs   (    0.37 ms per token,  2737.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.94 ms /    82 tokens (    1.40 ms per token,   713.43 tokens per second)\n",
      "llama_print_timings:        eval time =     302.56 ms /    16 runs   (   18.91 ms per token,    52.88 tokens per second)\n",
      "llama_print_timings:       total time =     478.58 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.33 ms /    17 runs   (    0.37 ms per token,  2687.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.97 ms /   195 tokens (    0.82 ms per token,  1226.65 tokens per second)\n",
      "llama_print_timings:        eval time =     295.79 ms /    16 runs   (   18.49 ms per token,    54.09 tokens per second)\n",
      "llama_print_timings:       total time =     515.95 ms /   211 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.99 ms /   145 runs   (    0.37 ms per token,  2736.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     171.86 ms /   246 tokens (    0.70 ms per token,  1431.43 tokens per second)\n",
      "llama_print_timings:        eval time =    2564.18 ms /   144 runs   (   17.81 ms per token,    56.16 tokens per second)\n",
      "llama_print_timings:       total time =    3250.24 ms /   390 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.38 ms /   112 runs   (    0.38 ms per token,  2642.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     218.58 ms /   283 tokens (    0.77 ms per token,  1294.73 tokens per second)\n",
      "llama_print_timings:        eval time =    2017.28 ms /   111 runs   (   18.17 ms per token,    55.02 tokens per second)\n",
      "llama_print_timings:       total time =    2639.29 ms /   394 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      93.79 ms /   256 runs   (    0.37 ms per token,  2729.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     537.37 ms /   796 tokens (    0.68 ms per token,  1481.30 tokens per second)\n",
      "llama_print_timings:        eval time =    4709.50 ms /   255 runs   (   18.47 ms per token,    54.15 tokens per second)\n",
      "llama_print_timings:       total time =    6194.58 ms /  1051 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.83 ms /   131 runs   (    0.37 ms per token,  2682.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.56 ms /   230 tokens (    0.75 ms per token,  1332.89 tokens per second)\n",
      "llama_print_timings:        eval time =    2438.58 ms /   130 runs   (   18.76 ms per token,    53.31 tokens per second)\n",
      "llama_print_timings:       total time =    3094.20 ms /   360 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      75.85 ms /   203 runs   (    0.37 ms per token,  2676.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     321.76 ms /   446 tokens (    0.72 ms per token,  1386.12 tokens per second)\n",
      "llama_print_timings:        eval time =    3774.87 ms /   202 runs   (   18.69 ms per token,    53.51 tokens per second)\n",
      "llama_print_timings:       total time =    4838.18 ms /   648 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.88 ms /   118 runs   (    0.36 ms per token,  2751.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.80 ms /   244 tokens (    0.64 ms per token,  1556.08 tokens per second)\n",
      "llama_print_timings:        eval time =    2096.84 ms /   117 runs   (   17.92 ms per token,    55.80 tokens per second)\n",
      "llama_print_timings:       total time =    2675.62 ms /   361 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.20 ms /    17 runs   (    0.36 ms per token,  2740.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.19 ms /    34 tokens (    3.15 ms per token,   317.20 tokens per second)\n",
      "llama_print_timings:        eval time =     287.38 ms /    16 runs   (   17.96 ms per token,    55.67 tokens per second)\n",
      "llama_print_timings:       total time =     456.05 ms /    50 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.86 ms /    23 runs   (    0.39 ms per token,  2595.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.25 ms /    90 tokens (    1.37 ms per token,   730.25 tokens per second)\n",
      "llama_print_timings:        eval time =     401.99 ms /    22 runs   (   18.27 ms per token,    54.73 tokens per second)\n",
      "llama_print_timings:       total time =     608.21 ms /   112 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      25.97 ms /    68 runs   (    0.38 ms per token,  2618.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.45 ms /   131 tokens (    1.22 ms per token,   821.58 tokens per second)\n",
      "llama_print_timings:        eval time =    1178.60 ms /    67 runs   (   17.59 ms per token,    56.85 tokens per second)\n",
      "llama_print_timings:       total time =    1576.33 ms /   198 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.16 ms /    21 runs   (    0.39 ms per token,  2572.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     115.71 ms /    93 tokens (    1.24 ms per token,   803.72 tokens per second)\n",
      "llama_print_timings:        eval time =     349.50 ms /    20 runs   (   17.47 ms per token,    57.23 tokens per second)\n",
      "llama_print_timings:       total time =     540.56 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.35 ms /   100 runs   (    0.36 ms per token,  2751.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.12 ms /   132 tokens (    1.09 ms per token,   915.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1699.61 ms /    99 runs   (   17.17 ms per token,    58.25 tokens per second)\n",
      "llama_print_timings:       total time =    2189.70 ms /   231 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      84.19 ms /   224 runs   (    0.38 ms per token,  2660.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     728.82 ms /  1097 tokens (    0.66 ms per token,  1505.17 tokens per second)\n",
      "llama_print_timings:        eval time =    4273.93 ms /   223 runs   (   19.17 ms per token,    52.18 tokens per second)\n",
      "llama_print_timings:       total time =    5811.12 ms /  1320 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.05 ms /   109 runs   (    0.37 ms per token,  2721.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.40 ms /   146 tokens (    1.04 ms per token,   958.00 tokens per second)\n",
      "llama_print_timings:        eval time =    1968.32 ms /   108 runs   (   18.23 ms per token,    54.87 tokens per second)\n",
      "llama_print_timings:       total time =    2493.18 ms /   254 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      26.35 ms /    72 runs   (    0.37 ms per token,  2732.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.93 ms /    79 tokens (    1.57 ms per token,   637.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1254.88 ms /    71 runs   (   17.67 ms per token,    56.58 tokens per second)\n",
      "llama_print_timings:       total time =    1623.36 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.84 ms /    74 runs   (    0.38 ms per token,  2658.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.67 ms /   147 tokens (    1.02 ms per token,   982.19 tokens per second)\n",
      "llama_print_timings:        eval time =    1305.17 ms /    73 runs   (   17.88 ms per token,    55.93 tokens per second)\n",
      "llama_print_timings:       total time =    1709.41 ms /   220 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.31 ms /   127 runs   (    0.37 ms per token,  2684.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.93 ms /   216 tokens (    0.76 ms per token,  1317.61 tokens per second)\n",
      "llama_print_timings:        eval time =    2313.70 ms /   126 runs   (   18.36 ms per token,    54.46 tokens per second)\n",
      "llama_print_timings:       total time =    2928.51 ms /   342 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.70 ms /    17 runs   (    0.39 ms per token,  2538.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.49 ms /   128 tokens (    1.06 ms per token,   944.71 tokens per second)\n",
      "llama_print_timings:        eval time =     298.38 ms /    16 runs   (   18.65 ms per token,    53.62 tokens per second)\n",
      "llama_print_timings:       total time =     499.54 ms /   144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.64 ms /   113 runs   (    0.37 ms per token,  2713.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.26 ms /   203 tokens (    0.83 ms per token,  1206.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1978.16 ms /   112 runs   (   17.66 ms per token,    56.62 tokens per second)\n",
      "llama_print_timings:       total time =    2537.50 ms /   315 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.81 ms /   117 runs   (    0.37 ms per token,  2733.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.43 ms /    66 tokens (    1.64 ms per token,   608.70 tokens per second)\n",
      "llama_print_timings:        eval time =    2059.97 ms /   116 runs   (   17.76 ms per token,    56.31 tokens per second)\n",
      "llama_print_timings:       total time =    2570.86 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.03 ms /    21 runs   (    0.38 ms per token,  2614.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.91 ms /    25 tokens (    6.68 ms per token,   149.78 tokens per second)\n",
      "llama_print_timings:        eval time =     377.85 ms /    20 runs   (   18.89 ms per token,    52.93 tokens per second)\n",
      "llama_print_timings:       total time =     622.16 ms /    45 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     3 runs   (    0.39 ms per token,  2577.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.28 ms /   102 tokens (    1.24 ms per token,   807.73 tokens per second)\n",
      "llama_print_timings:        eval time =      42.33 ms /     2 runs   (   21.16 ms per token,    47.25 tokens per second)\n",
      "llama_print_timings:       total time =     179.21 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2608.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.92 ms /    70 tokens (    1.73 ms per token,   578.90 tokens per second)\n",
      "llama_print_timings:        eval time =      39.32 ms /     2 runs   (   19.66 ms per token,    50.87 tokens per second)\n",
      "llama_print_timings:       total time =     171.77 ms /    72 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      89.11 ms /   239 runs   (    0.37 ms per token,  2682.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.80 ms /   196 tokens (    0.85 ms per token,  1175.10 tokens per second)\n",
      "llama_print_timings:        eval time =    4279.19 ms /   238 runs   (   17.98 ms per token,    55.62 tokens per second)\n",
      "llama_print_timings:       total time =    5314.03 ms /   434 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.43 ms /    90 runs   (    0.37 ms per token,  2692.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     176.34 ms /   188 tokens (    0.94 ms per token,  1066.12 tokens per second)\n",
      "llama_print_timings:        eval time =    1628.50 ms /    89 runs   (   18.30 ms per token,    54.65 tokens per second)\n",
      "llama_print_timings:       total time =    2118.91 ms /   277 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      94.37 ms /   255 runs   (    0.37 ms per token,  2702.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     289.87 ms /   462 tokens (    0.63 ms per token,  1593.83 tokens per second)\n",
      "llama_print_timings:        eval time =    4659.64 ms /   254 runs   (   18.35 ms per token,    54.51 tokens per second)\n",
      "llama_print_timings:       total time =    5869.81 ms /   716 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      54.34 ms /   146 runs   (    0.37 ms per token,  2686.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.85 ms /   247 tokens (    0.67 ms per token,  1489.32 tokens per second)\n",
      "llama_print_timings:        eval time =    2616.41 ms /   145 runs   (   18.04 ms per token,    55.42 tokens per second)\n",
      "llama_print_timings:       total time =    3301.92 ms /   392 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.90 ms /    24 runs   (    0.37 ms per token,  2697.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.86 ms /   125 tokens (    1.00 ms per token,  1001.15 tokens per second)\n",
      "llama_print_timings:        eval time =     431.94 ms /    23 runs   (   18.78 ms per token,    53.25 tokens per second)\n",
      "llama_print_timings:       total time =     640.26 ms /   148 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.53 ms /   115 runs   (    0.38 ms per token,  2641.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     173.30 ms /   214 tokens (    0.81 ms per token,  1234.88 tokens per second)\n",
      "llama_print_timings:        eval time =    2081.51 ms /   114 runs   (   18.26 ms per token,    54.77 tokens per second)\n",
      "llama_print_timings:       total time =    2665.69 ms /   328 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.66 ms /    80 runs   (    0.38 ms per token,  2609.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     104.70 ms /    50 tokens (    2.09 ms per token,   477.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1388.18 ms /    79 runs   (   17.57 ms per token,    56.91 tokens per second)\n",
      "llama_print_timings:       total time =    1770.65 ms /   129 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      64.82 ms /   176 runs   (    0.37 ms per token,  2715.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     191.88 ms /   192 tokens (    1.00 ms per token,  1000.62 tokens per second)\n",
      "llama_print_timings:        eval time =    3253.32 ms /   175 runs   (   18.59 ms per token,    53.79 tokens per second)\n",
      "llama_print_timings:       total time =    4073.30 ms /   367 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.04 ms /   146 runs   (    0.36 ms per token,  2752.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.93 ms /   119 tokens (    1.05 ms per token,   952.56 tokens per second)\n",
      "llama_print_timings:        eval time =    2479.70 ms /   145 runs   (   17.10 ms per token,    58.47 tokens per second)\n",
      "llama_print_timings:       total time =    3117.94 ms /   264 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.38 ms /    93 runs   (    0.37 ms per token,  2705.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.24 ms /   110 tokens (    1.14 ms per token,   878.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1594.75 ms /    92 runs   (   17.33 ms per token,    57.69 tokens per second)\n",
      "llama_print_timings:       total time =    2040.80 ms /   202 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.14 ms /    80 runs   (    0.38 ms per token,  2653.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     171.18 ms /   253 tokens (    0.68 ms per token,  1477.99 tokens per second)\n",
      "llama_print_timings:        eval time =    1443.09 ms /    79 runs   (   18.27 ms per token,    54.74 tokens per second)\n",
      "llama_print_timings:       total time =    1891.55 ms /   332 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      60.52 ms /   159 runs   (    0.38 ms per token,  2627.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.45 ms /   123 tokens (    1.00 ms per token,   996.32 tokens per second)\n",
      "llama_print_timings:        eval time =    2826.28 ms /   158 runs   (   17.89 ms per token,    55.90 tokens per second)\n",
      "llama_print_timings:       total time =    3522.66 ms /   281 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.27 ms /    93 runs   (    0.38 ms per token,  2637.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     505.52 ms /   657 tokens (    0.77 ms per token,  1299.64 tokens per second)\n",
      "llama_print_timings:        eval time =    1770.36 ms /    92 runs   (   19.24 ms per token,    51.97 tokens per second)\n",
      "llama_print_timings:       total time =    2599.58 ms /   749 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2622.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.41 ms /    52 tokens (    2.28 ms per token,   439.14 tokens per second)\n",
      "llama_print_timings:        eval time =      41.19 ms /     2 runs   (   20.60 ms per token,    48.55 tokens per second)\n",
      "llama_print_timings:       total time =     171.12 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.38 ms /    98 runs   (    0.38 ms per token,  2621.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.12 ms /   124 tokens (    1.08 ms per token,   924.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1742.73 ms /    97 runs   (   17.97 ms per token,    55.66 tokens per second)\n",
      "llama_print_timings:       total time =    2218.99 ms /   221 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2613.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.55 ms /   143 tokens (    1.14 ms per token,   874.36 tokens per second)\n",
      "llama_print_timings:        eval time =      35.27 ms /     2 runs   (   17.63 ms per token,    56.71 tokens per second)\n",
      "llama_print_timings:       total time =     208.69 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.09 ms /   115 runs   (    0.36 ms per token,  2798.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     189.33 ms /   241 tokens (    0.79 ms per token,  1272.92 tokens per second)\n",
      "llama_print_timings:        eval time =    2077.81 ms /   114 runs   (   18.23 ms per token,    54.87 tokens per second)\n",
      "llama_print_timings:       total time =    2669.52 ms /   355 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      95.78 ms /   256 runs   (    0.37 ms per token,  2672.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     240.97 ms /   419 tokens (    0.58 ms per token,  1738.78 tokens per second)\n",
      "llama_print_timings:        eval time =    4671.49 ms /   255 runs   (   18.32 ms per token,    54.59 tokens per second)\n",
      "llama_print_timings:       total time =    5851.72 ms /   674 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.10 ms /    21 runs   (    0.39 ms per token,  2592.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     111.23 ms /    42 tokens (    2.65 ms per token,   377.59 tokens per second)\n",
      "llama_print_timings:        eval time =     359.27 ms /    20 runs   (   17.96 ms per token,    55.67 tokens per second)\n",
      "llama_print_timings:       total time =     550.73 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2604.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.87 ms /   126 tokens (    0.94 ms per token,  1060.03 tokens per second)\n",
      "llama_print_timings:        eval time =      36.94 ms /     2 runs   (   18.47 ms per token,    54.15 tokens per second)\n",
      "llama_print_timings:       total time =     166.84 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.16 ms /    21 runs   (    0.39 ms per token,  2572.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     173.70 ms /   233 tokens (    0.75 ms per token,  1341.39 tokens per second)\n",
      "llama_print_timings:        eval time =     368.98 ms /    20 runs   (   18.45 ms per token,    54.20 tokens per second)\n",
      "llama_print_timings:       total time =     615.33 ms /   253 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.19 ms /    25 runs   (    0.37 ms per token,  2721.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.44 ms /   118 tokens (    1.17 ms per token,   852.35 tokens per second)\n",
      "llama_print_timings:        eval time =     454.42 ms /    24 runs   (   18.93 ms per token,    52.82 tokens per second)\n",
      "llama_print_timings:       total time =     678.25 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      64.82 ms /   173 runs   (    0.37 ms per token,  2668.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     242.39 ms /   299 tokens (    0.81 ms per token,  1233.55 tokens per second)\n",
      "llama_print_timings:        eval time =    3136.93 ms /   172 runs   (   18.24 ms per token,    54.83 tokens per second)\n",
      "llama_print_timings:       total time =    3995.66 ms /   471 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.60 ms /    96 runs   (    0.37 ms per token,  2696.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     186.28 ms /   231 tokens (    0.81 ms per token,  1240.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1757.43 ms /    95 runs   (   18.50 ms per token,    54.06 tokens per second)\n",
      "llama_print_timings:       total time =    2281.54 ms /   326 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      56.16 ms /   152 runs   (    0.37 ms per token,  2706.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     231.11 ms /   327 tokens (    0.71 ms per token,  1414.94 tokens per second)\n",
      "llama_print_timings:        eval time =    2688.31 ms /   151 runs   (   17.80 ms per token,    56.17 tokens per second)\n",
      "llama_print_timings:       total time =    3471.33 ms /   478 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.84 ms /    21 runs   (    0.37 ms per token,  2678.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.98 ms /    38 tokens (    2.84 ms per token,   351.92 tokens per second)\n",
      "llama_print_timings:        eval time =     356.47 ms /    20 runs   (   17.82 ms per token,    56.11 tokens per second)\n",
      "llama_print_timings:       total time =     539.11 ms /    58 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.11 ms /     3 runs   (    0.37 ms per token,  2712.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.45 ms /   210 tokens (    0.71 ms per token,  1405.19 tokens per second)\n",
      "llama_print_timings:        eval time =      37.89 ms /     2 runs   (   18.94 ms per token,    52.79 tokens per second)\n",
      "llama_print_timings:       total time =     197.93 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.10 ms /   108 runs   (    0.37 ms per token,  2693.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     270.99 ms /   502 tokens (    0.54 ms per token,  1852.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1969.72 ms /   107 runs   (   18.41 ms per token,    54.32 tokens per second)\n",
      "llama_print_timings:       total time =    2626.99 ms /   609 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.11 ms /    21 runs   (    0.39 ms per token,  2589.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     175.25 ms /   222 tokens (    0.79 ms per token,  1266.77 tokens per second)\n",
      "llama_print_timings:        eval time =     389.86 ms /    20 runs   (   19.49 ms per token,    51.30 tokens per second)\n",
      "llama_print_timings:       total time =     640.79 ms /   242 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.76 ms /   130 runs   (    0.38 ms per token,  2612.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     211.01 ms /   267 tokens (    0.79 ms per token,  1265.35 tokens per second)\n",
      "llama_print_timings:        eval time =    2321.24 ms /   129 runs   (   17.99 ms per token,    55.57 tokens per second)\n",
      "llama_print_timings:       total time =    2988.49 ms /   396 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.63 ms /   112 runs   (    0.37 ms per token,  2690.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.36 ms /    46 tokens (    2.44 ms per token,   409.42 tokens per second)\n",
      "llama_print_timings:        eval time =    2004.03 ms /   111 runs   (   18.05 ms per token,    55.39 tokens per second)\n",
      "llama_print_timings:       total time =    2510.19 ms /   157 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      11.06 ms /    29 runs   (    0.38 ms per token,  2622.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.92 ms /   200 tokens (    0.82 ms per token,  1220.10 tokens per second)\n",
      "llama_print_timings:        eval time =     510.29 ms /    28 runs   (   18.22 ms per token,    54.87 tokens per second)\n",
      "llama_print_timings:       total time =     775.33 ms /   228 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      26.68 ms /    72 runs   (    0.37 ms per token,  2698.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.48 ms /   112 tokens (    1.17 ms per token,   858.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1255.21 ms /    71 runs   (   17.68 ms per token,    56.56 tokens per second)\n",
      "llama_print_timings:       total time =    1635.91 ms /   183 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.39 ms /    19 runs   (    0.39 ms per token,  2570.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.76 ms /   133 tokens (    1.19 ms per token,   837.76 tokens per second)\n",
      "llama_print_timings:        eval time =     343.63 ms /    18 runs   (   19.09 ms per token,    52.38 tokens per second)\n",
      "llama_print_timings:       total time =     568.02 ms /   151 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      80.77 ms /   216 runs   (    0.37 ms per token,  2674.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     167.72 ms /   164 tokens (    1.02 ms per token,   977.81 tokens per second)\n",
      "llama_print_timings:        eval time =    3883.45 ms /   215 runs   (   18.06 ms per token,    55.36 tokens per second)\n",
      "llama_print_timings:       total time =    4844.52 ms /   379 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.30 ms /   120 runs   (    0.37 ms per token,  2708.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.84 ms /   131 tokens (    1.14 ms per token,   880.11 tokens per second)\n",
      "llama_print_timings:        eval time =    2098.90 ms /   119 runs   (   17.64 ms per token,    56.70 tokens per second)\n",
      "llama_print_timings:       total time =    2672.69 ms /   250 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.18 ms /    26 runs   (    0.39 ms per token,  2554.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.98 ms /   116 tokens (    1.08 ms per token,   928.19 tokens per second)\n",
      "llama_print_timings:        eval time =     455.07 ms /    25 runs   (   18.20 ms per token,    54.94 tokens per second)\n",
      "llama_print_timings:       total time =     673.16 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      75.90 ms /   204 runs   (    0.37 ms per token,  2687.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.79 ms /   259 tokens (    0.84 ms per token,  1194.68 tokens per second)\n",
      "llama_print_timings:        eval time =    3686.43 ms /   203 runs   (   18.16 ms per token,    55.07 tokens per second)\n",
      "llama_print_timings:       total time =    4649.73 ms /   462 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      94.97 ms /   256 runs   (    0.37 ms per token,  2695.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     455.34 ms /   582 tokens (    0.78 ms per token,  1278.17 tokens per second)\n",
      "llama_print_timings:        eval time =    4813.83 ms /   255 runs   (   18.88 ms per token,    52.97 tokens per second)\n",
      "llama_print_timings:       total time =    6208.50 ms /   837 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.80 ms /    85 runs   (    0.37 ms per token,  2672.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.80 ms /   184 tokens (    0.90 ms per token,  1109.79 tokens per second)\n",
      "llama_print_timings:        eval time =    1482.47 ms /    84 runs   (   17.65 ms per token,    56.66 tokens per second)\n",
      "llama_print_timings:       total time =    1944.48 ms /   268 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.22 ms /   103 runs   (    0.37 ms per token,  2694.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.47 ms /   107 tokens (    1.18 ms per token,   846.04 tokens per second)\n",
      "llama_print_timings:        eval time =    1785.66 ms /   102 runs   (   17.51 ms per token,    57.12 tokens per second)\n",
      "llama_print_timings:       total time =    2270.44 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.71 ms /    18 runs   (    0.37 ms per token,  2684.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     113.80 ms /    37 tokens (    3.08 ms per token,   325.15 tokens per second)\n",
      "llama_print_timings:        eval time =     316.38 ms /    17 runs   (   18.61 ms per token,    53.73 tokens per second)\n",
      "llama_print_timings:       total time =     495.41 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      73.39 ms /   201 runs   (    0.37 ms per token,  2738.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.77 ms /   146 tokens (    1.11 ms per token,   896.99 tokens per second)\n",
      "llama_print_timings:        eval time =    3606.70 ms /   200 runs   (   18.03 ms per token,    55.45 tokens per second)\n",
      "llama_print_timings:       total time =    4490.32 ms /   346 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.50 ms /   122 runs   (    0.38 ms per token,  2623.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     178.22 ms /   163 tokens (    1.09 ms per token,   914.59 tokens per second)\n",
      "llama_print_timings:        eval time =    2352.13 ms /   121 runs   (   19.44 ms per token,    51.44 tokens per second)\n",
      "llama_print_timings:       total time =    2979.25 ms /   284 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2542.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.56 ms /   170 tokens (    0.96 ms per token,  1045.75 tokens per second)\n",
      "llama_print_timings:        eval time =      39.77 ms /     2 runs   (   19.88 ms per token,    50.29 tokens per second)\n",
      "llama_print_timings:       total time =     212.54 ms /   172 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.28 ms /   109 runs   (    0.38 ms per token,  2640.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.25 ms /   155 tokens (    1.04 ms per token,   961.25 tokens per second)\n",
      "llama_print_timings:        eval time =    2020.70 ms /   108 runs   (   18.71 ms per token,    53.45 tokens per second)\n",
      "llama_print_timings:       total time =    2592.10 ms /   263 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.30 ms /    27 runs   (    0.38 ms per token,  2621.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.10 ms /    65 tokens (    1.88 ms per token,   532.35 tokens per second)\n",
      "llama_print_timings:        eval time =     467.87 ms /    26 runs   (   17.99 ms per token,    55.57 tokens per second)\n",
      "llama_print_timings:       total time =     690.07 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.30 ms /    21 runs   (    0.40 ms per token,  2530.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.39 ms /   112 tokens (    1.13 ms per token,   886.15 tokens per second)\n",
      "llama_print_timings:        eval time =     384.72 ms /    20 runs   (   19.24 ms per token,    51.99 tokens per second)\n",
      "llama_print_timings:       total time =     594.42 ms /   132 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.86 ms /    18 runs   (    0.38 ms per token,  2625.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.41 ms /    70 tokens (    1.63 ms per token,   611.84 tokens per second)\n",
      "llama_print_timings:        eval time =     333.65 ms /    17 runs   (   19.63 ms per token,    50.95 tokens per second)\n",
      "llama_print_timings:       total time =     521.31 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      14.70 ms /    42 runs   (    0.35 ms per token,  2856.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.55 ms /    94 tokens (    1.40 ms per token,   714.56 tokens per second)\n",
      "llama_print_timings:        eval time =     794.35 ms /    41 runs   (   19.37 ms per token,    51.61 tokens per second)\n",
      "llama_print_timings:       total time =    1075.86 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      63.34 ms /   169 runs   (    0.37 ms per token,  2667.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     191.89 ms /   223 tokens (    0.86 ms per token,  1162.11 tokens per second)\n",
      "llama_print_timings:        eval time =    3117.72 ms /   168 runs   (   18.56 ms per token,    53.89 tokens per second)\n",
      "llama_print_timings:       total time =    3939.06 ms /   391 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2553.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.19 ms /    60 tokens (    1.99 ms per token,   503.39 tokens per second)\n",
      "llama_print_timings:        eval time =      39.63 ms /     2 runs   (   19.82 ms per token,    50.46 tokens per second)\n",
      "llama_print_timings:       total time =     169.93 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.45 ms /    25 runs   (    0.38 ms per token,  2646.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.35 ms /    61 tokens (    1.91 ms per token,   524.28 tokens per second)\n",
      "llama_print_timings:        eval time =     444.40 ms /    24 runs   (   18.52 ms per token,    54.01 tokens per second)\n",
      "llama_print_timings:       total time =     650.15 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2608.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.74 ms /    66 tokens (    1.86 ms per token,   537.71 tokens per second)\n",
      "llama_print_timings:        eval time =      42.48 ms /     2 runs   (   21.24 ms per token,    47.08 tokens per second)\n",
      "llama_print_timings:       total time =     175.44 ms /    68 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.06 ms /    18 runs   (    0.39 ms per token,  2549.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.06 ms /    60 tokens (    1.97 ms per token,   508.23 tokens per second)\n",
      "llama_print_timings:        eval time =     316.30 ms /    17 runs   (   18.61 ms per token,    53.75 tokens per second)\n",
      "llama_print_timings:       total time =     497.86 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.38 ms /    22 runs   (    0.38 ms per token,  2626.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     128.77 ms /   108 tokens (    1.19 ms per token,   838.74 tokens per second)\n",
      "llama_print_timings:        eval time =     398.32 ms /    21 runs   (   18.97 ms per token,    52.72 tokens per second)\n",
      "llama_print_timings:       total time =     612.73 ms /   129 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.38 ms /   122 runs   (    0.37 ms per token,  2688.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.74 ms /   182 tokens (    0.92 ms per token,  1091.53 tokens per second)\n",
      "llama_print_timings:        eval time =    2315.99 ms /   121 runs   (   19.14 ms per token,    52.25 tokens per second)\n",
      "llama_print_timings:       total time =    2951.04 ms /   303 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.94 ms /   128 runs   (    0.37 ms per token,  2727.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     150.52 ms /   129 tokens (    1.17 ms per token,   857.02 tokens per second)\n",
      "llama_print_timings:        eval time =    2300.60 ms /   127 runs   (   18.11 ms per token,    55.20 tokens per second)\n",
      "llama_print_timings:       total time =    2915.13 ms /   256 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    17 runs   (    0.38 ms per token,  2609.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.68 ms /    89 tokens (    1.36 ms per token,   737.49 tokens per second)\n",
      "llama_print_timings:        eval time =     315.37 ms /    16 runs   (   19.71 ms per token,    50.73 tokens per second)\n",
      "llama_print_timings:       total time =     500.42 ms /   105 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      84.52 ms /   229 runs   (    0.37 ms per token,  2709.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.32 ms /   387 tokens (    0.69 ms per token,  1458.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4289.41 ms /   228 runs   (   18.81 ms per token,    53.15 tokens per second)\n",
      "llama_print_timings:       total time =    5402.99 ms /   615 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.45 ms /   121 runs   (    0.36 ms per token,  2785.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     171.43 ms /   212 tokens (    0.81 ms per token,  1236.69 tokens per second)\n",
      "llama_print_timings:        eval time =    2079.20 ms /   120 runs   (   17.33 ms per token,    57.71 tokens per second)\n",
      "llama_print_timings:       total time =    2673.04 ms /   332 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.19 ms /    23 runs   (    0.36 ms per token,  2807.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     171.46 ms /   166 tokens (    1.03 ms per token,   968.16 tokens per second)\n",
      "llama_print_timings:        eval time =     405.84 ms /    22 runs   (   18.45 ms per token,    54.21 tokens per second)\n",
      "llama_print_timings:       total time =     661.57 ms /   188 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.10 ms /    21 runs   (    0.39 ms per token,  2593.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     111.61 ms /   102 tokens (    1.09 ms per token,   913.87 tokens per second)\n",
      "llama_print_timings:        eval time =     362.13 ms /    20 runs   (   18.11 ms per token,    55.23 tokens per second)\n",
      "llama_print_timings:       total time =     551.14 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.19 ms /    17 runs   (    0.36 ms per token,  2746.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.79 ms /    60 tokens (    1.85 ms per token,   541.55 tokens per second)\n",
      "llama_print_timings:        eval time =     282.71 ms /    16 runs   (   17.67 ms per token,    56.60 tokens per second)\n",
      "llama_print_timings:       total time =     455.66 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.45 ms /   146 runs   (    0.36 ms per token,  2783.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.93 ms /   116 tokens (    1.13 ms per token,   885.96 tokens per second)\n",
      "llama_print_timings:        eval time =    2616.60 ms /   145 runs   (   18.05 ms per token,    55.42 tokens per second)\n",
      "llama_print_timings:       total time =    3274.75 ms /   261 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.18 ms /   100 runs   (    0.37 ms per token,  2689.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.76 ms /    57 tokens (    1.94 ms per token,   514.62 tokens per second)\n",
      "llama_print_timings:        eval time =    1747.49 ms /    99 runs   (   17.65 ms per token,    56.65 tokens per second)\n",
      "llama_print_timings:       total time =    2211.39 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.50 ms /    92 runs   (    0.37 ms per token,  2666.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.73 ms /    51 tokens (    2.11 ms per token,   473.42 tokens per second)\n",
      "llama_print_timings:        eval time =    1628.55 ms /    91 runs   (   17.90 ms per token,    55.88 tokens per second)\n",
      "llama_print_timings:       total time =    2060.61 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.63 ms /    84 runs   (    0.38 ms per token,  2655.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.35 ms /   125 tokens (    1.11 ms per token,   903.50 tokens per second)\n",
      "llama_print_timings:        eval time =    1514.45 ms /    83 runs   (   18.25 ms per token,    54.81 tokens per second)\n",
      "llama_print_timings:       total time =    1952.68 ms /   208 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.43 ms /    17 runs   (    0.38 ms per token,  2644.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.86 ms /   124 tokens (    0.98 ms per token,  1017.59 tokens per second)\n",
      "llama_print_timings:        eval time =     290.37 ms /    16 runs   (   18.15 ms per token,    55.10 tokens per second)\n",
      "llama_print_timings:       total time =     474.22 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.47 ms /   124 runs   (    0.37 ms per token,  2668.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     153.94 ms /   227 tokens (    0.68 ms per token,  1474.60 tokens per second)\n",
      "llama_print_timings:        eval time =    2237.13 ms /   123 runs   (   18.19 ms per token,    54.98 tokens per second)\n",
      "llama_print_timings:       total time =    2834.40 ms /   350 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.11 ms /    76 runs   (    0.37 ms per token,  2704.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     244.27 ms /   285 tokens (    0.86 ms per token,  1166.76 tokens per second)\n",
      "llama_print_timings:        eval time =    1312.50 ms /    75 runs   (   17.50 ms per token,    57.14 tokens per second)\n",
      "llama_print_timings:       total time =    1819.78 ms /   360 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.07 ms /     3 runs   (    0.36 ms per token,  2793.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.98 ms /    49 tokens (    2.22 ms per token,   449.64 tokens per second)\n",
      "llama_print_timings:        eval time =      37.98 ms /     2 runs   (   18.99 ms per token,    52.65 tokens per second)\n",
      "llama_print_timings:       total time =     157.88 ms /    51 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.99 ms /    23 runs   (    0.39 ms per token,  2558.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.41 ms /    87 tokens (    1.40 ms per token,   716.61 tokens per second)\n",
      "llama_print_timings:        eval time =     383.09 ms /    22 runs   (   17.41 ms per token,    57.43 tokens per second)\n",
      "llama_print_timings:       total time =     584.96 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.28 ms /   120 runs   (    0.38 ms per token,  2650.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.10 ms /   147 tokens (    1.03 ms per token,   966.44 tokens per second)\n",
      "llama_print_timings:        eval time =    2223.87 ms /   119 runs   (   18.69 ms per token,    53.51 tokens per second)\n",
      "llama_print_timings:       total time =    2820.46 ms /   266 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.10 ms /    22 runs   (    0.37 ms per token,  2716.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     115.62 ms /    83 tokens (    1.39 ms per token,   717.84 tokens per second)\n",
      "llama_print_timings:        eval time =     393.79 ms /    21 runs   (   18.75 ms per token,    53.33 tokens per second)\n",
      "llama_print_timings:       total time =     595.40 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.08 ms /    92 runs   (    0.37 ms per token,  2699.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.77 ms /   109 tokens (    1.14 ms per token,   880.68 tokens per second)\n",
      "llama_print_timings:        eval time =    1654.85 ms /    91 runs   (   18.19 ms per token,    54.99 tokens per second)\n",
      "llama_print_timings:       total time =    2107.69 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.27 ms /    96 runs   (    0.38 ms per token,  2646.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.60 ms /   131 tokens (    1.22 ms per token,   820.79 tokens per second)\n",
      "llama_print_timings:        eval time =    1685.55 ms /    95 runs   (   17.74 ms per token,    56.36 tokens per second)\n",
      "llama_print_timings:       total time =    2190.89 ms /   226 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      91.48 ms /   250 runs   (    0.37 ms per token,  2732.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     260.46 ms /   405 tokens (    0.64 ms per token,  1554.93 tokens per second)\n",
      "llama_print_timings:        eval time =    4715.71 ms /   249 runs   (   18.94 ms per token,    52.80 tokens per second)\n",
      "llama_print_timings:       total time =    5894.59 ms /   654 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.74 ms /   105 runs   (    0.37 ms per token,  2710.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.24 ms /    81 tokens (    1.46 ms per token,   685.06 tokens per second)\n",
      "llama_print_timings:        eval time =    1790.22 ms /   104 runs   (   17.21 ms per token,    58.09 tokens per second)\n",
      "llama_print_timings:       total time =    2274.25 ms /   185 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      63.70 ms /   169 runs   (    0.38 ms per token,  2652.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.69 ms /   202 tokens (    0.78 ms per token,  1289.15 tokens per second)\n",
      "llama_print_timings:        eval time =    3035.36 ms /   168 runs   (   18.07 ms per token,    55.35 tokens per second)\n",
      "llama_print_timings:       total time =    3797.45 ms /   370 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.70 ms /   105 runs   (    0.38 ms per token,  2644.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.53 ms /   161 tokens (    0.92 ms per token,  1083.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1946.36 ms /   104 runs   (   18.72 ms per token,    53.43 tokens per second)\n",
      "llama_print_timings:       total time =    2489.26 ms /   265 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      29.27 ms /    75 runs   (    0.39 ms per token,  2562.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.09 ms /    87 tokens (    1.61 ms per token,   621.01 tokens per second)\n",
      "llama_print_timings:        eval time =    1403.43 ms /    74 runs   (   18.97 ms per token,    52.73 tokens per second)\n",
      "llama_print_timings:       total time =    1839.67 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      56.86 ms /   154 runs   (    0.37 ms per token,  2708.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     187.39 ms /   173 tokens (    1.08 ms per token,   923.22 tokens per second)\n",
      "llama_print_timings:        eval time =    2901.63 ms /   153 runs   (   18.96 ms per token,    52.73 tokens per second)\n",
      "llama_print_timings:       total time =    3678.09 ms /   326 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.86 ms /   117 runs   (    0.37 ms per token,  2729.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.80 ms /   291 tokens (    0.92 ms per token,  1086.65 tokens per second)\n",
      "llama_print_timings:        eval time =    2125.98 ms /   116 runs   (   18.33 ms per token,    54.56 tokens per second)\n",
      "llama_print_timings:       total time =    2827.48 ms /   407 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      93.74 ms /   256 runs   (    0.37 ms per token,  2730.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     182.03 ms /   243 tokens (    0.75 ms per token,  1334.92 tokens per second)\n",
      "llama_print_timings:        eval time =    4589.81 ms /   255 runs   (   18.00 ms per token,    55.56 tokens per second)\n",
      "llama_print_timings:       total time =    5731.59 ms /   498 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.09 ms /   104 runs   (    0.38 ms per token,  2660.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.76 ms /   121 tokens (    1.12 ms per token,   891.28 tokens per second)\n",
      "llama_print_timings:        eval time =    1889.22 ms /   103 runs   (   18.34 ms per token,    54.52 tokens per second)\n",
      "llama_print_timings:       total time =    2405.88 ms /   224 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2620.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.16 ms /    52 tokens (    2.12 ms per token,   472.04 tokens per second)\n",
      "llama_print_timings:        eval time =      38.06 ms /     2 runs   (   19.03 ms per token,    52.54 tokens per second)\n",
      "llama_print_timings:       total time =     161.54 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.38 ms /    26 runs   (    0.36 ms per token,  2770.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.39 ms /    33 tokens (    3.65 ms per token,   274.12 tokens per second)\n",
      "llama_print_timings:        eval time =     450.81 ms /    25 runs   (   18.03 ms per token,    55.46 tokens per second)\n",
      "llama_print_timings:       total time =     667.43 ms /    58 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.24 ms /   121 runs   (    0.37 ms per token,  2674.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     202.03 ms /   257 tokens (    0.79 ms per token,  1272.08 tokens per second)\n",
      "llama_print_timings:        eval time =    2257.90 ms /   120 runs   (   18.82 ms per token,    53.15 tokens per second)\n",
      "llama_print_timings:       total time =    2920.39 ms /   377 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      67.53 ms /   175 runs   (    0.39 ms per token,  2591.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     194.16 ms /   171 tokens (    1.14 ms per token,   880.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3499.76 ms /   174 runs   (   20.11 ms per token,    49.72 tokens per second)\n",
      "llama_print_timings:       total time =    4391.79 ms /   345 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.78 ms /   107 runs   (    0.38 ms per token,  2623.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.17 ms /   104 tokens (    1.06 ms per token,   943.96 tokens per second)\n",
      "llama_print_timings:        eval time =    2143.70 ms /   106 runs   (   20.22 ms per token,    49.45 tokens per second)\n",
      "llama_print_timings:       total time =    2693.62 ms /   210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.72 ms /    17 runs   (    0.40 ms per token,  2529.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     179.93 ms /   193 tokens (    0.93 ms per token,  1072.66 tokens per second)\n",
      "llama_print_timings:        eval time =     303.25 ms /    16 runs   (   18.95 ms per token,    52.76 tokens per second)\n",
      "llama_print_timings:       total time =     554.04 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.34 ms /    19 runs   (    0.44 ms per token,  2279.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.57 ms /    63 tokens (    1.72 ms per token,   580.29 tokens per second)\n",
      "llama_print_timings:        eval time =     387.28 ms /    18 runs   (   21.52 ms per token,    46.48 tokens per second)\n",
      "llama_print_timings:       total time =     593.45 ms /    81 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.20 ms /     3 runs   (    0.40 ms per token,  2502.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     146.68 ms /    35 tokens (    4.19 ms per token,   238.61 tokens per second)\n",
      "llama_print_timings:        eval time =      51.14 ms /     2 runs   (   25.57 ms per token,    39.11 tokens per second)\n",
      "llama_print_timings:       total time =     213.93 ms /    37 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.01 ms /   116 runs   (    0.43 ms per token,  2319.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     206.81 ms /   234 tokens (    0.88 ms per token,  1131.48 tokens per second)\n",
      "llama_print_timings:        eval time =    2552.69 ms /   115 runs   (   22.20 ms per token,    45.05 tokens per second)\n",
      "llama_print_timings:       total time =    3342.62 ms /   349 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      84.64 ms /   230 runs   (    0.37 ms per token,  2717.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     278.16 ms /   417 tokens (    0.67 ms per token,  1499.12 tokens per second)\n",
      "llama_print_timings:        eval time =    5054.63 ms /   229 runs   (   22.07 ms per token,    45.30 tokens per second)\n",
      "llama_print_timings:       total time =    6252.75 ms /   646 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2604.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     188.82 ms /   114 tokens (    1.66 ms per token,   603.74 tokens per second)\n",
      "llama_print_timings:        eval time =      47.48 ms /     2 runs   (   23.74 ms per token,    42.13 tokens per second)\n",
      "llama_print_timings:       total time =     248.84 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.78 ms /    21 runs   (    0.37 ms per token,  2697.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     127.39 ms /    47 tokens (    2.71 ms per token,   368.93 tokens per second)\n",
      "llama_print_timings:        eval time =     428.63 ms /    20 runs   (   21.43 ms per token,    46.66 tokens per second)\n",
      "llama_print_timings:       total time =     640.03 ms /    67 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.55 ms /   115 runs   (    0.37 ms per token,  2702.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     186.98 ms /   160 tokens (    1.17 ms per token,   855.72 tokens per second)\n",
      "llama_print_timings:        eval time =    2421.81 ms /   114 runs   (   21.24 ms per token,    47.07 tokens per second)\n",
      "llama_print_timings:       total time =    3050.75 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      96.11 ms /   256 runs   (    0.38 ms per token,  2663.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     331.62 ms /   416 tokens (    0.80 ms per token,  1254.43 tokens per second)\n",
      "llama_print_timings:        eval time =    5663.38 ms /   255 runs   (   22.21 ms per token,    45.03 tokens per second)\n",
      "llama_print_timings:       total time =    7057.00 ms /   671 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.48 ms /   113 runs   (    0.38 ms per token,  2659.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.39 ms /   152 tokens (    1.06 ms per token,   941.81 tokens per second)\n",
      "llama_print_timings:        eval time =    2150.05 ms /   112 runs   (   19.20 ms per token,    52.09 tokens per second)\n",
      "llama_print_timings:       total time =    2759.96 ms /   264 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      61.78 ms /   149 runs   (    0.41 ms per token,  2411.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.04 ms /   191 tokens (    0.95 ms per token,  1055.04 tokens per second)\n",
      "llama_print_timings:        eval time =    3136.52 ms /   148 runs   (   21.19 ms per token,    47.19 tokens per second)\n",
      "llama_print_timings:       total time =    4076.03 ms /   339 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.38 ms /     3 runs   (    0.46 ms per token,  2177.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.45 ms /    50 tokens (    2.77 ms per token,   361.15 tokens per second)\n",
      "llama_print_timings:        eval time =      46.31 ms /     2 runs   (   23.15 ms per token,    43.19 tokens per second)\n",
      "llama_print_timings:       total time =     203.77 ms /    52 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      92.25 ms /   238 runs   (    0.39 ms per token,  2579.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     309.26 ms /   339 tokens (    0.91 ms per token,  1096.15 tokens per second)\n",
      "llama_print_timings:        eval time =    5175.81 ms /   237 runs   (   21.84 ms per token,    45.79 tokens per second)\n",
      "llama_print_timings:       total time =    6551.81 ms /   576 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.91 ms /    98 runs   (    0.39 ms per token,  2584.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.00 ms /   112 tokens (    1.35 ms per token,   741.73 tokens per second)\n",
      "llama_print_timings:        eval time =    1930.79 ms /    97 runs   (   19.91 ms per token,    50.24 tokens per second)\n",
      "llama_print_timings:       total time =    2459.06 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.69 ms /   120 runs   (    0.40 ms per token,  2516.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.00 ms /   134 tokens (    1.16 ms per token,   864.52 tokens per second)\n",
      "llama_print_timings:        eval time =    2508.05 ms /   119 runs   (   21.08 ms per token,    47.45 tokens per second)\n",
      "llama_print_timings:       total time =    3174.63 ms /   253 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.64 ms /     3 runs   (    0.55 ms per token,  1834.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.44 ms /    85 tokens (    1.48 ms per token,   677.63 tokens per second)\n",
      "llama_print_timings:        eval time =      52.72 ms /     2 runs   (   26.36 ms per token,    37.94 tokens per second)\n",
      "llama_print_timings:       total time =     196.90 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.69 ms /   134 runs   (    0.37 ms per token,  2696.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.09 ms /    98 tokens (    1.42 ms per token,   704.55 tokens per second)\n",
      "llama_print_timings:        eval time =    2870.16 ms /   133 runs   (   21.58 ms per token,    46.34 tokens per second)\n",
      "llama_print_timings:       total time =    3550.91 ms /   231 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.17 ms /     3 runs   (    0.39 ms per token,  2566.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.42 ms /    53 tokens (    2.67 ms per token,   374.78 tokens per second)\n",
      "llama_print_timings:        eval time =      43.47 ms /     2 runs   (   21.73 ms per token,    46.01 tokens per second)\n",
      "llama_print_timings:       total time =     198.73 ms /    55 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.48 ms /    22 runs   (    0.39 ms per token,  2594.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     115.42 ms /    45 tokens (    2.56 ms per token,   389.89 tokens per second)\n",
      "llama_print_timings:        eval time =     420.89 ms /    21 runs   (   20.04 ms per token,    49.89 tokens per second)\n",
      "llama_print_timings:       total time =     629.72 ms /    66 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.80 ms /    17 runs   (    0.40 ms per token,  2500.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.97 ms /   120 tokens (    1.36 ms per token,   736.33 tokens per second)\n",
      "llama_print_timings:        eval time =     350.63 ms /    16 runs   (   21.91 ms per token,    45.63 tokens per second)\n",
      "llama_print_timings:       total time =     591.88 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.61 ms /   103 runs   (    0.38 ms per token,  2600.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.66 ms /   154 tokens (    1.12 ms per token,   891.94 tokens per second)\n",
      "llama_print_timings:        eval time =    2185.91 ms /   102 runs   (   21.43 ms per token,    46.66 tokens per second)\n",
      "llama_print_timings:       total time =    2791.47 ms /   256 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.51 ms /    97 runs   (    0.38 ms per token,  2656.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     185.18 ms /   156 tokens (    1.19 ms per token,   842.45 tokens per second)\n",
      "llama_print_timings:        eval time =    2136.78 ms /    96 runs   (   22.26 ms per token,    44.93 tokens per second)\n",
      "llama_print_timings:       total time =    2704.88 ms /   252 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      11.97 ms /    30 runs   (    0.40 ms per token,  2506.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.22 ms /   122 tokens (    1.32 ms per token,   756.72 tokens per second)\n",
      "llama_print_timings:        eval time =     643.13 ms /    29 runs   (   22.18 ms per token,    45.09 tokens per second)\n",
      "llama_print_timings:       total time =     925.02 ms /   151 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.97 ms /   144 runs   (    0.37 ms per token,  2718.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     509.57 ms /   521 tokens (    0.98 ms per token,  1022.44 tokens per second)\n",
      "llama_print_timings:        eval time =    3047.77 ms /   143 runs   (   21.31 ms per token,    46.92 tokens per second)\n",
      "llama_print_timings:       total time =    4105.95 ms /   664 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      75.29 ms /   199 runs   (    0.38 ms per token,  2643.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     198.76 ms /   216 tokens (    0.92 ms per token,  1086.75 tokens per second)\n",
      "llama_print_timings:        eval time =    4214.18 ms /   198 runs   (   21.28 ms per token,    46.98 tokens per second)\n",
      "llama_print_timings:       total time =    5200.57 ms /   414 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.07 ms /     3 runs   (    0.36 ms per token,  2795.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.56 ms /    63 tokens (    1.90 ms per token,   526.94 tokens per second)\n",
      "llama_print_timings:        eval time =      42.00 ms /     2 runs   (   21.00 ms per token,    47.61 tokens per second)\n",
      "llama_print_timings:       total time =     175.51 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      97.14 ms /   256 runs   (    0.38 ms per token,  2635.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     330.52 ms /   426 tokens (    0.78 ms per token,  1288.87 tokens per second)\n",
      "llama_print_timings:        eval time =    5571.18 ms /   255 runs   (   21.85 ms per token,    45.77 tokens per second)\n",
      "llama_print_timings:       total time =    6907.76 ms /   681 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.78 ms /   119 runs   (    0.38 ms per token,  2657.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.28 ms /   116 tokens (    1.15 ms per token,   870.37 tokens per second)\n",
      "llama_print_timings:        eval time =    2435.54 ms /   118 runs   (   20.64 ms per token,    48.45 tokens per second)\n",
      "llama_print_timings:       total time =    3014.94 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.44 ms /    22 runs   (    0.38 ms per token,  2606.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.70 ms /    57 tokens (    2.14 ms per token,   468.38 tokens per second)\n",
      "llama_print_timings:        eval time =     462.51 ms /    21 runs   (   22.02 ms per token,    45.40 tokens per second)\n",
      "llama_print_timings:       total time =     669.39 ms /    78 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.10 ms /    23 runs   (    0.40 ms per token,  2526.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.31 ms /   105 tokens (    1.45 ms per token,   689.38 tokens per second)\n",
      "llama_print_timings:        eval time =     458.96 ms /    22 runs   (   20.86 ms per token,    47.93 tokens per second)\n",
      "llama_print_timings:       total time =     701.73 ms /   127 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      58.90 ms /   157 runs   (    0.38 ms per token,  2665.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     185.51 ms /   144 tokens (    1.29 ms per token,   776.23 tokens per second)\n",
      "llama_print_timings:        eval time =    3243.72 ms /   156 runs   (   20.79 ms per token,    48.09 tokens per second)\n",
      "llama_print_timings:       total time =    4024.95 ms /   300 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.84 ms /   132 runs   (    0.36 ms per token,  2759.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     338.74 ms /   445 tokens (    0.76 ms per token,  1313.68 tokens per second)\n",
      "llama_print_timings:        eval time =    2882.03 ms /   131 runs   (   22.00 ms per token,    45.45 tokens per second)\n",
      "llama_print_timings:       total time =    3720.37 ms /   576 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      57.41 ms /   152 runs   (    0.38 ms per token,  2647.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     185.00 ms /   144 tokens (    1.28 ms per token,   778.38 tokens per second)\n",
      "llama_print_timings:        eval time =    3175.24 ms /   151 runs   (   21.03 ms per token,    47.56 tokens per second)\n",
      "llama_print_timings:       total time =    3937.72 ms /   295 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.56 ms /    95 runs   (    0.38 ms per token,  2598.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     192.24 ms /   131 tokens (    1.47 ms per token,   681.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1998.05 ms /    94 runs   (   21.26 ms per token,    47.05 tokens per second)\n",
      "llama_print_timings:       total time =    2558.23 ms /   225 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.98 ms /   120 runs   (    0.37 ms per token,  2728.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     127.59 ms /    98 tokens (    1.30 ms per token,   768.10 tokens per second)\n",
      "llama_print_timings:        eval time =    2346.20 ms /   119 runs   (   19.72 ms per token,    50.72 tokens per second)\n",
      "llama_print_timings:       total time =    2933.60 ms /   217 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.98 ms /   120 runs   (    0.38 ms per token,  2609.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.85 ms /    40 tokens (    3.07 ms per token,   325.60 tokens per second)\n",
      "llama_print_timings:        eval time =    2692.74 ms /   119 runs   (   22.63 ms per token,    44.19 tokens per second)\n",
      "llama_print_timings:       total time =    3309.10 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.03 ms /   124 runs   (    0.38 ms per token,  2636.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.08 ms /   126 tokens (    1.25 ms per token,   802.16 tokens per second)\n",
      "llama_print_timings:        eval time =    2704.99 ms /   123 runs   (   21.99 ms per token,    45.47 tokens per second)\n",
      "llama_print_timings:       total time =    3341.33 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.65 ms /   107 runs   (    0.39 ms per token,  2568.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.74 ms /    85 tokens (    1.70 ms per token,   587.27 tokens per second)\n",
      "llama_print_timings:        eval time =    2464.01 ms /   106 runs   (   23.25 ms per token,    43.02 tokens per second)\n",
      "llama_print_timings:       total time =    3057.09 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.37 ms /    19 runs   (    0.39 ms per token,  2579.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     117.61 ms /    63 tokens (    1.87 ms per token,   535.65 tokens per second)\n",
      "llama_print_timings:        eval time =     405.04 ms /    18 runs   (   22.50 ms per token,    44.44 tokens per second)\n",
      "llama_print_timings:       total time =     596.54 ms /    81 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      66.01 ms /   176 runs   (    0.38 ms per token,  2666.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     299.53 ms /   323 tokens (    0.93 ms per token,  1078.37 tokens per second)\n",
      "llama_print_timings:        eval time =    3802.47 ms /   175 runs   (   21.73 ms per token,    46.02 tokens per second)\n",
      "llama_print_timings:       total time =    4774.43 ms /   498 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.92 ms /   109 runs   (    0.37 ms per token,  2730.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.54 ms /    98 tokens (    1.61 ms per token,   622.06 tokens per second)\n",
      "llama_print_timings:        eval time =    2242.69 ms /   108 runs   (   20.77 ms per token,    48.16 tokens per second)\n",
      "llama_print_timings:       total time =    2811.91 ms /   206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      26.99 ms /    72 runs   (    0.37 ms per token,  2667.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.07 ms /    55 tokens (    2.38 ms per token,   419.63 tokens per second)\n",
      "llama_print_timings:        eval time =    1524.46 ms /    71 runs   (   21.47 ms per token,    46.57 tokens per second)\n",
      "llama_print_timings:       total time =    1931.30 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.27 ms /   128 runs   (    0.36 ms per token,  2766.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     405.84 ms /   475 tokens (    0.85 ms per token,  1170.40 tokens per second)\n",
      "llama_print_timings:        eval time =    2808.91 ms /   127 runs   (   22.12 ms per token,    45.21 tokens per second)\n",
      "llama_print_timings:       total time =    3701.64 ms /   602 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.12 ms /   124 runs   (    0.39 ms per token,  2576.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     202.65 ms /   202 tokens (    1.00 ms per token,   996.81 tokens per second)\n",
      "llama_print_timings:        eval time =    2694.93 ms /   123 runs   (   21.91 ms per token,    45.64 tokens per second)\n",
      "llama_print_timings:       total time =    3375.39 ms /   325 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.80 ms /   131 runs   (    0.37 ms per token,  2684.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.78 ms /   106 tokens (    1.46 ms per token,   684.86 tokens per second)\n",
      "llama_print_timings:        eval time =    2792.72 ms /   130 runs   (   21.48 ms per token,    46.55 tokens per second)\n",
      "llama_print_timings:       total time =    3444.55 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.32 ms /    88 runs   (    0.38 ms per token,  2641.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     746.70 ms /   919 tokens (    0.81 ms per token,  1230.75 tokens per second)\n",
      "llama_print_timings:        eval time =    1894.23 ms /    87 runs   (   21.77 ms per token,    45.93 tokens per second)\n",
      "llama_print_timings:       total time =    2969.76 ms /  1006 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      66.63 ms /   174 runs   (    0.38 ms per token,  2611.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.53 ms /   169 tokens (    1.27 ms per token,   787.76 tokens per second)\n",
      "llama_print_timings:        eval time =    3745.50 ms /   173 runs   (   21.65 ms per token,    46.19 tokens per second)\n",
      "llama_print_timings:       total time =    4626.28 ms /   342 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.48 ms /   129 runs   (    0.38 ms per token,  2660.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     381.62 ms /   465 tokens (    0.82 ms per token,  1218.48 tokens per second)\n",
      "llama_print_timings:        eval time =    2884.83 ms /   128 runs   (   22.54 ms per token,    44.37 tokens per second)\n",
      "llama_print_timings:       total time =    3760.94 ms /   593 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.34 ms /    23 runs   (    0.41 ms per token,  2462.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.09 ms /   126 tokens (    1.25 ms per token,   797.02 tokens per second)\n",
      "llama_print_timings:        eval time =     464.96 ms /    22 runs   (   21.13 ms per token,    47.32 tokens per second)\n",
      "llama_print_timings:       total time =     725.75 ms /   148 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.35 ms /    83 runs   (    0.38 ms per token,  2647.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     184.74 ms /   153 tokens (    1.21 ms per token,   828.21 tokens per second)\n",
      "llama_print_timings:        eval time =    1828.18 ms /    82 runs   (   22.29 ms per token,    44.85 tokens per second)\n",
      "llama_print_timings:       total time =    2330.12 ms /   235 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      76.96 ms /   206 runs   (    0.37 ms per token,  2676.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     312.24 ms /   286 tokens (    1.09 ms per token,   915.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4441.59 ms /   205 runs   (   21.67 ms per token,    46.15 tokens per second)\n",
      "llama_print_timings:       total time =    5590.32 ms /   491 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.24 ms /     3 runs   (    0.41 ms per token,  2425.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.53 ms /    84 tokens (    1.82 ms per token,   550.70 tokens per second)\n",
      "llama_print_timings:        eval time =      53.80 ms /     2 runs   (   26.90 ms per token,    37.17 tokens per second)\n",
      "llama_print_timings:       total time =     219.79 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      25.64 ms /    65 runs   (    0.39 ms per token,  2535.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.49 ms /   114 tokens (    1.43 ms per token,   701.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1377.75 ms /    64 runs   (   21.53 ms per token,    46.45 tokens per second)\n",
      "llama_print_timings:       total time =    1805.90 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.23 ms /    96 runs   (    0.39 ms per token,  2578.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.38 ms /   102 tokens (    1.52 ms per token,   656.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1975.82 ms /    95 runs   (   20.80 ms per token,    48.08 tokens per second)\n",
      "llama_print_timings:       total time =    2499.72 ms /   197 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      95.05 ms /   256 runs   (    0.37 ms per token,  2693.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     189.30 ms /   147 tokens (    1.29 ms per token,   776.54 tokens per second)\n",
      "llama_print_timings:        eval time =    5635.43 ms /   255 runs   (   22.10 ms per token,    45.25 tokens per second)\n",
      "llama_print_timings:       total time =    6846.23 ms /   402 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.59 ms /   119 runs   (    0.37 ms per token,  2668.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     228.09 ms /   218 tokens (    1.05 ms per token,   955.74 tokens per second)\n",
      "llama_print_timings:        eval time =    2508.89 ms /   118 runs   (   21.26 ms per token,    47.03 tokens per second)\n",
      "llama_print_timings:       total time =    3181.47 ms /   336 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.69 ms /    87 runs   (    0.38 ms per token,  2661.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     177.57 ms /   188 tokens (    0.94 ms per token,  1058.71 tokens per second)\n",
      "llama_print_timings:        eval time =    1747.42 ms /    86 runs   (   20.32 ms per token,    49.22 tokens per second)\n",
      "llama_print_timings:       total time =    2253.40 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.70 ms /    74 runs   (    0.37 ms per token,  2671.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.72 ms /    90 tokens (    1.47 ms per token,   678.12 tokens per second)\n",
      "llama_print_timings:        eval time =    1514.28 ms /    73 runs   (   20.74 ms per token,    48.21 tokens per second)\n",
      "llama_print_timings:       total time =    1923.05 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.85 ms /    17 runs   (    0.40 ms per token,  2482.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.66 ms /    58 tokens (    2.11 ms per token,   472.85 tokens per second)\n",
      "llama_print_timings:        eval time =     346.71 ms /    16 runs   (   21.67 ms per token,    46.15 tokens per second)\n",
      "llama_print_timings:       total time =     547.76 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       0.93 ms /     3 runs   (    0.31 ms per token,  3211.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     188.67 ms /   144 tokens (    1.31 ms per token,   763.25 tokens per second)\n",
      "llama_print_timings:        eval time =      49.39 ms /     2 runs   (   24.70 ms per token,    40.49 tokens per second)\n",
      "llama_print_timings:       total time =     249.26 ms /   146 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.43 ms /    89 runs   (    0.39 ms per token,  2585.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     232.07 ms /   194 tokens (    1.20 ms per token,   835.96 tokens per second)\n",
      "llama_print_timings:        eval time =    1966.99 ms /    88 runs   (   22.35 ms per token,    44.74 tokens per second)\n",
      "llama_print_timings:       total time =    2557.48 ms /   282 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.20 ms /   125 runs   (    0.38 ms per token,  2648.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     142.75 ms /    91 tokens (    1.57 ms per token,   637.50 tokens per second)\n",
      "llama_print_timings:        eval time =    2645.28 ms /   124 runs   (   21.33 ms per token,    46.88 tokens per second)\n",
      "llama_print_timings:       total time =    3267.43 ms /   215 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.91 ms /   102 runs   (    0.36 ms per token,  2763.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.88 ms /   101 tokens (    1.34 ms per token,   748.80 tokens per second)\n",
      "llama_print_timings:        eval time =    2069.21 ms /   101 runs   (   20.49 ms per token,    48.81 tokens per second)\n",
      "llama_print_timings:       total time =    2588.39 ms /   202 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.79 ms /    17 runs   (    0.40 ms per token,  2502.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     191.19 ms /   202 tokens (    0.95 ms per token,  1056.52 tokens per second)\n",
      "llama_print_timings:        eval time =     377.99 ms /    16 runs   (   23.62 ms per token,    42.33 tokens per second)\n",
      "llama_print_timings:       total time =     640.37 ms /   218 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.29 ms /   115 runs   (    0.37 ms per token,  2719.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     136.93 ms /    45 tokens (    3.04 ms per token,   328.64 tokens per second)\n",
      "llama_print_timings:        eval time =    2379.22 ms /   114 runs   (   20.87 ms per token,    47.91 tokens per second)\n",
      "llama_print_timings:       total time =    2953.32 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.32 ms /    90 runs   (    0.37 ms per token,  2701.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     183.91 ms /   175 tokens (    1.05 ms per token,   951.56 tokens per second)\n",
      "llama_print_timings:        eval time =    1748.19 ms /    89 runs   (   19.64 ms per token,    50.91 tokens per second)\n",
      "llama_print_timings:       total time =    2264.43 ms /   264 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.28 ms /    22 runs   (    0.38 ms per token,  2658.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.48 ms /    33 tokens (    3.62 ms per token,   276.21 tokens per second)\n",
      "llama_print_timings:        eval time =     425.21 ms /    21 runs   (   20.25 ms per token,    49.39 tokens per second)\n",
      "llama_print_timings:       total time =     627.00 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.87 ms /   117 runs   (    0.38 ms per token,  2607.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     258.85 ms /   259 tokens (    1.00 ms per token,  1000.56 tokens per second)\n",
      "llama_print_timings:        eval time =    2467.49 ms /   116 runs   (   21.27 ms per token,    47.01 tokens per second)\n",
      "llama_print_timings:       total time =    3197.28 ms /   375 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.55 ms /    74 runs   (    0.37 ms per token,  2686.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     169.51 ms /   135 tokens (    1.26 ms per token,   796.42 tokens per second)\n",
      "llama_print_timings:        eval time =    1573.71 ms /    73 runs   (   21.56 ms per token,    46.39 tokens per second)\n",
      "llama_print_timings:       total time =    2018.11 ms /   208 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.60 ms /   139 runs   (    0.38 ms per token,  2642.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     307.31 ms /   447 tokens (    0.69 ms per token,  1454.54 tokens per second)\n",
      "llama_print_timings:        eval time =    3174.86 ms /   138 runs   (   23.01 ms per token,    43.47 tokens per second)\n",
      "llama_print_timings:       total time =    4024.25 ms /   585 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.56 ms /    21 runs   (    0.36 ms per token,  2778.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.58 ms /    45 tokens (    3.08 ms per token,   324.72 tokens per second)\n",
      "llama_print_timings:        eval time =     427.34 ms /    20 runs   (   21.37 ms per token,    46.80 tokens per second)\n",
      "llama_print_timings:       total time =     645.16 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.53 ms /    22 runs   (    0.39 ms per token,  2578.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.96 ms /    46 tokens (    2.67 ms per token,   374.11 tokens per second)\n",
      "llama_print_timings:        eval time =     449.33 ms /    21 runs   (   21.40 ms per token,    46.74 tokens per second)\n",
      "llama_print_timings:       total time =     659.53 ms /    67 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      25.01 ms /    66 runs   (    0.38 ms per token,  2639.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.93 ms /    61 tokens (    1.88 ms per token,   530.74 tokens per second)\n",
      "llama_print_timings:        eval time =    1378.11 ms /    65 runs   (   21.20 ms per token,    47.17 tokens per second)\n",
      "llama_print_timings:       total time =    1740.34 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.58 ms /   144 runs   (    0.37 ms per token,  2687.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     147.69 ms /    89 tokens (    1.66 ms per token,   602.61 tokens per second)\n",
      "llama_print_timings:        eval time =    2959.01 ms /   143 runs   (   20.69 ms per token,    48.33 tokens per second)\n",
      "llama_print_timings:       total time =    3682.01 ms /   232 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     3 runs   (    0.37 ms per token,  2676.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     192.31 ms /   140 tokens (    1.37 ms per token,   727.98 tokens per second)\n",
      "llama_print_timings:        eval time =      48.80 ms /     2 runs   (   24.40 ms per token,    40.98 tokens per second)\n",
      "llama_print_timings:       total time =     253.33 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.41 ms /    27 runs   (    0.39 ms per token,  2592.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     127.89 ms /    54 tokens (    2.37 ms per token,   422.23 tokens per second)\n",
      "llama_print_timings:        eval time =     543.79 ms /    26 runs   (   20.91 ms per token,    47.81 tokens per second)\n",
      "llama_print_timings:       total time =     776.57 ms /    80 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.76 ms /    96 runs   (    0.36 ms per token,  2761.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     276.72 ms /   314 tokens (    0.88 ms per token,  1134.71 tokens per second)\n",
      "llama_print_timings:        eval time =    1990.73 ms /    95 runs   (   20.96 ms per token,    47.72 tokens per second)\n",
      "llama_print_timings:       total time =    2625.28 ms /   409 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.04 ms /    85 runs   (    0.38 ms per token,  2652.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     143.32 ms /    78 tokens (    1.84 ms per token,   544.23 tokens per second)\n",
      "llama_print_timings:        eval time =    1730.46 ms /    84 runs   (   20.60 ms per token,    48.54 tokens per second)\n",
      "llama_print_timings:       total time =    2200.53 ms /   162 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.31 ms /   120 runs   (    0.38 ms per token,  2648.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.37 ms /   108 tokens (    1.21 ms per token,   828.44 tokens per second)\n",
      "llama_print_timings:        eval time =    2571.76 ms /   119 runs   (   21.61 ms per token,    46.27 tokens per second)\n",
      "llama_print_timings:       total time =    3166.33 ms /   227 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.10 ms /     3 runs   (    0.37 ms per token,  2714.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.04 ms /    80 tokens (    1.90 ms per token,   526.18 tokens per second)\n",
      "llama_print_timings:        eval time =      55.51 ms /     2 runs   (   27.75 ms per token,    36.03 tokens per second)\n",
      "llama_print_timings:       total time =     217.90 ms /    82 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.72 ms /    22 runs   (    0.40 ms per token,  2524.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.61 ms /    89 tokens (    1.83 ms per token,   547.33 tokens per second)\n",
      "llama_print_timings:        eval time =     474.48 ms /    21 runs   (   22.59 ms per token,    44.26 tokens per second)\n",
      "llama_print_timings:       total time =     721.56 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      71.22 ms /   189 runs   (    0.38 ms per token,  2653.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.92 ms /   161 tokens (    1.33 ms per token,   752.61 tokens per second)\n",
      "llama_print_timings:        eval time =    3990.98 ms /   188 runs   (   21.23 ms per token,    47.11 tokens per second)\n",
      "llama_print_timings:       total time =    4940.94 ms /   349 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.94 ms /    21 runs   (    0.38 ms per token,  2645.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.68 ms /    42 tokens (    3.21 ms per token,   311.84 tokens per second)\n",
      "llama_print_timings:        eval time =     410.57 ms /    20 runs   (   20.53 ms per token,    48.71 tokens per second)\n",
      "llama_print_timings:       total time =     628.83 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.11 ms /    18 runs   (    0.40 ms per token,  2531.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.68 ms /    97 tokens (    1.38 ms per token,   725.60 tokens per second)\n",
      "llama_print_timings:        eval time =     352.80 ms /    17 runs   (   20.75 ms per token,    48.19 tokens per second)\n",
      "llama_print_timings:       total time =     560.85 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.55 ms /    17 runs   (    0.39 ms per token,  2596.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     136.72 ms /   112 tokens (    1.22 ms per token,   819.19 tokens per second)\n",
      "llama_print_timings:        eval time =     343.60 ms /    16 runs   (   21.47 ms per token,    46.57 tokens per second)\n",
      "llama_print_timings:       total time =     551.25 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.80 ms /   140 runs   (    0.38 ms per token,  2651.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.22 ms /   141 tokens (    1.29 ms per token,   778.05 tokens per second)\n",
      "llama_print_timings:        eval time =    2908.12 ms /   139 runs   (   20.92 ms per token,    47.80 tokens per second)\n",
      "llama_print_timings:       total time =    3627.61 ms /   280 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.77 ms /    76 runs   (    0.38 ms per token,  2641.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.57 ms /    60 tokens (    2.24 ms per token,   445.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1552.79 ms /    75 runs   (   20.70 ms per token,    48.30 tokens per second)\n",
      "llama_print_timings:       total time =    1975.92 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.40 ms /    22 runs   (    0.38 ms per token,  2619.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.54 ms /    66 tokens (    2.30 ms per token,   435.52 tokens per second)\n",
      "llama_print_timings:        eval time =     490.19 ms /    21 runs   (   23.34 ms per token,    42.84 tokens per second)\n",
      "llama_print_timings:       total time =     742.12 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.12 ms /    97 runs   (    0.39 ms per token,  2544.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     205.43 ms /   173 tokens (    1.19 ms per token,   842.14 tokens per second)\n",
      "llama_print_timings:        eval time =    2143.95 ms /    96 runs   (   22.33 ms per token,    44.78 tokens per second)\n",
      "llama_print_timings:       total time =    2735.19 ms /   269 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.97 ms /    93 runs   (    0.42 ms per token,  2386.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.47 ms /   283 tokens (    0.95 ms per token,  1058.08 tokens per second)\n",
      "llama_print_timings:        eval time =    2041.17 ms /    92 runs   (   22.19 ms per token,    45.07 tokens per second)\n",
      "llama_print_timings:       total time =    2717.51 ms /   375 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.75 ms /    78 runs   (    0.41 ms per token,  2457.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.03 ms /   101 tokens (    1.54 ms per token,   647.33 tokens per second)\n",
      "llama_print_timings:        eval time =    1659.17 ms /    77 runs   (   21.55 ms per token,    46.41 tokens per second)\n",
      "llama_print_timings:       total time =    2167.19 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.13 ms /    93 runs   (    0.38 ms per token,  2647.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     179.55 ms /   144 tokens (    1.25 ms per token,   802.02 tokens per second)\n",
      "llama_print_timings:        eval time =    1974.10 ms /    92 runs   (   21.46 ms per token,    46.60 tokens per second)\n",
      "llama_print_timings:       total time =    2535.76 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.54 ms /   109 runs   (    0.37 ms per token,  2688.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     270.10 ms /   313 tokens (    0.86 ms per token,  1158.83 tokens per second)\n",
      "llama_print_timings:        eval time =    2309.67 ms /   108 runs   (   21.39 ms per token,    46.76 tokens per second)\n",
      "llama_print_timings:       total time =    3013.48 ms /   421 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.15 ms /    21 runs   (    0.39 ms per token,  2577.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.21 ms /   139 tokens (    1.15 ms per token,   873.08 tokens per second)\n",
      "llama_print_timings:        eval time =     409.75 ms /    20 runs   (   20.49 ms per token,    48.81 tokens per second)\n",
      "llama_print_timings:       total time =     654.61 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.44 ms /   101 runs   (    0.38 ms per token,  2627.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     195.58 ms /   164 tokens (    1.19 ms per token,   838.55 tokens per second)\n",
      "llama_print_timings:        eval time =    2224.17 ms /   100 runs   (   22.24 ms per token,    44.96 tokens per second)\n",
      "llama_print_timings:       total time =    2839.74 ms /   264 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2533.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.95 ms /    95 tokens (    1.58 ms per token,   633.54 tokens per second)\n",
      "llama_print_timings:        eval time =      60.58 ms /     2 runs   (   30.29 ms per token,    33.02 tokens per second)\n",
      "llama_print_timings:       total time =     220.96 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2540.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.77 ms /    86 tokens (    1.81 ms per token,   552.11 tokens per second)\n",
      "llama_print_timings:        eval time =      57.99 ms /     2 runs   (   29.00 ms per token,    34.49 tokens per second)\n",
      "llama_print_timings:       total time =     225.92 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      60.70 ms /   161 runs   (    0.38 ms per token,  2652.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     225.49 ms /   231 tokens (    0.98 ms per token,  1024.44 tokens per second)\n",
      "llama_print_timings:        eval time =    3554.27 ms /   160 runs   (   22.21 ms per token,    45.02 tokens per second)\n",
      "llama_print_timings:       total time =    4414.77 ms /   391 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.69 ms /    22 runs   (    0.40 ms per token,  2530.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.23 ms /    74 tokens (    1.81 ms per token,   551.28 tokens per second)\n",
      "llama_print_timings:        eval time =     477.94 ms /    21 runs   (   22.76 ms per token,    43.94 tokens per second)\n",
      "llama_print_timings:       total time =     712.19 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.32 ms /    19 runs   (    0.39 ms per token,  2595.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     127.90 ms /    56 tokens (    2.28 ms per token,   437.84 tokens per second)\n",
      "llama_print_timings:        eval time =     387.80 ms /    18 runs   (   21.54 ms per token,    46.42 tokens per second)\n",
      "llama_print_timings:       total time =     590.55 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.72 ms /   135 runs   (    0.38 ms per token,  2661.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     191.06 ms /   169 tokens (    1.13 ms per token,   884.53 tokens per second)\n",
      "llama_print_timings:        eval time =    2803.49 ms /   134 runs   (   20.92 ms per token,    47.80 tokens per second)\n",
      "llama_print_timings:       total time =    3511.25 ms /   303 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.28 ms /   129 runs   (    0.37 ms per token,  2672.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     169.93 ms /   198 tokens (    0.86 ms per token,  1165.19 tokens per second)\n",
      "llama_print_timings:        eval time =    2603.41 ms /   128 runs   (   20.34 ms per token,    49.17 tokens per second)\n",
      "llama_print_timings:       total time =    3280.14 ms /   326 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2643.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     182.66 ms /   154 tokens (    1.19 ms per token,   843.09 tokens per second)\n",
      "llama_print_timings:        eval time =      46.06 ms /     2 runs   (   23.03 ms per token,    43.42 tokens per second)\n",
      "llama_print_timings:       total time =     239.41 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.67 ms /   108 runs   (    0.39 ms per token,  2591.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.00 ms /    85 tokens (    1.64 ms per token,   611.50 tokens per second)\n",
      "llama_print_timings:        eval time =    2229.03 ms /   107 runs   (   20.83 ms per token,    48.00 tokens per second)\n",
      "llama_print_timings:       total time =    2793.41 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.68 ms /    17 runs   (    0.39 ms per token,  2546.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.90 ms /   104 tokens (    1.28 ms per token,   782.57 tokens per second)\n",
      "llama_print_timings:        eval time =     380.91 ms /    16 runs   (   23.81 ms per token,    42.00 tokens per second)\n",
      "llama_print_timings:       total time =     583.37 ms /   120 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.97 ms /   141 runs   (    0.38 ms per token,  2661.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     188.33 ms /   138 tokens (    1.36 ms per token,   732.77 tokens per second)\n",
      "llama_print_timings:        eval time =    3046.84 ms /   140 runs   (   21.76 ms per token,    45.95 tokens per second)\n",
      "llama_print_timings:       total time =    3791.00 ms /   278 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.60 ms /   105 runs   (    0.39 ms per token,  2586.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     546.92 ms /   551 tokens (    0.99 ms per token,  1007.46 tokens per second)\n",
      "llama_print_timings:        eval time =    2273.10 ms /   104 runs   (   21.86 ms per token,    45.75 tokens per second)\n",
      "llama_print_timings:       total time =    3239.17 ms /   655 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.60 ms /   121 runs   (    0.39 ms per token,  2596.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.99 ms /    70 tokens (    2.03 ms per token,   493.00 tokens per second)\n",
      "llama_print_timings:        eval time =    2609.62 ms /   120 runs   (   21.75 ms per token,    45.98 tokens per second)\n",
      "llama_print_timings:       total time =    3237.84 ms /   190 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.58 ms /    94 runs   (    0.37 ms per token,  2718.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     360.18 ms /   443 tokens (    0.81 ms per token,  1229.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1944.31 ms /    93 runs   (   20.91 ms per token,    47.83 tokens per second)\n",
      "llama_print_timings:       total time =    2656.94 ms /   536 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.83 ms /   131 runs   (    0.37 ms per token,  2682.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     197.28 ms /   213 tokens (    0.93 ms per token,  1079.69 tokens per second)\n",
      "llama_print_timings:        eval time =    2756.58 ms /   130 runs   (   21.20 ms per token,    47.16 tokens per second)\n",
      "llama_print_timings:       total time =    3445.48 ms /   343 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      94.78 ms /   254 runs   (    0.37 ms per token,  2680.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     186.54 ms /   244 tokens (    0.76 ms per token,  1308.04 tokens per second)\n",
      "llama_print_timings:        eval time =    5400.26 ms /   253 runs   (   21.34 ms per token,    46.85 tokens per second)\n",
      "llama_print_timings:       total time =    6586.78 ms /   497 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.27 ms /    88 runs   (    0.38 ms per token,  2645.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     197.38 ms /   139 tokens (    1.42 ms per token,   704.23 tokens per second)\n",
      "llama_print_timings:        eval time =    1953.15 ms /    87 runs   (   22.45 ms per token,    44.54 tokens per second)\n",
      "llama_print_timings:       total time =    2487.55 ms /   226 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.83 ms /    87 runs   (    0.38 ms per token,  2649.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     222.31 ms /   200 tokens (    1.11 ms per token,   899.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1806.09 ms /    86 runs   (   21.00 ms per token,    47.62 tokens per second)\n",
      "llama_print_timings:       total time =    2362.60 ms /   286 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.71 ms /    19 runs   (    0.41 ms per token,  2463.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.30 ms /    41 tokens (    3.45 ms per token,   290.16 tokens per second)\n",
      "llama_print_timings:        eval time =     413.56 ms /    18 runs   (   22.98 ms per token,    43.52 tokens per second)\n",
      "llama_print_timings:       total time =     642.08 ms /    59 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.41 ms /   122 runs   (    0.37 ms per token,  2686.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     191.84 ms /   151 tokens (    1.27 ms per token,   787.13 tokens per second)\n",
      "llama_print_timings:        eval time =    2569.34 ms /   121 runs   (   21.23 ms per token,    47.09 tokens per second)\n",
      "llama_print_timings:       total time =    3236.38 ms /   272 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    17 runs   (    0.38 ms per token,  2637.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     196.66 ms /   202 tokens (    0.97 ms per token,  1027.15 tokens per second)\n",
      "llama_print_timings:        eval time =     363.50 ms /    16 runs   (   22.72 ms per token,    44.02 tokens per second)\n",
      "llama_print_timings:       total time =     647.57 ms /   218 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.45 ms /   108 runs   (    0.39 ms per token,  2544.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.77 ms /    81 tokens (    1.63 ms per token,   614.71 tokens per second)\n",
      "llama_print_timings:        eval time =    2331.51 ms /   107 runs   (   21.79 ms per token,    45.89 tokens per second)\n",
      "llama_print_timings:       total time =    2904.17 ms /   188 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.09 ms /     3 runs   (    0.36 ms per token,  2742.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     137.06 ms /    53 tokens (    2.59 ms per token,   386.70 tokens per second)\n",
      "llama_print_timings:        eval time =      46.08 ms /     2 runs   (   23.04 ms per token,    43.40 tokens per second)\n",
      "llama_print_timings:       total time =     195.31 ms /    55 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      58.80 ms /   155 runs   (    0.38 ms per token,  2635.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     170.08 ms /   113 tokens (    1.51 ms per token,   664.38 tokens per second)\n",
      "llama_print_timings:        eval time =    3329.15 ms /   154 runs   (   21.62 ms per token,    46.26 tokens per second)\n",
      "llama_print_timings:       total time =    4132.99 ms /   267 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     3 runs   (    0.37 ms per token,  2673.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.92 ms /    49 tokens (    2.45 ms per token,   408.59 tokens per second)\n",
      "llama_print_timings:        eval time =      38.07 ms /     2 runs   (   19.04 ms per token,    52.53 tokens per second)\n",
      "llama_print_timings:       total time =     169.00 ms /    51 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      92.12 ms /   248 runs   (    0.37 ms per token,  2692.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     586.94 ms /   707 tokens (    0.83 ms per token,  1204.55 tokens per second)\n",
      "llama_print_timings:        eval time =    5530.06 ms /   247 runs   (   22.39 ms per token,    44.67 tokens per second)\n",
      "llama_print_timings:       total time =    7131.59 ms /   954 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.37 ms /   112 runs   (    0.37 ms per token,  2707.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     175.46 ms /   138 tokens (    1.27 ms per token,   786.53 tokens per second)\n",
      "llama_print_timings:        eval time =    2426.80 ms /   111 runs   (   21.86 ms per token,    45.74 tokens per second)\n",
      "llama_print_timings:       total time =    3047.85 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      19.44 ms /    51 runs   (    0.38 ms per token,  2623.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.86 ms /    99 tokens (    1.40 ms per token,   712.94 tokens per second)\n",
      "llama_print_timings:        eval time =    1077.96 ms /    50 runs   (   21.56 ms per token,    46.38 tokens per second)\n",
      "llama_print_timings:       total time =    1414.39 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.69 ms /   135 runs   (    0.38 ms per token,  2663.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.39 ms /   172 tokens (    1.16 ms per token,   862.64 tokens per second)\n",
      "llama_print_timings:        eval time =    2905.08 ms /   134 runs   (   21.68 ms per token,    46.13 tokens per second)\n",
      "llama_print_timings:       total time =    3631.59 ms /   306 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.38 ms /    81 runs   (    0.39 ms per token,  2580.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.08 ms /   183 tokens (    1.18 ms per token,   846.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1769.98 ms /    80 runs   (   22.12 ms per token,    45.20 tokens per second)\n",
      "llama_print_timings:       total time =    2301.20 ms /   263 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.80 ms /    77 runs   (    0.37 ms per token,  2673.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.31 ms /    71 tokens (    1.99 ms per token,   502.43 tokens per second)\n",
      "llama_print_timings:        eval time =    1668.83 ms /    76 runs   (   21.96 ms per token,    45.54 tokens per second)\n",
      "llama_print_timings:       total time =    2107.09 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.97 ms /   100 runs   (    0.38 ms per token,  2633.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     146.88 ms /    55 tokens (    2.67 ms per token,   374.45 tokens per second)\n",
      "llama_print_timings:        eval time =    2076.39 ms /    99 runs   (   20.97 ms per token,    47.68 tokens per second)\n",
      "llama_print_timings:       total time =    2608.82 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.16 ms /    21 runs   (    0.39 ms per token,  2573.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     180.13 ms /   135 tokens (    1.33 ms per token,   749.48 tokens per second)\n",
      "llama_print_timings:        eval time =     426.03 ms /    20 runs   (   21.30 ms per token,    46.95 tokens per second)\n",
      "llama_print_timings:       total time =     689.75 ms /   155 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.59 ms /    17 runs   (    0.39 ms per token,  2578.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.32 ms /    91 tokens (    1.67 ms per token,   597.42 tokens per second)\n",
      "llama_print_timings:        eval time =     366.57 ms /    16 runs   (   22.91 ms per token,    43.65 tokens per second)\n",
      "llama_print_timings:       total time =     591.92 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.35 ms /   136 runs   (    0.36 ms per token,  2755.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     210.25 ms /   197 tokens (    1.07 ms per token,   936.98 tokens per second)\n",
      "llama_print_timings:        eval time =    2901.21 ms /   135 runs   (   21.49 ms per token,    46.53 tokens per second)\n",
      "llama_print_timings:       total time =    3634.67 ms /   332 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    17 runs   (    0.38 ms per token,  2609.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.30 ms /    46 tokens (    3.22 ms per token,   310.18 tokens per second)\n",
      "llama_print_timings:        eval time =     367.08 ms /    16 runs   (   22.94 ms per token,    43.59 tokens per second)\n",
      "llama_print_timings:       total time =     582.16 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.41 ms /    99 runs   (    0.37 ms per token,  2718.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.60 ms /    96 tokens (    1.52 ms per token,   659.36 tokens per second)\n",
      "llama_print_timings:        eval time =    2084.87 ms /    98 runs   (   21.27 ms per token,    47.01 tokens per second)\n",
      "llama_print_timings:       total time =    2604.90 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.88 ms /    22 runs   (    0.40 ms per token,  2476.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.99 ms /    66 tokens (    2.03 ms per token,   492.57 tokens per second)\n",
      "llama_print_timings:        eval time =     459.84 ms /    21 runs   (   21.90 ms per token,    45.67 tokens per second)\n",
      "llama_print_timings:       total time =     692.67 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.46 ms /   102 runs   (    0.38 ms per token,  2652.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     150.58 ms /    84 tokens (    1.79 ms per token,   557.84 tokens per second)\n",
      "llama_print_timings:        eval time =    2155.99 ms /   101 runs   (   21.35 ms per token,    46.85 tokens per second)\n",
      "llama_print_timings:       total time =    2701.71 ms /   185 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       0.93 ms /     3 runs   (    0.31 ms per token,  3236.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     360.48 ms /   418 tokens (    0.86 ms per token,  1159.56 tokens per second)\n",
      "llama_print_timings:        eval time =      48.88 ms /     2 runs   (   24.44 ms per token,    40.92 tokens per second)\n",
      "llama_print_timings:       total time =     420.73 ms /   420 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.11 ms /     3 runs   (    0.37 ms per token,  2702.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.83 ms /    59 tokens (    2.59 ms per token,   386.04 tokens per second)\n",
      "llama_print_timings:        eval time =      43.90 ms /     2 runs   (   21.95 ms per token,    45.56 tokens per second)\n",
      "llama_print_timings:       total time =     208.69 ms /    61 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.09 ms /   140 runs   (    0.38 ms per token,  2637.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.83 ms /   110 tokens (    1.41 ms per token,   710.47 tokens per second)\n",
      "llama_print_timings:        eval time =    3116.20 ms /   139 runs   (   22.42 ms per token,    44.61 tokens per second)\n",
      "llama_print_timings:       total time =    3862.74 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     3 runs   (    0.39 ms per token,  2583.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.93 ms /    69 tokens (    2.04 ms per token,   489.60 tokens per second)\n",
      "llama_print_timings:        eval time =      42.39 ms /     2 runs   (   21.20 ms per token,    47.18 tokens per second)\n",
      "llama_print_timings:       total time =     196.49 ms /    71 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.84 ms /   109 runs   (    0.37 ms per token,  2668.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.00 ms /   126 tokens (    1.10 ms per token,   906.47 tokens per second)\n",
      "llama_print_timings:        eval time =    1944.60 ms /   108 runs   (   18.01 ms per token,    55.54 tokens per second)\n",
      "llama_print_timings:       total time =    2481.73 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.42 ms /    94 runs   (    0.38 ms per token,  2653.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.23 ms /    68 tokens (    1.65 ms per token,   605.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1811.50 ms /    93 runs   (   19.48 ms per token,    51.34 tokens per second)\n",
      "llama_print_timings:       total time =    2280.79 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.94 ms /    23 runs   (    0.39 ms per token,  2572.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.02 ms /   168 tokens (    1.08 ms per token,   928.08 tokens per second)\n",
      "llama_print_timings:        eval time =     402.89 ms /    22 runs   (   18.31 ms per token,    54.61 tokens per second)\n",
      "llama_print_timings:       total time =     667.62 ms /   190 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.50 ms /    95 runs   (    0.38 ms per token,  2602.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     176.38 ms /   195 tokens (    0.90 ms per token,  1105.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1827.10 ms /    94 runs   (   19.44 ms per token,    51.45 tokens per second)\n",
      "llama_print_timings:       total time =    2359.35 ms /   289 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.99 ms /    94 runs   (    0.38 ms per token,  2611.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.93 ms /   121 tokens (    1.04 ms per token,   960.82 tokens per second)\n",
      "llama_print_timings:        eval time =    1760.31 ms /    93 runs   (   18.93 ms per token,    52.83 tokens per second)\n",
      "llama_print_timings:       total time =    2235.95 ms /   214 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.71 ms /    17 runs   (    0.39 ms per token,  2535.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.14 ms /   128 tokens (    1.22 ms per token,   819.79 tokens per second)\n",
      "llama_print_timings:        eval time =     325.75 ms /    16 runs   (   20.36 ms per token,    49.12 tokens per second)\n",
      "llama_print_timings:       total time =     549.54 ms /   144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      29.71 ms /    81 runs   (    0.37 ms per token,  2726.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.20 ms /   103 tokens (    1.28 ms per token,   779.10 tokens per second)\n",
      "llama_print_timings:        eval time =    1521.09 ms /    80 runs   (   19.01 ms per token,    52.59 tokens per second)\n",
      "llama_print_timings:       total time =    1944.84 ms /   183 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      81.37 ms /   219 runs   (    0.37 ms per token,  2691.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     271.27 ms /   371 tokens (    0.73 ms per token,  1367.65 tokens per second)\n",
      "llama_print_timings:        eval time =    4245.85 ms /   218 runs   (   19.48 ms per token,    51.34 tokens per second)\n",
      "llama_print_timings:       total time =    5348.81 ms /   589 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      51.05 ms /   135 runs   (    0.38 ms per token,  2644.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     167.07 ms /   150 tokens (    1.11 ms per token,   897.82 tokens per second)\n",
      "llama_print_timings:        eval time =    2588.21 ms /   134 runs   (   19.31 ms per token,    51.77 tokens per second)\n",
      "llama_print_timings:       total time =    3261.97 ms /   284 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.02 ms /    21 runs   (    0.38 ms per token,  2620.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.82 ms /    75 tokens (    1.56 ms per token,   642.04 tokens per second)\n",
      "llama_print_timings:        eval time =     394.33 ms /    20 runs   (   19.72 ms per token,    50.72 tokens per second)\n",
      "llama_print_timings:       total time =     600.32 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.76 ms /    92 runs   (    0.38 ms per token,  2646.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.77 ms /   141 tokens (    1.10 ms per token,   911.04 tokens per second)\n",
      "llama_print_timings:        eval time =    1798.41 ms /    91 runs   (   19.76 ms per token,    50.60 tokens per second)\n",
      "llama_print_timings:       total time =    2301.01 ms /   232 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.24 ms /    94 runs   (    0.39 ms per token,  2593.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.06 ms /   120 tokens (    1.23 ms per token,   810.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1812.61 ms /    93 runs   (   19.49 ms per token,    51.31 tokens per second)\n",
      "llama_print_timings:       total time =    2311.51 ms /   213 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      81.23 ms /   220 runs   (    0.37 ms per token,  2708.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     515.59 ms /   597 tokens (    0.86 ms per token,  1157.89 tokens per second)\n",
      "llama_print_timings:        eval time =    4495.44 ms /   219 runs   (   20.53 ms per token,    48.72 tokens per second)\n",
      "llama_print_timings:       total time =    5846.00 ms /   816 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.83 ms /   110 runs   (    0.38 ms per token,  2629.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     179.71 ms /   199 tokens (    0.90 ms per token,  1107.32 tokens per second)\n",
      "llama_print_timings:        eval time =    2067.74 ms /   109 runs   (   18.97 ms per token,    52.71 tokens per second)\n",
      "llama_print_timings:       total time =    2680.57 ms /   308 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      51.44 ms /   134 runs   (    0.38 ms per token,  2604.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.65 ms /    99 tokens (    1.18 ms per token,   848.71 tokens per second)\n",
      "llama_print_timings:        eval time =    2643.91 ms /   133 runs   (   19.88 ms per token,    50.30 tokens per second)\n",
      "llama_print_timings:       total time =    3297.04 ms /   232 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2542.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     146.69 ms /    97 tokens (    1.51 ms per token,   661.26 tokens per second)\n",
      "llama_print_timings:        eval time =      47.91 ms /     2 runs   (   23.95 ms per token,    41.75 tokens per second)\n",
      "llama_print_timings:       total time =     207.79 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.31 ms /     3 runs   (    0.44 ms per token,  2288.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.78 ms /    74 tokens (    2.19 ms per token,   457.41 tokens per second)\n",
      "llama_print_timings:        eval time =      46.34 ms /     2 runs   (   23.17 ms per token,    43.16 tokens per second)\n",
      "llama_print_timings:       total time =     225.56 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.55 ms /     3 runs   (    0.52 ms per token,  1934.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     183.68 ms /   138 tokens (    1.33 ms per token,   751.29 tokens per second)\n",
      "llama_print_timings:        eval time =      44.86 ms /     2 runs   (   22.43 ms per token,    44.59 tokens per second)\n",
      "llama_print_timings:       total time =     248.08 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.27 ms /    89 runs   (    0.41 ms per token,  2453.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     183.55 ms /   252 tokens (    0.73 ms per token,  1372.95 tokens per second)\n",
      "llama_print_timings:        eval time =    1962.66 ms /    88 runs   (   22.30 ms per token,    44.84 tokens per second)\n",
      "llama_print_timings:       total time =    2521.16 ms /   340 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.88 ms /    17 runs   (    0.40 ms per token,  2470.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.07 ms /    42 tokens (    3.74 ms per token,   267.39 tokens per second)\n",
      "llama_print_timings:        eval time =     381.63 ms /    16 runs   (   23.85 ms per token,    41.93 tokens per second)\n",
      "llama_print_timings:       total time =     622.94 ms /    58 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      61.97 ms /   161 runs   (    0.38 ms per token,  2597.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     233.18 ms /   161 tokens (    1.45 ms per token,   690.47 tokens per second)\n",
      "llama_print_timings:        eval time =    3634.89 ms /   160 runs   (   22.72 ms per token,    44.02 tokens per second)\n",
      "llama_print_timings:       total time =    4552.07 ms /   321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.93 ms /    22 runs   (    0.36 ms per token,  2774.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.07 ms /    80 tokens (    1.99 ms per token,   502.93 tokens per second)\n",
      "llama_print_timings:        eval time =     484.55 ms /    21 runs   (   23.07 ms per token,    43.34 tokens per second)\n",
      "llama_print_timings:       total time =     732.64 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.67 ms /    28 runs   (    0.38 ms per token,  2624.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.00 ms /    90 tokens (    1.71 ms per token,   584.43 tokens per second)\n",
      "llama_print_timings:        eval time =     619.10 ms /    27 runs   (   22.93 ms per token,    43.61 tokens per second)\n",
      "llama_print_timings:       total time =     894.40 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.36 ms /   101 runs   (    0.37 ms per token,  2703.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.92 ms /   110 tokens (    1.42 ms per token,   705.49 tokens per second)\n",
      "llama_print_timings:        eval time =    2213.76 ms /   100 runs   (   22.14 ms per token,    45.17 tokens per second)\n",
      "llama_print_timings:       total time =    2768.33 ms /   210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.87 ms /   117 runs   (    0.37 ms per token,  2667.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.86 ms /   100 tokens (    1.55 ms per token,   645.73 tokens per second)\n",
      "llama_print_timings:        eval time =    2509.11 ms /   116 runs   (   21.63 ms per token,    46.23 tokens per second)\n",
      "llama_print_timings:       total time =    3136.68 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.25 ms /    17 runs   (    0.43 ms per token,  2345.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.99 ms /    65 tokens (    2.09 ms per token,   477.96 tokens per second)\n",
      "llama_print_timings:        eval time =     380.44 ms /    16 runs   (   23.78 ms per token,    42.06 tokens per second)\n",
      "llama_print_timings:       total time =     607.68 ms /    81 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      59.74 ms /   162 runs   (    0.37 ms per token,  2711.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.55 ms /   124 tokens (    1.23 ms per token,   812.85 tokens per second)\n",
      "llama_print_timings:        eval time =    3515.81 ms /   161 runs   (   21.84 ms per token,    45.79 tokens per second)\n",
      "llama_print_timings:       total time =    4314.33 ms /   285 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.01 ms /   131 runs   (    0.38 ms per token,  2619.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     224.86 ms /   222 tokens (    1.01 ms per token,   987.27 tokens per second)\n",
      "llama_print_timings:        eval time =    3110.29 ms /   130 runs   (   23.93 ms per token,    41.80 tokens per second)\n",
      "llama_print_timings:       total time =    3851.46 ms /   352 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.79 ms /    20 runs   (    0.39 ms per token,  2566.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.24 ms /   101 tokens (    1.62 ms per token,   618.73 tokens per second)\n",
      "llama_print_timings:        eval time =     443.04 ms /    19 runs   (   23.32 ms per token,    42.89 tokens per second)\n",
      "llama_print_timings:       total time =     688.46 ms /   120 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.79 ms /   124 runs   (    0.37 ms per token,  2708.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     150.44 ms /    67 tokens (    2.25 ms per token,   445.35 tokens per second)\n",
      "llama_print_timings:        eval time =    2746.80 ms /   123 runs   (   22.33 ms per token,    44.78 tokens per second)\n",
      "llama_print_timings:       total time =    3377.73 ms /   190 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.16 ms /   118 runs   (    0.37 ms per token,  2672.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     207.53 ms /   213 tokens (    0.97 ms per token,  1026.35 tokens per second)\n",
      "llama_print_timings:        eval time =    2659.93 ms /   117 runs   (   22.73 ms per token,    43.99 tokens per second)\n",
      "llama_print_timings:       total time =    3333.68 ms /   330 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      84.46 ms /   235 runs   (    0.36 ms per token,  2782.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     618.82 ms /   616 tokens (    1.00 ms per token,   995.45 tokens per second)\n",
      "llama_print_timings:        eval time =    5494.56 ms /   234 runs   (   23.48 ms per token,    42.59 tokens per second)\n",
      "llama_print_timings:       total time =    7068.81 ms /   850 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.45 ms /   110 runs   (    0.37 ms per token,  2719.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     195.86 ms /   174 tokens (    1.13 ms per token,   888.37 tokens per second)\n",
      "llama_print_timings:        eval time =    2477.71 ms /   109 runs   (   22.73 ms per token,    43.99 tokens per second)\n",
      "llama_print_timings:       total time =    3088.05 ms /   283 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     3 runs   (    0.39 ms per token,  2577.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     233.20 ms /   194 tokens (    1.20 ms per token,   831.90 tokens per second)\n",
      "llama_print_timings:        eval time =      57.56 ms /     2 runs   (   28.78 ms per token,    34.75 tokens per second)\n",
      "llama_print_timings:       total time =     308.93 ms /   196 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.77 ms /    76 runs   (    0.38 ms per token,  2641.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.51 ms /    36 tokens (    4.01 ms per token,   249.12 tokens per second)\n",
      "llama_print_timings:        eval time =    1695.16 ms /    75 runs   (   22.60 ms per token,    44.24 tokens per second)\n",
      "llama_print_timings:       total time =    2151.29 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     3 runs   (    0.37 ms per token,  2683.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     142.87 ms /    56 tokens (    2.55 ms per token,   391.95 tokens per second)\n",
      "llama_print_timings:        eval time =      48.28 ms /     2 runs   (   24.14 ms per token,    41.43 tokens per second)\n",
      "llama_print_timings:       total time =     206.26 ms /    58 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.13 ms /   133 runs   (    0.37 ms per token,  2706.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     211.43 ms /   190 tokens (    1.11 ms per token,   898.66 tokens per second)\n",
      "llama_print_timings:        eval time =    3002.91 ms /   132 runs   (   22.75 ms per token,    43.96 tokens per second)\n",
      "llama_print_timings:       total time =    3753.24 ms /   322 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      19.79 ms /    55 runs   (    0.36 ms per token,  2779.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     153.25 ms /    35 tokens (    4.38 ms per token,   228.38 tokens per second)\n",
      "llama_print_timings:        eval time =    1201.35 ms /    54 runs   (   22.25 ms per token,    44.95 tokens per second)\n",
      "llama_print_timings:       total time =    1561.51 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.68 ms /    27 runs   (    0.40 ms per token,  2529.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.15 ms /    78 tokens (    1.81 ms per token,   552.60 tokens per second)\n",
      "llama_print_timings:        eval time =     570.92 ms /    26 runs   (   21.96 ms per token,    45.54 tokens per second)\n",
      "llama_print_timings:       total time =     818.96 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      64.11 ms /   166 runs   (    0.39 ms per token,  2589.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     246.44 ms /   221 tokens (    1.12 ms per token,   896.76 tokens per second)\n",
      "llama_print_timings:        eval time =    3715.13 ms /   165 runs   (   22.52 ms per token,    44.41 tokens per second)\n",
      "llama_print_timings:       total time =    4615.90 ms /   386 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      83.18 ms /   215 runs   (    0.39 ms per token,  2584.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     218.59 ms /   174 tokens (    1.26 ms per token,   796.00 tokens per second)\n",
      "llama_print_timings:        eval time =    4745.33 ms /   214 runs   (   22.17 ms per token,    45.10 tokens per second)\n",
      "llama_print_timings:       total time =    5846.15 ms /   388 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      51.96 ms /   136 runs   (    0.38 ms per token,  2617.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.94 ms /   146 tokens (    1.02 ms per token,   980.23 tokens per second)\n",
      "llama_print_timings:        eval time =    2609.08 ms /   135 runs   (   19.33 ms per token,    51.74 tokens per second)\n",
      "llama_print_timings:       total time =    3310.42 ms /   281 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.49 ms /   109 runs   (    0.38 ms per token,  2626.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     238.06 ms /   271 tokens (    0.88 ms per token,  1138.38 tokens per second)\n",
      "llama_print_timings:        eval time =    2030.78 ms /   108 runs   (   18.80 ms per token,    53.18 tokens per second)\n",
      "llama_print_timings:       total time =    2685.08 ms /   379 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.71 ms /    23 runs   (    0.38 ms per token,  2640.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.59 ms /   142 tokens (    1.11 ms per token,   901.05 tokens per second)\n",
      "llama_print_timings:        eval time =     426.45 ms /    22 runs   (   19.38 ms per token,    51.59 tokens per second)\n",
      "llama_print_timings:       total time =     673.74 ms /   164 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      77.25 ms /   189 runs   (    0.41 ms per token,  2446.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.56 ms /    68 tokens (    1.74 ms per token,   573.54 tokens per second)\n",
      "llama_print_timings:        eval time =    3602.84 ms /   188 runs   (   19.16 ms per token,    52.18 tokens per second)\n",
      "llama_print_timings:       total time =    4536.03 ms /   256 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      90.31 ms /   234 runs   (    0.39 ms per token,  2591.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     860.44 ms /  1090 tokens (    0.79 ms per token,  1266.79 tokens per second)\n",
      "llama_print_timings:        eval time =    4877.59 ms /   233 runs   (   20.93 ms per token,    47.77 tokens per second)\n",
      "llama_print_timings:       total time =    6752.95 ms /  1323 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.25 ms /     3 runs   (    0.42 ms per token,  2403.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.86 ms /   108 tokens (    1.17 ms per token,   851.32 tokens per second)\n",
      "llama_print_timings:        eval time =      40.73 ms /     2 runs   (   20.37 ms per token,    49.10 tokens per second)\n",
      "llama_print_timings:       total time =     182.60 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      29.47 ms /    78 runs   (    0.38 ms per token,  2646.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.76 ms /    87 tokens (    1.37 ms per token,   732.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1446.96 ms /    77 runs   (   18.79 ms per token,    53.21 tokens per second)\n",
      "llama_print_timings:       total time =    1853.62 ms /   164 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.35 ms /    96 runs   (    0.39 ms per token,  2570.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.75 ms /    91 tokens (    1.30 ms per token,   766.32 tokens per second)\n",
      "llama_print_timings:        eval time =    1801.96 ms /    95 runs   (   18.97 ms per token,    52.72 tokens per second)\n",
      "llama_print_timings:       total time =    2292.60 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.49 ms /    23 runs   (    0.37 ms per token,  2709.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     116.20 ms /    66 tokens (    1.76 ms per token,   567.98 tokens per second)\n",
      "llama_print_timings:        eval time =     398.31 ms /    22 runs   (   18.11 ms per token,    55.23 tokens per second)\n",
      "llama_print_timings:       total time =     608.06 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.66 ms /    20 runs   (    0.38 ms per token,  2610.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.41 ms /    82 tokens (    1.50 ms per token,   664.45 tokens per second)\n",
      "llama_print_timings:        eval time =     356.50 ms /    19 runs   (   18.76 ms per token,    53.30 tokens per second)\n",
      "llama_print_timings:       total time =     574.00 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      55.14 ms /   146 runs   (    0.38 ms per token,  2647.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.85 ms /    71 tokens (    1.69 ms per token,   592.41 tokens per second)\n",
      "llama_print_timings:        eval time =    2688.82 ms /   145 runs   (   18.54 ms per token,    53.93 tokens per second)\n",
      "llama_print_timings:       total time =    3366.26 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      92.68 ms /   251 runs   (    0.37 ms per token,  2708.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     320.19 ms /   449 tokens (    0.71 ms per token,  1402.31 tokens per second)\n",
      "llama_print_timings:        eval time =    4913.81 ms /   250 runs   (   19.66 ms per token,    50.88 tokens per second)\n",
      "llama_print_timings:       total time =    6201.41 ms /   699 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.32 ms /    78 runs   (    0.39 ms per token,  2572.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.97 ms /    62 tokens (    1.92 ms per token,   521.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1450.98 ms /    77 runs   (   18.84 ms per token,    53.07 tokens per second)\n",
      "llama_print_timings:       total time =    1875.06 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      71.44 ms /   195 runs   (    0.37 ms per token,  2729.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     169.54 ms /   178 tokens (    0.95 ms per token,  1049.88 tokens per second)\n",
      "llama_print_timings:        eval time =    3649.91 ms /   194 runs   (   18.81 ms per token,    53.15 tokens per second)\n",
      "llama_print_timings:       total time =    4569.68 ms /   372 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.72 ms /   114 runs   (    0.37 ms per token,  2668.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     169.07 ms /   238 tokens (    0.71 ms per token,  1407.70 tokens per second)\n",
      "llama_print_timings:        eval time =    2121.22 ms /   113 runs   (   18.77 ms per token,    53.27 tokens per second)\n",
      "llama_print_timings:       total time =    2719.89 ms /   351 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      96.20 ms /   256 runs   (    0.38 ms per token,  2661.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     241.35 ms /   346 tokens (    0.70 ms per token,  1433.58 tokens per second)\n",
      "llama_print_timings:        eval time =    4977.99 ms /   255 runs   (   19.52 ms per token,    51.23 tokens per second)\n",
      "llama_print_timings:       total time =    6239.28 ms /   601 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.69 ms /    17 runs   (    0.39 ms per token,  2541.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.30 ms /   122 tokens (    1.08 ms per token,   922.17 tokens per second)\n",
      "llama_print_timings:        eval time =     305.57 ms /    16 runs   (   19.10 ms per token,    52.36 tokens per second)\n",
      "llama_print_timings:       total time =     504.87 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.89 ms /    24 runs   (    0.37 ms per token,  2699.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.91 ms /    87 tokens (    1.41 ms per token,   707.85 tokens per second)\n",
      "llama_print_timings:        eval time =     443.66 ms /    23 runs   (   19.29 ms per token,    51.84 tokens per second)\n",
      "llama_print_timings:       total time =     660.37 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.44 ms /   135 runs   (    0.37 ms per token,  2676.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.27 ms /   230 tokens (    0.79 ms per token,  1268.80 tokens per second)\n",
      "llama_print_timings:        eval time =    2517.29 ms /   134 runs   (   18.79 ms per token,    53.23 tokens per second)\n",
      "llama_print_timings:       total time =    3206.92 ms /   364 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.70 ms /   132 runs   (    0.38 ms per token,  2655.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.43 ms /   179 tokens (    0.93 ms per token,  1075.56 tokens per second)\n",
      "llama_print_timings:        eval time =    2538.29 ms /   131 runs   (   19.38 ms per token,    51.61 tokens per second)\n",
      "llama_print_timings:       total time =    3220.65 ms /   310 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.73 ms /    98 runs   (    0.39 ms per token,  2597.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     170.94 ms /   200 tokens (    0.85 ms per token,  1170.02 tokens per second)\n",
      "llama_print_timings:        eval time =    1812.76 ms /    97 runs   (   18.69 ms per token,    53.51 tokens per second)\n",
      "llama_print_timings:       total time =    2351.75 ms /   297 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      71.02 ms /   193 runs   (    0.37 ms per token,  2717.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     227.34 ms /   330 tokens (    0.69 ms per token,  1451.55 tokens per second)\n",
      "llama_print_timings:        eval time =    3713.47 ms /   192 runs   (   19.34 ms per token,    51.70 tokens per second)\n",
      "llama_print_timings:       total time =    4681.95 ms /   522 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.69 ms /    17 runs   (    0.39 ms per token,  2541.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     227.77 ms /   263 tokens (    0.87 ms per token,  1154.69 tokens per second)\n",
      "llama_print_timings:        eval time =     306.38 ms /    16 runs   (   19.15 ms per token,    52.22 tokens per second)\n",
      "llama_print_timings:       total time =     597.18 ms /   279 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.73 ms /    91 runs   (    0.37 ms per token,  2697.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     169.09 ms /   198 tokens (    0.85 ms per token,  1171.00 tokens per second)\n",
      "llama_print_timings:        eval time =    1687.70 ms /    90 runs   (   18.75 ms per token,    53.33 tokens per second)\n",
      "llama_print_timings:       total time =    2194.19 ms /   288 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      24.65 ms /    66 runs   (    0.37 ms per token,  2677.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     117.55 ms /    66 tokens (    1.78 ms per token,   561.46 tokens per second)\n",
      "llama_print_timings:        eval time =    1218.55 ms /    65 runs   (   18.75 ms per token,    53.34 tokens per second)\n",
      "llama_print_timings:       total time =    1579.60 ms /   131 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.63 ms /    22 runs   (    0.39 ms per token,  2547.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.42 ms /   123 tokens (    1.07 ms per token,   935.93 tokens per second)\n",
      "llama_print_timings:        eval time =     392.94 ms /    21 runs   (   18.71 ms per token,    53.44 tokens per second)\n",
      "llama_print_timings:       total time =     605.93 ms /   144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.93 ms /   131 runs   (    0.37 ms per token,  2677.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     171.62 ms /   171 tokens (    1.00 ms per token,   996.36 tokens per second)\n",
      "llama_print_timings:        eval time =    2430.76 ms /   130 runs   (   18.70 ms per token,    53.48 tokens per second)\n",
      "llama_print_timings:       total time =    3097.14 ms /   301 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.41 ms /    79 runs   (    0.38 ms per token,  2597.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.59 ms /    54 tokens (    2.01 ms per token,   497.31 tokens per second)\n",
      "llama_print_timings:        eval time =    1454.41 ms /    78 runs   (   18.65 ms per token,    53.63 tokens per second)\n",
      "llama_print_timings:       total time =    1856.82 ms /   132 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      93.80 ms /   256 runs   (    0.37 ms per token,  2729.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     506.73 ms /   652 tokens (    0.78 ms per token,  1286.68 tokens per second)\n",
      "llama_print_timings:        eval time =    5100.29 ms /   255 runs   (   20.00 ms per token,    50.00 tokens per second)\n",
      "llama_print_timings:       total time =    6618.75 ms /   907 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.55 ms /    17 runs   (    0.39 ms per token,  2594.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.01 ms /   151 tokens (    1.05 ms per token,   955.64 tokens per second)\n",
      "llama_print_timings:        eval time =     295.10 ms /    16 runs   (   18.44 ms per token,    54.22 tokens per second)\n",
      "llama_print_timings:       total time =     523.62 ms /   167 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.76 ms /   111 runs   (    0.38 ms per token,  2657.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     117.49 ms /    72 tokens (    1.63 ms per token,   612.84 tokens per second)\n",
      "llama_print_timings:        eval time =    2042.51 ms /   110 runs   (   18.57 ms per token,    53.86 tokens per second)\n",
      "llama_print_timings:       total time =    2574.81 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      29.10 ms /    76 runs   (    0.38 ms per token,  2611.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.47 ms /   120 tokens (    1.12 ms per token,   892.41 tokens per second)\n",
      "llama_print_timings:        eval time =    1395.00 ms /    75 runs   (   18.60 ms per token,    53.76 tokens per second)\n",
      "llama_print_timings:       total time =    1816.11 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.27 ms /    72 runs   (    0.38 ms per token,  2640.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.72 ms /   102 tokens (    1.24 ms per token,   804.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1336.74 ms /    71 runs   (   18.83 ms per token,    53.11 tokens per second)\n",
      "llama_print_timings:       total time =    1738.15 ms /   173 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      61.77 ms /   166 runs   (    0.37 ms per token,  2687.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     187.00 ms /   241 tokens (    0.78 ms per token,  1288.75 tokens per second)\n",
      "llama_print_timings:        eval time =    3133.04 ms /   165 runs   (   18.99 ms per token,    52.66 tokens per second)\n",
      "llama_print_timings:       total time =    3961.98 ms /   406 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.01 ms /   111 runs   (    0.38 ms per token,  2642.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     180.08 ms /   189 tokens (    0.95 ms per token,  1049.55 tokens per second)\n",
      "llama_print_timings:        eval time =    2041.41 ms /   110 runs   (   18.56 ms per token,    53.88 tokens per second)\n",
      "llama_print_timings:       total time =    2638.79 ms /   299 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       0.95 ms /     3 runs   (    0.32 ms per token,  3147.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.40 ms /   138 tokens (    1.17 ms per token,   855.02 tokens per second)\n",
      "llama_print_timings:        eval time =      40.61 ms /     2 runs   (   20.31 ms per token,    49.25 tokens per second)\n",
      "llama_print_timings:       total time =     212.32 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      21.50 ms /    58 runs   (    0.37 ms per token,  2697.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.43 ms /    43 tokens (    2.57 ms per token,   389.39 tokens per second)\n",
      "llama_print_timings:        eval time =    1057.58 ms /    57 runs   (   18.55 ms per token,    53.90 tokens per second)\n",
      "llama_print_timings:       total time =    1385.75 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      62.63 ms /   168 runs   (    0.37 ms per token,  2682.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     238.28 ms /   336 tokens (    0.71 ms per token,  1410.12 tokens per second)\n",
      "llama_print_timings:        eval time =    3205.47 ms /   167 runs   (   19.19 ms per token,    52.10 tokens per second)\n",
      "llama_print_timings:       total time =    4100.35 ms /   503 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.35 ms /   129 runs   (    0.37 ms per token,  2667.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.17 ms /   110 tokens (    1.13 ms per token,   885.85 tokens per second)\n",
      "llama_print_timings:        eval time =    2545.34 ms /   128 runs   (   19.89 ms per token,    50.29 tokens per second)\n",
      "llama_print_timings:       total time =    3177.74 ms /   238 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.59 ms /   128 runs   (    0.37 ms per token,  2689.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     460.00 ms /   563 tokens (    0.82 ms per token,  1223.91 tokens per second)\n",
      "llama_print_timings:        eval time =    2487.70 ms /   127 runs   (   19.59 ms per token,    51.05 tokens per second)\n",
      "llama_print_timings:       total time =    3427.48 ms /   690 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.35 ms /    95 runs   (    0.37 ms per token,  2687.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.83 ms /   580 tokens (    0.70 ms per token,  1422.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1921.77 ms /    94 runs   (   20.44 ms per token,    48.91 tokens per second)\n",
      "llama_print_timings:       total time =    2678.93 ms /   674 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.96 ms /   105 runs   (    0.37 ms per token,  2695.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     184.53 ms /   189 tokens (    0.98 ms per token,  1024.25 tokens per second)\n",
      "llama_print_timings:        eval time =    1973.58 ms /   104 runs   (   18.98 ms per token,    52.70 tokens per second)\n",
      "llama_print_timings:       total time =    2550.54 ms /   293 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.56 ms /   100 runs   (    0.38 ms per token,  2662.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     171.46 ms /   182 tokens (    0.94 ms per token,  1061.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1889.72 ms /    99 runs   (   19.09 ms per token,    52.39 tokens per second)\n",
      "llama_print_timings:       total time =    2430.46 ms /   281 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2606.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     253.97 ms /   359 tokens (    0.71 ms per token,  1413.56 tokens per second)\n",
      "llama_print_timings:        eval time =      38.83 ms /     2 runs   (   19.41 ms per token,    51.51 tokens per second)\n",
      "llama_print_timings:       total time =     304.85 ms /   361 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.47 ms /    17 runs   (    0.38 ms per token,  2626.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.62 ms /    91 tokens (    1.31 ms per token,   760.72 tokens per second)\n",
      "llama_print_timings:        eval time =     297.66 ms /    16 runs   (   18.60 ms per token,    53.75 tokens per second)\n",
      "llama_print_timings:       total time =     486.28 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.66 ms /   108 runs   (    0.38 ms per token,  2655.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.44 ms /   108 tokens (    1.10 ms per token,   911.85 tokens per second)\n",
      "llama_print_timings:        eval time =    2026.25 ms /   107 runs   (   18.94 ms per token,    52.81 tokens per second)\n",
      "llama_print_timings:       total time =    2559.47 ms /   215 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.53 ms /   105 runs   (    0.38 ms per token,  2656.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.90 ms /   172 tokens (    0.98 ms per token,  1018.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1989.11 ms /   104 runs   (   19.13 ms per token,    52.28 tokens per second)\n",
      "llama_print_timings:       total time =    2552.69 ms /   276 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.79 ms /    21 runs   (    0.37 ms per token,  2694.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     117.03 ms /    50 tokens (    2.34 ms per token,   427.26 tokens per second)\n",
      "llama_print_timings:        eval time =     385.31 ms /    20 runs   (   19.27 ms per token,    51.91 tokens per second)\n",
      "llama_print_timings:       total time =     581.92 ms /    70 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      66.34 ms /   175 runs   (    0.38 ms per token,  2637.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     175.86 ms /   218 tokens (    0.81 ms per token,  1239.63 tokens per second)\n",
      "llama_print_timings:        eval time =    3351.62 ms /   174 runs   (   19.26 ms per token,    51.92 tokens per second)\n",
      "llama_print_timings:       total time =    4193.38 ms /   392 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.96 ms /   111 runs   (    0.39 ms per token,  2583.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.95 ms /    61 tokens (    1.85 ms per token,   540.06 tokens per second)\n",
      "llama_print_timings:        eval time =    2094.25 ms /   110 runs   (   19.04 ms per token,    52.52 tokens per second)\n",
      "llama_print_timings:       total time =    2676.79 ms /   171 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.24 ms /     3 runs   (    0.41 ms per token,  2425.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.45 ms /   104 tokens (    1.17 ms per token,   856.29 tokens per second)\n",
      "llama_print_timings:        eval time =      42.36 ms /     2 runs   (   21.18 ms per token,    47.21 tokens per second)\n",
      "llama_print_timings:       total time =     187.05 ms /   106 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      83.28 ms /   225 runs   (    0.37 ms per token,  2701.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     300.88 ms /   467 tokens (    0.64 ms per token,  1552.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4454.45 ms /   224 runs   (   19.89 ms per token,    50.29 tokens per second)\n",
      "llama_print_timings:       total time =    5631.68 ms /   691 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.23 ms /    94 runs   (    0.39 ms per token,  2594.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.27 ms /    93 tokens (    1.33 ms per token,   754.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1737.29 ms /    93 runs   (   18.68 ms per token,    53.53 tokens per second)\n",
      "llama_print_timings:       total time =    2212.00 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.52 ms /    93 runs   (    0.38 ms per token,  2618.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     298.83 ms /   436 tokens (    0.69 ms per token,  1459.03 tokens per second)\n",
      "llama_print_timings:        eval time =    1845.39 ms /    92 runs   (   20.06 ms per token,    49.85 tokens per second)\n",
      "llama_print_timings:       total time =    2490.05 ms /   528 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.65 ms /    17 runs   (    0.39 ms per token,  2554.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.06 ms /    52 tokens (    2.08 ms per token,   481.21 tokens per second)\n",
      "llama_print_timings:        eval time =     299.69 ms /    16 runs   (   18.73 ms per token,    53.39 tokens per second)\n",
      "llama_print_timings:       total time =     474.44 ms /    68 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.59 ms /   117 runs   (    0.37 ms per token,  2684.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.25 ms /    94 tokens (    1.29 ms per token,   775.29 tokens per second)\n",
      "llama_print_timings:        eval time =    2203.82 ms /   116 runs   (   19.00 ms per token,    52.64 tokens per second)\n",
      "llama_print_timings:       total time =    2768.70 ms /   210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.63 ms /    87 runs   (    0.38 ms per token,  2666.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.37 ms /    93 tokens (    1.33 ms per token,   753.81 tokens per second)\n",
      "llama_print_timings:        eval time =    1606.35 ms /    86 runs   (   18.68 ms per token,    53.54 tokens per second)\n",
      "llama_print_timings:       total time =    2055.23 ms /   179 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.38 ms /    17 runs   (    0.38 ms per token,  2665.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.87 ms /    71 tokens (    1.76 ms per token,   568.58 tokens per second)\n",
      "llama_print_timings:        eval time =     295.00 ms /    16 runs   (   18.44 ms per token,    54.24 tokens per second)\n",
      "llama_print_timings:       total time =     488.68 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      69.78 ms /   184 runs   (    0.38 ms per token,  2636.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.43 ms /   203 tokens (    0.79 ms per token,  1265.33 tokens per second)\n",
      "llama_print_timings:        eval time =    3506.78 ms /   183 runs   (   19.16 ms per token,    52.18 tokens per second)\n",
      "llama_print_timings:       total time =    4385.47 ms /   386 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.48 ms /    93 runs   (    0.38 ms per token,  2620.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.53 ms /   152 tokens (    1.02 ms per token,   977.30 tokens per second)\n",
      "llama_print_timings:        eval time =    1717.96 ms /    92 runs   (   18.67 ms per token,    53.55 tokens per second)\n",
      "llama_print_timings:       total time =    2232.70 ms /   244 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.30 ms /   105 runs   (    0.37 ms per token,  2671.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     174.57 ms /   188 tokens (    0.93 ms per token,  1076.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1985.78 ms /   104 runs   (   19.09 ms per token,    52.37 tokens per second)\n",
      "llama_print_timings:       total time =    2550.89 ms /   292 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.92 ms /   107 runs   (    0.36 ms per token,  2749.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     242.96 ms /   293 tokens (    0.83 ms per token,  1205.95 tokens per second)\n",
      "llama_print_timings:        eval time =    2061.75 ms /   106 runs   (   19.45 ms per token,    51.41 tokens per second)\n",
      "llama_print_timings:       total time =    2704.73 ms /   399 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2638.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.50 ms /    60 tokens (    1.98 ms per token,   506.32 tokens per second)\n",
      "llama_print_timings:        eval time =      49.48 ms /     2 runs   (   24.74 ms per token,    40.42 tokens per second)\n",
      "llama_print_timings:       total time =     179.53 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      89.44 ms /   242 runs   (    0.37 ms per token,  2705.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     186.51 ms /   245 tokens (    0.76 ms per token,  1313.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4656.35 ms /   241 runs   (   19.32 ms per token,    51.76 tokens per second)\n",
      "llama_print_timings:       total time =    5785.92 ms /   486 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /     3 runs   (    0.38 ms per token,  2664.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.12 ms /    70 tokens (    1.72 ms per token,   582.76 tokens per second)\n",
      "llama_print_timings:        eval time =      40.79 ms /     2 runs   (   20.39 ms per token,    49.03 tokens per second)\n",
      "llama_print_timings:       total time =     172.84 ms /    72 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.98 ms /   145 runs   (    0.35 ms per token,  2844.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     242.67 ms /   352 tokens (    0.69 ms per token,  1450.55 tokens per second)\n",
      "llama_print_timings:        eval time =    2787.90 ms /   144 runs   (   19.36 ms per token,    51.65 tokens per second)\n",
      "llama_print_timings:       total time =    3564.16 ms /   496 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.69 ms /   113 runs   (    0.38 ms per token,  2647.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     239.23 ms /   298 tokens (    0.80 ms per token,  1245.66 tokens per second)\n",
      "llama_print_timings:        eval time =    2119.83 ms /   112 runs   (   18.93 ms per token,    52.83 tokens per second)\n",
      "llama_print_timings:       total time =    2786.75 ms /   410 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.73 ms /    93 runs   (    0.37 ms per token,  2677.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.09 ms /    62 tokens (    1.81 ms per token,   553.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1709.25 ms /    92 runs   (   18.58 ms per token,    53.82 tokens per second)\n",
      "llama_print_timings:       total time =    2162.69 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      81.10 ms /   223 runs   (    0.36 ms per token,  2749.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     238.72 ms /   277 tokens (    0.86 ms per token,  1160.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4331.27 ms /   222 runs   (   19.51 ms per token,    51.26 tokens per second)\n",
      "llama_print_timings:       total time =    5422.52 ms /   499 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.72 ms /   118 runs   (    0.36 ms per token,  2762.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     252.89 ms /   348 tokens (    0.73 ms per token,  1376.08 tokens per second)\n",
      "llama_print_timings:        eval time =    2277.82 ms /   117 runs   (   19.47 ms per token,    51.36 tokens per second)\n",
      "llama_print_timings:       total time =    2966.50 ms /   465 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.19 ms /    22 runs   (    0.37 ms per token,  2686.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     150.36 ms /   135 tokens (    1.11 ms per token,   897.87 tokens per second)\n",
      "llama_print_timings:        eval time =     382.92 ms /    21 runs   (   18.23 ms per token,    54.84 tokens per second)\n",
      "llama_print_timings:       total time =     614.89 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      83.75 ms /   223 runs   (    0.38 ms per token,  2662.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.58 ms /   230 tokens (    0.70 ms per token,  1423.49 tokens per second)\n",
      "llama_print_timings:        eval time =    4243.73 ms /   222 runs   (   19.12 ms per token,    52.31 tokens per second)\n",
      "llama_print_timings:       total time =    5259.18 ms /   452 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.29 ms /   108 runs   (    0.38 ms per token,  2615.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.82 ms /    94 tokens (    1.27 ms per token,   784.50 tokens per second)\n",
      "llama_print_timings:        eval time =    2024.57 ms /   107 runs   (   18.92 ms per token,    52.85 tokens per second)\n",
      "llama_print_timings:       total time =    2562.45 ms /   201 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.05 ms /    21 runs   (    0.38 ms per token,  2609.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     177.14 ms /   186 tokens (    0.95 ms per token,  1050.04 tokens per second)\n",
      "llama_print_timings:        eval time =     380.55 ms /    20 runs   (   19.03 ms per token,    52.56 tokens per second)\n",
      "llama_print_timings:       total time =     636.22 ms /   206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      60.07 ms /   156 runs   (    0.39 ms per token,  2597.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.84 ms /   185 tokens (    0.93 ms per token,  1070.34 tokens per second)\n",
      "llama_print_timings:        eval time =    3012.11 ms /   155 runs   (   19.43 ms per token,    51.46 tokens per second)\n",
      "llama_print_timings:       total time =    3775.86 ms /   340 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.77 ms /    18 runs   (    0.38 ms per token,  2658.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.89 ms /    50 tokens (    2.26 ms per token,   442.89 tokens per second)\n",
      "llama_print_timings:        eval time =     316.13 ms /    17 runs   (   18.60 ms per token,    53.77 tokens per second)\n",
      "llama_print_timings:       total time =     501.95 ms /    67 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.89 ms /    98 runs   (    0.38 ms per token,  2656.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.64 ms /   216 tokens (    0.84 ms per token,  1189.18 tokens per second)\n",
      "llama_print_timings:        eval time =    1804.58 ms /    97 runs   (   18.60 ms per token,    53.75 tokens per second)\n",
      "llama_print_timings:       total time =    2350.88 ms /   313 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.09 ms /     3 runs   (    0.36 ms per token,  2749.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.07 ms /    86 tokens (    1.42 ms per token,   704.52 tokens per second)\n",
      "llama_print_timings:        eval time =      38.65 ms /     2 runs   (   19.33 ms per token,    51.74 tokens per second)\n",
      "llama_print_timings:       total time =     172.48 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.63 ms /    73 runs   (    0.38 ms per token,  2642.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.50 ms /    94 tokens (    1.32 ms per token,   755.03 tokens per second)\n",
      "llama_print_timings:        eval time =    1365.95 ms /    72 runs   (   18.97 ms per token,    52.71 tokens per second)\n",
      "llama_print_timings:       total time =    1761.12 ms /   166 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.08 ms /    18 runs   (    0.39 ms per token,  2541.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.37 ms /   146 tokens (    1.07 ms per token,   933.67 tokens per second)\n",
      "llama_print_timings:        eval time =     321.10 ms /    17 runs   (   18.89 ms per token,    52.94 tokens per second)\n",
      "llama_print_timings:       total time =     547.58 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.68 ms /   104 runs   (    0.38 ms per token,  2620.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.05 ms /   120 tokens (    1.15 ms per token,   869.23 tokens per second)\n",
      "llama_print_timings:        eval time =    1922.08 ms /   103 runs   (   18.66 ms per token,    53.59 tokens per second)\n",
      "llama_print_timings:       total time =    2449.81 ms /   223 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.05 ms /    21 runs   (    0.38 ms per token,  2608.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.56 ms /    78 tokens (    1.53 ms per token,   652.38 tokens per second)\n",
      "llama_print_timings:        eval time =     367.05 ms /    20 runs   (   18.35 ms per token,    54.49 tokens per second)\n",
      "llama_print_timings:       total time =     568.02 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      11.71 ms /    30 runs   (    0.39 ms per token,  2561.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     231.22 ms /   299 tokens (    0.77 ms per token,  1293.14 tokens per second)\n",
      "llama_print_timings:        eval time =     569.75 ms /    29 runs   (   19.65 ms per token,    50.90 tokens per second)\n",
      "llama_print_timings:       total time =     911.60 ms /   328 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.30 ms /     3 runs   (    0.43 ms per token,  2302.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.26 ms /   101 tokens (    1.24 ms per token,   806.30 tokens per second)\n",
      "llama_print_timings:        eval time =      39.22 ms /     2 runs   (   19.61 ms per token,    51.00 tokens per second)\n",
      "llama_print_timings:       total time =     176.28 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.15 ms /    98 runs   (    0.37 ms per token,  2711.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     241.02 ms /   288 tokens (    0.84 ms per token,  1194.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1817.14 ms /    97 runs   (   18.73 ms per token,    53.38 tokens per second)\n",
      "llama_print_timings:       total time =    2420.27 ms /   385 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.42 ms /   109 runs   (    0.37 ms per token,  2696.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.75 ms /   143 tokens (    1.09 ms per token,   918.15 tokens per second)\n",
      "llama_print_timings:        eval time =    2063.88 ms /   108 runs   (   19.11 ms per token,    52.33 tokens per second)\n",
      "llama_print_timings:       total time =    2629.79 ms /   251 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.54 ms /    97 runs   (    0.38 ms per token,  2654.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     128.82 ms /    90 tokens (    1.43 ms per token,   698.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1787.24 ms /    96 runs   (   18.62 ms per token,    53.71 tokens per second)\n",
      "llama_print_timings:       total time =    2281.65 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.98 ms /   104 runs   (    0.37 ms per token,  2737.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     243.98 ms /   305 tokens (    0.80 ms per token,  1250.12 tokens per second)\n",
      "llama_print_timings:        eval time =    1941.92 ms /   103 runs   (   18.85 ms per token,    53.04 tokens per second)\n",
      "llama_print_timings:       total time =    2571.91 ms /   408 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.55 ms /   127 runs   (    0.38 ms per token,  2615.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.73 ms /   116 tokens (    1.13 ms per token,   887.30 tokens per second)\n",
      "llama_print_timings:        eval time =    2376.36 ms /   126 runs   (   18.86 ms per token,    53.02 tokens per second)\n",
      "llama_print_timings:       total time =    2988.11 ms /   242 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     3 runs   (    0.39 ms per token,  2597.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     510.66 ms /   706 tokens (    0.72 ms per token,  1382.52 tokens per second)\n",
      "llama_print_timings:        eval time =      39.69 ms /     2 runs   (   19.85 ms per token,    50.39 tokens per second)\n",
      "llama_print_timings:       total time =     562.68 ms /   708 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.07 ms /    91 runs   (    0.40 ms per token,  2522.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.88 ms /   115 tokens (    1.18 ms per token,   846.32 tokens per second)\n",
      "llama_print_timings:        eval time =    1754.01 ms /    90 runs   (   19.49 ms per token,    51.31 tokens per second)\n",
      "llama_print_timings:       total time =    2275.39 ms /   205 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.42 ms /    23 runs   (    0.45 ms per token,  2206.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     179.05 ms /   187 tokens (    0.96 ms per token,  1044.40 tokens per second)\n",
      "llama_print_timings:        eval time =     453.08 ms /    22 runs   (   20.59 ms per token,    48.56 tokens per second)\n",
      "llama_print_timings:       total time =     750.36 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.53 ms /     3 runs   (    0.51 ms per token,  1964.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     117.86 ms /    53 tokens (    2.22 ms per token,   449.67 tokens per second)\n",
      "llama_print_timings:        eval time =      41.43 ms /     2 runs   (   20.71 ms per token,    48.28 tokens per second)\n",
      "llama_print_timings:       total time =     179.64 ms /    55 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.41 ms /    87 runs   (    0.38 ms per token,  2603.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     115.02 ms /    49 tokens (    2.35 ms per token,   426.02 tokens per second)\n",
      "llama_print_timings:        eval time =    1593.02 ms /    86 runs   (   18.52 ms per token,    53.99 tokens per second)\n",
      "llama_print_timings:       total time =    2061.96 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.75 ms /    81 runs   (    0.35 ms per token,  2817.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     129.44 ms /   127 tokens (    1.02 ms per token,   981.17 tokens per second)\n",
      "llama_print_timings:        eval time =    1496.10 ms /    80 runs   (   18.70 ms per token,    53.47 tokens per second)\n",
      "llama_print_timings:       total time =    1925.29 ms /   207 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.90 ms /    21 runs   (    0.33 ms per token,  3045.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.79 ms /    56 tokens (    2.01 ms per token,   496.51 tokens per second)\n",
      "llama_print_timings:        eval time =     397.91 ms /    20 runs   (   19.90 ms per token,    50.26 tokens per second)\n",
      "llama_print_timings:       total time =     594.33 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.06 ms /     3 runs   (    0.35 ms per token,  2824.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.85 ms /    57 tokens (    2.01 ms per token,   496.30 tokens per second)\n",
      "llama_print_timings:        eval time =      35.84 ms /     2 runs   (   17.92 ms per token,    55.80 tokens per second)\n",
      "llama_print_timings:       total time =     162.53 ms /    59 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /     3 runs   (    0.38 ms per token,  2657.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     115.70 ms /    45 tokens (    2.57 ms per token,   388.94 tokens per second)\n",
      "llama_print_timings:        eval time =      40.58 ms /     2 runs   (   20.29 ms per token,    49.28 tokens per second)\n",
      "llama_print_timings:       total time =     168.95 ms /    47 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.37 ms /    80 runs   (    0.38 ms per token,  2634.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     197.58 ms /   197 tokens (    1.00 ms per token,   997.07 tokens per second)\n",
      "llama_print_timings:        eval time =    1501.14 ms /    79 runs   (   19.00 ms per token,    52.63 tokens per second)\n",
      "llama_print_timings:       total time =    2007.13 ms /   276 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.22 ms /   111 runs   (    0.40 ms per token,  2510.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     180.86 ms /   253 tokens (    0.71 ms per token,  1398.84 tokens per second)\n",
      "llama_print_timings:        eval time =    2131.01 ms /   110 runs   (   19.37 ms per token,    51.62 tokens per second)\n",
      "llama_print_timings:       total time =    2748.53 ms /   363 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      11.62 ms /    31 runs   (    0.37 ms per token,  2668.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.32 ms /    89 tokens (    1.33 ms per token,   752.22 tokens per second)\n",
      "llama_print_timings:        eval time =     584.20 ms /    30 runs   (   19.47 ms per token,    51.35 tokens per second)\n",
      "llama_print_timings:       total time =     816.94 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.00 ms /   102 runs   (    0.38 ms per token,  2615.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     115.20 ms /    60 tokens (    1.92 ms per token,   520.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1888.81 ms /   101 runs   (   18.70 ms per token,    53.47 tokens per second)\n",
      "llama_print_timings:       total time =    2401.02 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     3 runs   (    0.37 ms per token,  2683.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     111.86 ms /    48 tokens (    2.33 ms per token,   429.11 tokens per second)\n",
      "llama_print_timings:        eval time =      38.93 ms /     2 runs   (   19.46 ms per token,    51.38 tokens per second)\n",
      "llama_print_timings:       total time =     164.21 ms /    50 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      20.24 ms /    53 runs   (    0.38 ms per token,  2618.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.40 ms /    90 tokens (    1.35 ms per token,   741.36 tokens per second)\n",
      "llama_print_timings:        eval time =     970.74 ms /    52 runs   (   18.67 ms per token,    53.57 tokens per second)\n",
      "llama_print_timings:       total time =    1286.48 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.47 ms /   142 runs   (    0.38 ms per token,  2655.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.24 ms /   167 tokens (    0.95 ms per token,  1055.35 tokens per second)\n",
      "llama_print_timings:        eval time =    2667.58 ms /   141 runs   (   18.92 ms per token,    52.86 tokens per second)\n",
      "llama_print_timings:       total time =    3371.34 ms /   308 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      95.11 ms /   252 runs   (    0.38 ms per token,  2649.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     323.34 ms /   470 tokens (    0.69 ms per token,  1453.58 tokens per second)\n",
      "llama_print_timings:        eval time =    5207.30 ms /   251 runs   (   20.75 ms per token,    48.20 tokens per second)\n",
      "llama_print_timings:       total time =    6541.34 ms /   721 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      18.50 ms /    47 runs   (    0.39 ms per token,  2540.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.01 ms /    97 tokens (    1.44 ms per token,   692.79 tokens per second)\n",
      "llama_print_timings:        eval time =     881.18 ms /    46 runs   (   19.16 ms per token,    52.20 tokens per second)\n",
      "llama_print_timings:       total time =    1200.23 ms /   143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.19 ms /   123 runs   (    0.38 ms per token,  2663.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     185.16 ms /   188 tokens (    0.98 ms per token,  1015.35 tokens per second)\n",
      "llama_print_timings:        eval time =    2457.03 ms /   122 runs   (   20.14 ms per token,    49.65 tokens per second)\n",
      "llama_print_timings:       total time =    3118.47 ms /   310 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.89 ms /    98 runs   (    0.39 ms per token,  2586.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.79 ms /   196 tokens (    0.88 ms per token,  1134.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1851.11 ms /    97 runs   (   19.08 ms per token,    52.40 tokens per second)\n",
      "llama_print_timings:       total time =    2395.20 ms /   293 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      93.75 ms /   240 runs   (    0.39 ms per token,  2560.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.95 ms /   145 tokens (    1.12 ms per token,   889.85 tokens per second)\n",
      "llama_print_timings:        eval time =    4738.34 ms /   239 runs   (   19.83 ms per token,    50.44 tokens per second)\n",
      "llama_print_timings:       total time =    5906.39 ms /   384 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.45 ms /   106 runs   (    0.37 ms per token,  2686.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     137.66 ms /   115 tokens (    1.20 ms per token,   835.40 tokens per second)\n",
      "llama_print_timings:        eval time =    2016.23 ms /   105 runs   (   19.20 ms per token,    52.08 tokens per second)\n",
      "llama_print_timings:       total time =    2549.28 ms /   220 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.14 ms /    95 runs   (    0.42 ms per token,  2366.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     187.28 ms /   233 tokens (    0.80 ms per token,  1244.11 tokens per second)\n",
      "llama_print_timings:        eval time =    1915.37 ms /    94 runs   (   20.38 ms per token,    49.08 tokens per second)\n",
      "llama_print_timings:       total time =    2559.53 ms /   327 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      11.92 ms /    30 runs   (    0.40 ms per token,  2516.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     240.50 ms /   271 tokens (    0.89 ms per token,  1126.81 tokens per second)\n",
      "llama_print_timings:        eval time =     571.75 ms /    29 runs   (   19.72 ms per token,    50.72 tokens per second)\n",
      "llama_print_timings:       total time =     928.70 ms /   300 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.07 ms /    21 runs   (    0.38 ms per token,  2602.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.42 ms /   140 tokens (    1.11 ms per token,   900.76 tokens per second)\n",
      "llama_print_timings:        eval time =     379.58 ms /    20 runs   (   18.98 ms per token,    52.69 tokens per second)\n",
      "llama_print_timings:       total time =     626.31 ms /   160 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.21 ms /    81 runs   (    0.39 ms per token,  2595.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.48 ms /   136 tokens (    1.12 ms per token,   891.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1543.99 ms /    80 runs   (   19.30 ms per token,    51.81 tokens per second)\n",
      "llama_print_timings:       total time =    2019.14 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2622.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     117.15 ms /    44 tokens (    2.66 ms per token,   375.59 tokens per second)\n",
      "llama_print_timings:        eval time =      38.35 ms /     2 runs   (   19.17 ms per token,    52.16 tokens per second)\n",
      "llama_print_timings:       total time =     168.65 ms /    46 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.64 ms /   103 runs   (    0.42 ms per token,  2360.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.30 ms /   141 tokens (    1.14 ms per token,   874.16 tokens per second)\n",
      "llama_print_timings:        eval time =    2024.17 ms /   102 runs   (   19.84 ms per token,    50.39 tokens per second)\n",
      "llama_print_timings:       total time =    2672.99 ms /   243 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.23 ms /    22 runs   (    0.46 ms per token,  2150.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.23 ms /    73 tokens (    1.67 ms per token,   597.22 tokens per second)\n",
      "llama_print_timings:        eval time =     418.30 ms /    21 runs   (   19.92 ms per token,    50.20 tokens per second)\n",
      "llama_print_timings:       total time =     648.54 ms /    94 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.60 ms /    21 runs   (    0.36 ms per token,  2762.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.93 ms /    71 tokens (    1.68 ms per token,   597.00 tokens per second)\n",
      "llama_print_timings:        eval time =     380.37 ms /    20 runs   (   19.02 ms per token,    52.58 tokens per second)\n",
      "llama_print_timings:       total time =     585.50 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.57 ms /    24 runs   (    0.44 ms per token,  2271.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     254.50 ms /   314 tokens (    0.81 ms per token,  1233.81 tokens per second)\n",
      "llama_print_timings:        eval time =     464.66 ms /    23 runs   (   20.20 ms per token,    49.50 tokens per second)\n",
      "llama_print_timings:       total time =     826.43 ms /   337 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      80.51 ms /   213 runs   (    0.38 ms per token,  2645.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     261.75 ms /   387 tokens (    0.68 ms per token,  1478.49 tokens per second)\n",
      "llama_print_timings:        eval time =    4381.12 ms /   212 runs   (   20.67 ms per token,    48.39 tokens per second)\n",
      "llama_print_timings:       total time =    5492.27 ms /   599 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.85 ms /    71 runs   (    0.39 ms per token,  2549.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     111.73 ms /    98 tokens (    1.14 ms per token,   877.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1372.92 ms /    70 runs   (   19.61 ms per token,    50.99 tokens per second)\n",
      "llama_print_timings:       total time =    1768.62 ms /   168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.17 ms /    89 runs   (    0.47 ms per token,  2110.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.75 ms /   213 tokens (    0.81 ms per token,  1232.97 tokens per second)\n",
      "llama_print_timings:        eval time =    2005.30 ms /    88 runs   (   22.79 ms per token,    43.88 tokens per second)\n",
      "llama_print_timings:       total time =    2675.95 ms /   301 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.27 ms /     3 runs   (    0.42 ms per token,  2367.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.75 ms /   145 tokens (    1.49 ms per token,   668.99 tokens per second)\n",
      "llama_print_timings:        eval time =      49.57 ms /     2 runs   (   24.79 ms per token,    40.35 tokens per second)\n",
      "llama_print_timings:       total time =     280.90 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.89 ms /    99 runs   (    0.37 ms per token,  2683.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     204.72 ms /    23 tokens (    8.90 ms per token,   112.35 tokens per second)\n",
      "llama_print_timings:        eval time =    2182.43 ms /    98 runs   (   22.27 ms per token,    44.90 tokens per second)\n",
      "llama_print_timings:       total time =    2778.57 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.28 ms /   126 runs   (    0.38 ms per token,  2664.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     641.55 ms /   671 tokens (    0.96 ms per token,  1045.91 tokens per second)\n",
      "llama_print_timings:        eval time =    3036.71 ms /   125 runs   (   24.29 ms per token,    41.16 tokens per second)\n",
      "llama_print_timings:       total time =    4188.52 ms /   796 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.44 ms /    26 runs   (    0.36 ms per token,  2755.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.69 ms /    82 tokens (    1.85 ms per token,   540.56 tokens per second)\n",
      "llama_print_timings:        eval time =     587.64 ms /    25 runs   (   23.51 ms per token,    42.54 tokens per second)\n",
      "llama_print_timings:       total time =     849.29 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     3 runs   (    0.39 ms per token,  2581.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.12 ms /    96 tokens (    1.61 ms per token,   622.89 tokens per second)\n",
      "llama_print_timings:        eval time =      50.27 ms /     2 runs   (   25.13 ms per token,    39.79 tokens per second)\n",
      "llama_print_timings:       total time =     216.56 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      78.39 ms /   209 runs   (    0.38 ms per token,  2665.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     194.68 ms /   201 tokens (    0.97 ms per token,  1032.45 tokens per second)\n",
      "llama_print_timings:        eval time =    4619.74 ms /   208 runs   (   22.21 ms per token,    45.02 tokens per second)\n",
      "llama_print_timings:       total time =    5655.83 ms /   409 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       5.37 ms /    17 runs   (    0.32 ms per token,  3167.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.95 ms /    94 tokens (    1.33 ms per token,   752.31 tokens per second)\n",
      "llama_print_timings:        eval time =     360.14 ms /    16 runs   (   22.51 ms per token,    44.43 tokens per second)\n",
      "llama_print_timings:       total time =     555.57 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2615.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.89 ms /    70 tokens (    1.91 ms per token,   522.82 tokens per second)\n",
      "llama_print_timings:        eval time =      45.90 ms /     2 runs   (   22.95 ms per token,    43.57 tokens per second)\n",
      "llama_print_timings:       total time =     190.88 ms /    72 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.48 ms /    22 runs   (    0.39 ms per token,  2594.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.04 ms /    38 tokens (    3.71 ms per token,   269.43 tokens per second)\n",
      "llama_print_timings:        eval time =     455.41 ms /    21 runs   (   21.69 ms per token,    46.11 tokens per second)\n",
      "llama_print_timings:       total time =     677.62 ms /    59 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.40 ms /    18 runs   (    0.41 ms per token,  2431.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     127.53 ms /    55 tokens (    2.32 ms per token,   431.27 tokens per second)\n",
      "llama_print_timings:        eval time =     357.19 ms /    17 runs   (   21.01 ms per token,    47.59 tokens per second)\n",
      "llama_print_timings:       total time =     570.34 ms /    72 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.52 ms /    19 runs   (    0.40 ms per token,  2526.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.16 ms /    58 tokens (    2.28 ms per token,   438.87 tokens per second)\n",
      "llama_print_timings:        eval time =     409.97 ms /    18 runs   (   22.78 ms per token,    43.91 tokens per second)\n",
      "llama_print_timings:       total time =     613.73 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2540.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     146.32 ms /    77 tokens (    1.90 ms per token,   526.24 tokens per second)\n",
      "llama_print_timings:        eval time =      55.55 ms /     2 runs   (   27.77 ms per token,    36.01 tokens per second)\n",
      "llama_print_timings:       total time =     217.12 ms /    79 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.88 ms /    90 runs   (    0.39 ms per token,  2580.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     200.13 ms /   142 tokens (    1.41 ms per token,   709.55 tokens per second)\n",
      "llama_print_timings:        eval time =    1895.09 ms /    89 runs   (   21.29 ms per token,    46.96 tokens per second)\n",
      "llama_print_timings:       total time =    2446.99 ms /   231 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.19 ms /   109 runs   (    0.39 ms per token,  2583.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.15 ms /   127 tokens (    1.22 ms per token,   818.58 tokens per second)\n",
      "llama_print_timings:        eval time =    2418.13 ms /   108 runs   (   22.39 ms per token,    44.66 tokens per second)\n",
      "llama_print_timings:       total time =    3005.23 ms /   235 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.27 ms /    99 runs   (    0.38 ms per token,  2656.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     192.22 ms /   157 tokens (    1.22 ms per token,   816.78 tokens per second)\n",
      "llama_print_timings:        eval time =    1953.31 ms /    98 runs   (   19.93 ms per token,    50.17 tokens per second)\n",
      "llama_print_timings:       total time =    2516.52 ms /   255 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2544.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     147.22 ms /    70 tokens (    2.10 ms per token,   475.48 tokens per second)\n",
      "llama_print_timings:        eval time =      50.52 ms /     2 runs   (   25.26 ms per token,    39.59 tokens per second)\n",
      "llama_print_timings:       total time =     212.92 ms /    72 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.69 ms /    22 runs   (    0.39 ms per token,  2533.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.23 ms /   193 tokens (    1.03 ms per token,   968.72 tokens per second)\n",
      "llama_print_timings:        eval time =     485.25 ms /    21 runs   (   23.11 ms per token,    43.28 tokens per second)\n",
      "llama_print_timings:       total time =     779.56 ms /   214 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.45 ms /   109 runs   (    0.38 ms per token,  2629.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     203.43 ms /   159 tokens (    1.28 ms per token,   781.61 tokens per second)\n",
      "llama_print_timings:        eval time =    2321.81 ms /   108 runs   (   21.50 ms per token,    46.52 tokens per second)\n",
      "llama_print_timings:       total time =    2962.31 ms /   267 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      24.86 ms /    63 runs   (    0.39 ms per token,  2534.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.84 ms /    94 tokens (    1.69 ms per token,   591.78 tokens per second)\n",
      "llama_print_timings:        eval time =    1360.20 ms /    62 runs   (   21.94 ms per token,    45.58 tokens per second)\n",
      "llama_print_timings:       total time =    1766.44 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.44 ms /    78 runs   (    0.39 ms per token,  2562.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.54 ms /   110 tokens (    1.57 ms per token,   637.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1695.65 ms /    77 runs   (   22.02 ms per token,    45.41 tokens per second)\n",
      "llama_print_timings:       total time =    2181.16 ms /   187 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.25 ms /     3 runs   (    0.42 ms per token,  2405.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     191.41 ms /   133 tokens (    1.44 ms per token,   694.83 tokens per second)\n",
      "llama_print_timings:        eval time =      47.26 ms /     2 runs   (   23.63 ms per token,    42.32 tokens per second)\n",
      "llama_print_timings:       total time =     253.45 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.60 ms /   116 runs   (    0.37 ms per token,  2722.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     238.45 ms /   254 tokens (    0.94 ms per token,  1065.23 tokens per second)\n",
      "llama_print_timings:        eval time =    2605.40 ms /   115 runs   (   22.66 ms per token,    44.14 tokens per second)\n",
      "llama_print_timings:       total time =    3321.68 ms /   369 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2599.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     142.66 ms /    56 tokens (    2.55 ms per token,   392.54 tokens per second)\n",
      "llama_print_timings:        eval time =      45.10 ms /     2 runs   (   22.55 ms per token,    44.34 tokens per second)\n",
      "llama_print_timings:       total time =     201.73 ms /    58 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.68 ms /    18 runs   (    0.37 ms per token,  2693.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.38 ms /   116 tokens (    1.49 ms per token,   672.94 tokens per second)\n",
      "llama_print_timings:        eval time =     386.64 ms /    17 runs   (   22.74 ms per token,    43.97 tokens per second)\n",
      "llama_print_timings:       total time =     639.44 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.16 ms /   142 runs   (    0.37 ms per token,  2671.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     294.69 ms /   310 tokens (    0.95 ms per token,  1051.95 tokens per second)\n",
      "llama_print_timings:        eval time =    3081.23 ms /   141 runs   (   21.85 ms per token,    45.76 tokens per second)\n",
      "llama_print_timings:       total time =    3944.16 ms /   451 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      60.27 ms /   160 runs   (    0.38 ms per token,  2654.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.48 ms /   206 tokens (    1.05 ms per token,   956.01 tokens per second)\n",
      "llama_print_timings:        eval time =    3549.59 ms /   159 runs   (   22.32 ms per token,    44.79 tokens per second)\n",
      "llama_print_timings:       total time =    4444.03 ms /   365 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     3 runs   (    0.37 ms per token,  2678.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.45 ms /    56 tokens (    2.26 ms per token,   442.85 tokens per second)\n",
      "llama_print_timings:        eval time =      40.82 ms /     2 runs   (   20.41 ms per token,    48.99 tokens per second)\n",
      "llama_print_timings:       total time =     180.05 ms /    58 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.96 ms /   102 runs   (    0.38 ms per token,  2618.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.81 ms /    55 tokens (    2.47 ms per token,   404.97 tokens per second)\n",
      "llama_print_timings:        eval time =    2110.10 ms /   101 runs   (   20.89 ms per token,    47.86 tokens per second)\n",
      "llama_print_timings:       total time =    2635.16 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.27 ms /    94 runs   (    0.39 ms per token,  2591.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.33 ms /   117 tokens (    1.38 ms per token,   725.24 tokens per second)\n",
      "llama_print_timings:        eval time =    2111.90 ms /    93 runs   (   22.71 ms per token,    44.04 tokens per second)\n",
      "llama_print_timings:       total time =    2648.09 ms /   210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.64 ms /    99 runs   (    0.38 ms per token,  2630.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     194.20 ms /   214 tokens (    0.91 ms per token,  1101.96 tokens per second)\n",
      "llama_print_timings:        eval time =    2142.68 ms /    98 runs   (   21.86 ms per token,    45.74 tokens per second)\n",
      "llama_print_timings:       total time =    2736.91 ms /   312 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.97 ms /    18 runs   (    0.39 ms per token,  2583.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.06 ms /    43 tokens (    3.05 ms per token,   328.09 tokens per second)\n",
      "llama_print_timings:        eval time =     364.12 ms /    17 runs   (   21.42 ms per token,    46.69 tokens per second)\n",
      "llama_print_timings:       total time =     568.98 ms /    60 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      92.62 ms /   245 runs   (    0.38 ms per token,  2645.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     281.58 ms /   313 tokens (    0.90 ms per token,  1111.58 tokens per second)\n",
      "llama_print_timings:        eval time =    5517.06 ms /   244 runs   (   22.61 ms per token,    44.23 tokens per second)\n",
      "llama_print_timings:       total time =    6795.27 ms /   557 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      94.96 ms /   256 runs   (    0.37 ms per token,  2695.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     379.66 ms /   485 tokens (    0.78 ms per token,  1277.45 tokens per second)\n",
      "llama_print_timings:        eval time =    5652.55 ms /   255 runs   (   22.17 ms per token,    45.11 tokens per second)\n",
      "llama_print_timings:       total time =    7060.13 ms /   740 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.21 ms /     3 runs   (    0.40 ms per token,  2477.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.94 ms /    79 tokens (    1.78 ms per token,   560.52 tokens per second)\n",
      "llama_print_timings:        eval time =      47.61 ms /     2 runs   (   23.80 ms per token,    42.01 tokens per second)\n",
      "llama_print_timings:       total time =     205.27 ms /    81 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      65.69 ms /   175 runs   (    0.38 ms per token,  2663.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     333.56 ms /   464 tokens (    0.72 ms per token,  1391.04 tokens per second)\n",
      "llama_print_timings:        eval time =    3907.06 ms /   174 runs   (   22.45 ms per token,    44.53 tokens per second)\n",
      "llama_print_timings:       total time =    4912.58 ms /   638 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.18 ms /    22 runs   (    0.37 ms per token,  2688.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.35 ms /    49 tokens (    2.56 ms per token,   390.91 tokens per second)\n",
      "llama_print_timings:        eval time =     462.85 ms /    21 runs   (   22.04 ms per token,    45.37 tokens per second)\n",
      "llama_print_timings:       total time =     675.91 ms /    70 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      22.93 ms /    59 runs   (    0.39 ms per token,  2573.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     196.14 ms /   211 tokens (    0.93 ms per token,  1075.77 tokens per second)\n",
      "llama_print_timings:        eval time =    1287.17 ms /    58 runs   (   22.19 ms per token,    45.06 tokens per second)\n",
      "llama_print_timings:       total time =    1713.26 ms /   269 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.27 ms /   124 runs   (    0.38 ms per token,  2623.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.58 ms /   158 tokens (    1.36 ms per token,   732.91 tokens per second)\n",
      "llama_print_timings:        eval time =    2671.68 ms /   123 runs   (   21.72 ms per token,    46.04 tokens per second)\n",
      "llama_print_timings:       total time =    3376.80 ms /   281 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.92 ms /    96 runs   (    0.38 ms per token,  2600.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.67 ms /    57 tokens (    2.80 ms per token,   357.00 tokens per second)\n",
      "llama_print_timings:        eval time =    2009.07 ms /    95 runs   (   21.15 ms per token,    47.29 tokens per second)\n",
      "llama_print_timings:       total time =    2531.07 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.85 ms /   109 runs   (    0.37 ms per token,  2668.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.52 ms /   118 tokens (    1.31 ms per token,   763.66 tokens per second)\n",
      "llama_print_timings:        eval time =    2239.83 ms /   108 runs   (   20.74 ms per token,    48.22 tokens per second)\n",
      "llama_print_timings:       total time =    2801.92 ms /   226 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.41 ms /    17 runs   (    0.38 ms per token,  2653.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     142.50 ms /   119 tokens (    1.20 ms per token,   835.08 tokens per second)\n",
      "llama_print_timings:        eval time =     364.62 ms /    16 runs   (   22.79 ms per token,    43.88 tokens per second)\n",
      "llama_print_timings:       total time =     576.46 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    17 runs   (    0.38 ms per token,  2610.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.35 ms /   222 tokens (    0.97 ms per token,  1035.69 tokens per second)\n",
      "llama_print_timings:        eval time =     342.09 ms /    16 runs   (   21.38 ms per token,    46.77 tokens per second)\n",
      "llama_print_timings:       total time =     625.85 ms /   238 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     3 runs   (    0.37 ms per token,  2678.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.58 ms /    43 tokens (    2.90 ms per token,   345.15 tokens per second)\n",
      "llama_print_timings:        eval time =      41.10 ms /     2 runs   (   20.55 ms per token,    48.66 tokens per second)\n",
      "llama_print_timings:       total time =     177.21 ms /    45 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.69 ms /    17 runs   (    0.39 ms per token,  2540.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.34 ms /    71 tokens (    2.09 ms per token,   478.61 tokens per second)\n",
      "llama_print_timings:        eval time =     326.20 ms /    16 runs   (   20.39 ms per token,    49.05 tokens per second)\n",
      "llama_print_timings:       total time =     539.57 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.03 ms /    23 runs   (    0.39 ms per token,  2548.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     143.27 ms /    91 tokens (    1.57 ms per token,   635.16 tokens per second)\n",
      "llama_print_timings:        eval time =     455.64 ms /    22 runs   (   20.71 ms per token,    48.28 tokens per second)\n",
      "llama_print_timings:       total time =     687.38 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.02 ms /    21 runs   (    0.38 ms per token,  2618.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.66 ms /    68 tokens (    1.98 ms per token,   504.96 tokens per second)\n",
      "llama_print_timings:        eval time =     424.99 ms /    20 runs   (   21.25 ms per token,    47.06 tokens per second)\n",
      "llama_print_timings:       total time =     642.48 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      83.64 ms /   225 runs   (    0.37 ms per token,  2690.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     395.93 ms /   504 tokens (    0.79 ms per token,  1272.94 tokens per second)\n",
      "llama_print_timings:        eval time =    5076.95 ms /   224 runs   (   22.66 ms per token,    44.12 tokens per second)\n",
      "llama_print_timings:       total time =    6335.16 ms /   728 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      94.20 ms /   256 runs   (    0.37 ms per token,  2717.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     310.24 ms /   261 tokens (    1.19 ms per token,   841.29 tokens per second)\n",
      "llama_print_timings:        eval time =    5484.96 ms /   255 runs   (   21.51 ms per token,    46.49 tokens per second)\n",
      "llama_print_timings:       total time =    6798.10 ms /   516 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.72 ms /    94 runs   (    0.38 ms per token,  2631.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     175.26 ms /   139 tokens (    1.26 ms per token,   793.10 tokens per second)\n",
      "llama_print_timings:        eval time =    2012.60 ms /    93 runs   (   21.64 ms per token,    46.21 tokens per second)\n",
      "llama_print_timings:       total time =    2564.26 ms /   232 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.09 ms /    93 runs   (    0.39 ms per token,  2576.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.02 ms /   121 tokens (    1.34 ms per token,   746.81 tokens per second)\n",
      "llama_print_timings:        eval time =    2009.51 ms /    92 runs   (   21.84 ms per token,    45.78 tokens per second)\n",
      "llama_print_timings:       total time =    2562.61 ms /   213 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.30 ms /    24 runs   (    0.39 ms per token,  2580.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.38 ms /    63 tokens (    2.09 ms per token,   479.53 tokens per second)\n",
      "llama_print_timings:        eval time =     496.40 ms /    23 runs   (   21.58 ms per token,    46.33 tokens per second)\n",
      "llama_print_timings:       total time =     724.10 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.75 ms /    74 runs   (    0.39 ms per token,  2573.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     143.09 ms /   111 tokens (    1.29 ms per token,   775.75 tokens per second)\n",
      "llama_print_timings:        eval time =    1570.82 ms /    73 runs   (   21.52 ms per token,    46.47 tokens per second)\n",
      "llama_print_timings:       total time =    2011.91 ms /   184 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.88 ms /    26 runs   (    0.38 ms per token,  2630.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.08 ms /   100 tokens (    1.48 ms per token,   675.29 tokens per second)\n",
      "llama_print_timings:        eval time =     548.75 ms /    25 runs   (   21.95 ms per token,    45.56 tokens per second)\n",
      "llama_print_timings:       total time =     801.05 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /     3 runs   (    0.38 ms per token,  2654.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.45 ms /    49 tokens (    2.56 ms per token,   390.61 tokens per second)\n",
      "llama_print_timings:        eval time =      43.49 ms /     2 runs   (   21.74 ms per token,    45.99 tokens per second)\n",
      "llama_print_timings:       total time =     181.23 ms /    51 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /     3 runs   (    0.38 ms per token,  2661.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.99 ms /    47 tokens (    2.87 ms per token,   348.18 tokens per second)\n",
      "llama_print_timings:        eval time =      42.95 ms /     2 runs   (   21.47 ms per token,    46.57 tokens per second)\n",
      "llama_print_timings:       total time =     189.28 ms /    49 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.32 ms /   131 runs   (    0.38 ms per token,  2656.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     198.75 ms /   200 tokens (    0.99 ms per token,  1006.31 tokens per second)\n",
      "llama_print_timings:        eval time =    2853.30 ms /   130 runs   (   21.95 ms per token,    45.56 tokens per second)\n",
      "llama_print_timings:       total time =    3544.20 ms /   330 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      94.24 ms /   256 runs   (    0.37 ms per token,  2716.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     204.97 ms /   256 tokens (    0.80 ms per token,  1248.94 tokens per second)\n",
      "llama_print_timings:        eval time =    5405.56 ms /   255 runs   (   21.20 ms per token,    47.17 tokens per second)\n",
      "llama_print_timings:       total time =    6627.06 ms /   511 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.60 ms /   121 runs   (    0.39 ms per token,  2596.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     188.84 ms /   167 tokens (    1.13 ms per token,   884.34 tokens per second)\n",
      "llama_print_timings:        eval time =    2530.57 ms /   120 runs   (   21.09 ms per token,    47.42 tokens per second)\n",
      "llama_print_timings:       total time =    3193.14 ms /   287 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.15 ms /    26 runs   (    0.39 ms per token,  2561.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     170.62 ms /   138 tokens (    1.24 ms per token,   808.81 tokens per second)\n",
      "llama_print_timings:        eval time =     545.79 ms /    25 runs   (   21.83 ms per token,    45.81 tokens per second)\n",
      "llama_print_timings:       total time =     814.60 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.94 ms /    99 runs   (    0.38 ms per token,  2609.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     184.12 ms /   146 tokens (    1.26 ms per token,   792.98 tokens per second)\n",
      "llama_print_timings:        eval time =    2106.56 ms /    98 runs   (   21.50 ms per token,    46.52 tokens per second)\n",
      "llama_print_timings:       total time =    2674.40 ms /   244 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.07 ms /   110 runs   (    0.37 ms per token,  2678.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     329.27 ms /   406 tokens (    0.81 ms per token,  1233.02 tokens per second)\n",
      "llama_print_timings:        eval time =    2383.22 ms /   109 runs   (   21.86 ms per token,    45.74 tokens per second)\n",
      "llama_print_timings:       total time =    3126.96 ms /   515 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.47 ms /    92 runs   (    0.37 ms per token,  2669.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     184.15 ms /   160 tokens (    1.15 ms per token,   868.87 tokens per second)\n",
      "llama_print_timings:        eval time =    1963.19 ms /    91 runs   (   21.57 ms per token,    46.35 tokens per second)\n",
      "llama_print_timings:       total time =    2497.17 ms /   251 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.17 ms /     3 runs   (    0.39 ms per token,  2557.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.66 ms /    79 tokens (    1.92 ms per token,   520.89 tokens per second)\n",
      "llama_print_timings:        eval time =      54.87 ms /     2 runs   (   27.43 ms per token,    36.45 tokens per second)\n",
      "llama_print_timings:       total time =     219.07 ms /    81 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.30 ms /    73 runs   (    0.39 ms per token,  2579.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     180.27 ms /   131 tokens (    1.38 ms per token,   726.70 tokens per second)\n",
      "llama_print_timings:        eval time =    1441.89 ms /    72 runs   (   20.03 ms per token,    49.93 tokens per second)\n",
      "llama_print_timings:       total time =    1896.94 ms /   203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.11 ms /    98 runs   (    0.38 ms per token,  2640.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     316.04 ms /   365 tokens (    0.87 ms per token,  1154.91 tokens per second)\n",
      "llama_print_timings:        eval time =    2097.42 ms /    97 runs   (   21.62 ms per token,    46.25 tokens per second)\n",
      "llama_print_timings:       total time =    2793.16 ms /   462 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      96.64 ms /   256 runs   (    0.38 ms per token,  2648.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     301.07 ms /   367 tokens (    0.82 ms per token,  1218.98 tokens per second)\n",
      "llama_print_timings:        eval time =    5674.04 ms /   255 runs   (   22.25 ms per token,    44.94 tokens per second)\n",
      "llama_print_timings:       total time =    7001.69 ms /   622 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.58 ms /    17 runs   (    0.39 ms per token,  2582.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.40 ms /    38 tokens (    3.56 ms per token,   280.66 tokens per second)\n",
      "llama_print_timings:        eval time =     377.47 ms /    16 runs   (   23.59 ms per token,    42.39 tokens per second)\n",
      "llama_print_timings:       total time =     583.20 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.11 ms /     3 runs   (    0.37 ms per token,  2707.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.62 ms /    33 tokens (    4.23 ms per token,   236.35 tokens per second)\n",
      "llama_print_timings:        eval time =      50.33 ms /     2 runs   (   25.17 ms per token,    39.74 tokens per second)\n",
      "llama_print_timings:       total time =     202.88 ms /    35 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      79.96 ms /   213 runs   (    0.38 ms per token,  2663.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     425.74 ms /   494 tokens (    0.86 ms per token,  1160.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4683.30 ms /   212 runs   (   22.09 ms per token,    45.27 tokens per second)\n",
      "llama_print_timings:       total time =    5937.91 ms /   706 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.06 ms /    21 runs   (    0.38 ms per token,  2603.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     142.59 ms /    44 tokens (    3.24 ms per token,   308.57 tokens per second)\n",
      "llama_print_timings:        eval time =     444.06 ms /    20 runs   (   22.20 ms per token,    45.04 tokens per second)\n",
      "llama_print_timings:       total time =     667.29 ms /    64 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.25 ms /   128 runs   (    0.38 ms per token,  2653.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.44 ms /    97 tokens (    1.64 ms per token,   608.36 tokens per second)\n",
      "llama_print_timings:        eval time =    2724.73 ms /   127 runs   (   21.45 ms per token,    46.61 tokens per second)\n",
      "llama_print_timings:       total time =    3375.63 ms /   224 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.73 ms /    21 runs   (    0.37 ms per token,  2717.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.85 ms /    37 tokens (    3.40 ms per token,   294.01 tokens per second)\n",
      "llama_print_timings:        eval time =     457.08 ms /    20 runs   (   22.85 ms per token,    43.76 tokens per second)\n",
      "llama_print_timings:       total time =     665.73 ms /    57 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.84 ms /    18 runs   (    0.38 ms per token,  2630.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.01 ms /    96 tokens (    1.65 ms per token,   607.56 tokens per second)\n",
      "llama_print_timings:        eval time =     386.94 ms /    17 runs   (   22.76 ms per token,    43.93 tokens per second)\n",
      "llama_print_timings:       total time =     618.87 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.25 ms /    17 runs   (    0.37 ms per token,  2717.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.01 ms /    38 tokens (    3.26 ms per token,   306.44 tokens per second)\n",
      "llama_print_timings:        eval time =     332.57 ms /    16 runs   (   20.79 ms per token,    48.11 tokens per second)\n",
      "llama_print_timings:       total time =     523.05 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.46 ms /    87 runs   (    0.38 ms per token,  2599.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     189.87 ms /   129 tokens (    1.47 ms per token,   679.41 tokens per second)\n",
      "llama_print_timings:        eval time =    1824.10 ms /    86 runs   (   21.21 ms per token,    47.15 tokens per second)\n",
      "llama_print_timings:       total time =    2339.64 ms /   215 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.97 ms /   109 runs   (    0.37 ms per token,  2727.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     187.77 ms /    28 tokens (    6.71 ms per token,   149.12 tokens per second)\n",
      "llama_print_timings:        eval time =    2378.60 ms /   108 runs   (   22.02 ms per token,    45.40 tokens per second)\n",
      "llama_print_timings:       total time =    2984.13 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.28 ms /   107 runs   (    0.38 ms per token,  2656.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.29 ms /   100 tokens (    1.49 ms per token,   669.84 tokens per second)\n",
      "llama_print_timings:        eval time =    2278.87 ms /   106 runs   (   21.50 ms per token,    46.51 tokens per second)\n",
      "llama_print_timings:       total time =    2831.49 ms /   206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      78.92 ms /   216 runs   (    0.37 ms per token,  2736.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     316.48 ms /   400 tokens (    0.79 ms per token,  1263.91 tokens per second)\n",
      "llama_print_timings:        eval time =    4721.25 ms /   215 runs   (   21.96 ms per token,    45.54 tokens per second)\n",
      "llama_print_timings:       total time =    5878.02 ms /   615 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.88 ms /    85 runs   (    0.39 ms per token,  2584.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.04 ms /   217 tokens (    0.92 ms per token,  1090.22 tokens per second)\n",
      "llama_print_timings:        eval time =    1795.62 ms /    84 runs   (   21.38 ms per token,    46.78 tokens per second)\n",
      "llama_print_timings:       total time =    2326.55 ms /   301 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.39 ms /    17 runs   (    0.38 ms per token,  2660.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.01 ms /    85 tokens (    1.69 ms per token,   590.23 tokens per second)\n",
      "llama_print_timings:        eval time =     338.00 ms /    16 runs   (   21.12 ms per token,    47.34 tokens per second)\n",
      "llama_print_timings:       total time =     549.37 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.18 ms /    21 runs   (    0.39 ms per token,  2568.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     175.58 ms /   132 tokens (    1.33 ms per token,   751.81 tokens per second)\n",
      "llama_print_timings:        eval time =     431.76 ms /    20 runs   (   21.59 ms per token,    46.32 tokens per second)\n",
      "llama_print_timings:       total time =     691.38 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      23.94 ms /    64 runs   (    0.37 ms per token,  2673.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.28 ms /    56 tokens (    2.40 ms per token,   417.03 tokens per second)\n",
      "llama_print_timings:        eval time =    1353.67 ms /    63 runs   (   21.49 ms per token,    46.54 tokens per second)\n",
      "llama_print_timings:       total time =    1730.69 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.99 ms /   120 runs   (    0.38 ms per token,  2609.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     208.16 ms /   185 tokens (    1.13 ms per token,   888.74 tokens per second)\n",
      "llama_print_timings:        eval time =    2570.95 ms /   119 runs   (   21.60 ms per token,    46.29 tokens per second)\n",
      "llama_print_timings:       total time =    3253.05 ms /   304 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.08 ms /    90 runs   (    0.37 ms per token,  2720.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     237.29 ms /   181 tokens (    1.31 ms per token,   762.78 tokens per second)\n",
      "llama_print_timings:        eval time =    1897.08 ms /    89 runs   (   21.32 ms per token,    46.91 tokens per second)\n",
      "llama_print_timings:       total time =    2474.67 ms /   270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.32 ms /    17 runs   (    0.37 ms per token,  2691.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.11 ms /    94 tokens (    1.68 ms per token,   594.52 tokens per second)\n",
      "llama_print_timings:        eval time =     354.90 ms /    16 runs   (   22.18 ms per token,    45.08 tokens per second)\n",
      "llama_print_timings:       total time =     579.20 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.78 ms /    17 runs   (    0.40 ms per token,  2506.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.74 ms /    98 tokens (    1.45 ms per token,   691.39 tokens per second)\n",
      "llama_print_timings:        eval time =     352.67 ms /    16 runs   (   22.04 ms per token,    45.37 tokens per second)\n",
      "llama_print_timings:       total time =     565.52 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.11 ms /    21 runs   (    0.39 ms per token,  2590.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     301.51 ms /   264 tokens (    1.14 ms per token,   875.59 tokens per second)\n",
      "llama_print_timings:        eval time =     450.64 ms /    20 runs   (   22.53 ms per token,    44.38 tokens per second)\n",
      "llama_print_timings:       total time =     829.68 ms /   284 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.04 ms /   108 runs   (    0.38 ms per token,  2631.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     225.51 ms /   240 tokens (    0.94 ms per token,  1064.24 tokens per second)\n",
      "llama_print_timings:        eval time =    2286.18 ms /   107 runs   (   21.37 ms per token,    46.80 tokens per second)\n",
      "llama_print_timings:       total time =    2920.62 ms /   347 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.99 ms /    94 runs   (    0.38 ms per token,  2611.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     142.33 ms /   110 tokens (    1.29 ms per token,   772.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1953.82 ms /    93 runs   (   21.01 ms per token,    47.60 tokens per second)\n",
      "llama_print_timings:       total time =    2457.37 ms /   203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.19 ms /     3 runs   (    0.40 ms per token,  2523.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     137.75 ms /    64 tokens (    2.15 ms per token,   464.62 tokens per second)\n",
      "llama_print_timings:        eval time =      48.67 ms /     2 runs   (   24.34 ms per token,    41.09 tokens per second)\n",
      "llama_print_timings:       total time =     198.28 ms /    66 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.41 ms /    21 runs   (    0.40 ms per token,  2497.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     136.94 ms /    78 tokens (    1.76 ms per token,   569.60 tokens per second)\n",
      "llama_print_timings:        eval time =     479.68 ms /    20 runs   (   23.98 ms per token,    41.69 tokens per second)\n",
      "llama_print_timings:       total time =     706.50 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.21 ms /   122 runs   (    0.37 ms per token,  2698.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     290.99 ms /   273 tokens (    1.07 ms per token,   938.17 tokens per second)\n",
      "llama_print_timings:        eval time =    2690.96 ms /   121 runs   (   22.24 ms per token,    44.97 tokens per second)\n",
      "llama_print_timings:       total time =    3455.33 ms /   394 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.06 ms /   124 runs   (    0.37 ms per token,  2692.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     147.35 ms /    82 tokens (    1.80 ms per token,   556.49 tokens per second)\n",
      "llama_print_timings:        eval time =    2625.63 ms /   123 runs   (   21.35 ms per token,    46.85 tokens per second)\n",
      "llama_print_timings:       total time =    3264.79 ms /   205 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      13.21 ms /    34 runs   (    0.39 ms per token,  2574.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.69 ms /   100 tokens (    1.49 ms per token,   672.54 tokens per second)\n",
      "llama_print_timings:        eval time =     724.50 ms /    33 runs   (   21.95 ms per token,    45.55 tokens per second)\n",
      "llama_print_timings:       total time =    1000.95 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.47 ms /   107 runs   (    0.38 ms per token,  2644.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     200.13 ms /   202 tokens (    0.99 ms per token,  1009.33 tokens per second)\n",
      "llama_print_timings:        eval time =    2255.59 ms /   106 runs   (   21.28 ms per token,    46.99 tokens per second)\n",
      "llama_print_timings:       total time =    2860.13 ms /   308 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      95.31 ms /   256 runs   (    0.37 ms per token,  2685.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     374.30 ms /   411 tokens (    0.91 ms per token,  1098.04 tokens per second)\n",
      "llama_print_timings:        eval time =    5638.13 ms /   255 runs   (   22.11 ms per token,    45.23 tokens per second)\n",
      "llama_print_timings:       total time =    7019.15 ms /   666 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      23.47 ms /    62 runs   (    0.38 ms per token,  2641.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     185.34 ms /   211 tokens (    0.88 ms per token,  1138.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1391.92 ms /    61 runs   (   22.82 ms per token,    43.82 tokens per second)\n",
      "llama_print_timings:       total time =    1811.01 ms /   272 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      86.06 ms /   229 runs   (    0.38 ms per token,  2660.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     317.16 ms /   285 tokens (    1.11 ms per token,   898.59 tokens per second)\n",
      "llama_print_timings:        eval time =    5033.99 ms /   228 runs   (   22.08 ms per token,    45.29 tokens per second)\n",
      "llama_print_timings:       total time =    6252.35 ms /   513 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     3 runs   (    0.39 ms per token,  2577.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.34 ms /    74 tokens (    1.95 ms per token,   512.67 tokens per second)\n",
      "llama_print_timings:        eval time =      53.69 ms /     2 runs   (   26.85 ms per token,    37.25 tokens per second)\n",
      "llama_print_timings:       total time =     209.86 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.75 ms /    98 runs   (    0.39 ms per token,  2595.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.36 ms /    84 tokens (    1.87 ms per token,   533.80 tokens per second)\n",
      "llama_print_timings:        eval time =    2048.24 ms /    97 runs   (   21.12 ms per token,    47.36 tokens per second)\n",
      "llama_print_timings:       total time =    2572.19 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.23 ms /    72 runs   (    0.38 ms per token,  2644.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.69 ms /   167 tokens (    1.28 ms per token,   781.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1496.88 ms /    71 runs   (   21.08 ms per token,    47.43 tokens per second)\n",
      "llama_print_timings:       total time =    1985.54 ms /   238 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.41 ms /    85 runs   (    0.37 ms per token,  2705.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.90 ms /   107 tokens (    1.31 ms per token,   764.82 tokens per second)\n",
      "llama_print_timings:        eval time =    1806.43 ms /    84 runs   (   21.51 ms per token,    46.50 tokens per second)\n",
      "llama_print_timings:       total time =    2274.31 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      61.85 ms /   166 runs   (    0.37 ms per token,  2684.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     196.49 ms /   181 tokens (    1.09 ms per token,   921.17 tokens per second)\n",
      "llama_print_timings:        eval time =    3571.32 ms /   165 runs   (   21.64 ms per token,    46.20 tokens per second)\n",
      "llama_print_timings:       total time =    4406.21 ms /   346 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.64 ms /    27 runs   (    0.39 ms per token,  2538.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     142.03 ms /    98 tokens (    1.45 ms per token,   689.99 tokens per second)\n",
      "llama_print_timings:        eval time =     561.69 ms /    26 runs   (   21.60 ms per token,    46.29 tokens per second)\n",
      "llama_print_timings:       total time =     809.25 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2548.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     276.11 ms /   356 tokens (    0.78 ms per token,  1289.35 tokens per second)\n",
      "llama_print_timings:        eval time =      48.59 ms /     2 runs   (   24.29 ms per token,    41.16 tokens per second)\n",
      "llama_print_timings:       total time =     339.84 ms /   358 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.10 ms /     3 runs   (    0.37 ms per token,  2714.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     153.50 ms /   127 tokens (    1.21 ms per token,   827.35 tokens per second)\n",
      "llama_print_timings:        eval time =      46.28 ms /     2 runs   (   23.14 ms per token,    43.21 tokens per second)\n",
      "llama_print_timings:       total time =     212.13 ms /   129 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.45 ms /    27 runs   (    0.39 ms per token,  2583.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     176.62 ms /   174 tokens (    1.02 ms per token,   985.19 tokens per second)\n",
      "llama_print_timings:        eval time =     559.89 ms /    26 runs   (   21.53 ms per token,    46.44 tokens per second)\n",
      "llama_print_timings:       total time =     850.46 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.74 ms /   138 runs   (    0.38 ms per token,  2616.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     192.76 ms /   144 tokens (    1.34 ms per token,   747.05 tokens per second)\n",
      "llama_print_timings:        eval time =    3041.83 ms /   137 runs   (   22.20 ms per token,    45.04 tokens per second)\n",
      "llama_print_timings:       total time =    3777.14 ms /   281 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.17 ms /     3 runs   (    0.39 ms per token,  2572.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     284.63 ms /   284 tokens (    1.00 ms per token,   997.77 tokens per second)\n",
      "llama_print_timings:        eval time =      46.33 ms /     2 runs   (   23.17 ms per token,    43.16 tokens per second)\n",
      "llama_print_timings:       total time =     349.68 ms /   286 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /     3 runs   (    0.38 ms per token,  2661.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.05 ms /    60 tokens (    2.30 ms per token,   434.64 tokens per second)\n",
      "llama_print_timings:        eval time =      50.82 ms /     2 runs   (   25.41 ms per token,    39.35 tokens per second)\n",
      "llama_print_timings:       total time =     203.12 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.87 ms /    83 runs   (    0.38 ms per token,  2604.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     203.84 ms /   153 tokens (    1.33 ms per token,   750.58 tokens per second)\n",
      "llama_print_timings:        eval time =    1721.34 ms /    82 runs   (   20.99 ms per token,    47.64 tokens per second)\n",
      "llama_print_timings:       total time =    2243.12 ms /   235 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.20 ms /     3 runs   (    0.40 ms per token,  2495.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     128.32 ms /    83 tokens (    1.55 ms per token,   646.81 tokens per second)\n",
      "llama_print_timings:        eval time =      46.61 ms /     2 runs   (   23.30 ms per token,    42.91 tokens per second)\n",
      "llama_print_timings:       total time =     189.31 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      55.94 ms /   148 runs   (    0.38 ms per token,  2645.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     188.15 ms /   225 tokens (    0.84 ms per token,  1195.86 tokens per second)\n",
      "llama_print_timings:        eval time =    3288.21 ms /   147 runs   (   22.37 ms per token,    44.71 tokens per second)\n",
      "llama_print_timings:       total time =    4072.53 ms /   372 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.61 ms /    93 runs   (    0.37 ms per token,  2687.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.65 ms /    98 tokens (    1.37 ms per token,   727.80 tokens per second)\n",
      "llama_print_timings:        eval time =    1908.02 ms /    92 runs   (   20.74 ms per token,    48.22 tokens per second)\n",
      "llama_print_timings:       total time =    2394.69 ms /   190 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.70 ms /   139 runs   (    0.38 ms per token,  2637.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     146.39 ms /    76 tokens (    1.93 ms per token,   519.15 tokens per second)\n",
      "llama_print_timings:        eval time =    2922.60 ms /   138 runs   (   21.18 ms per token,    47.22 tokens per second)\n",
      "llama_print_timings:       total time =    3601.21 ms /   214 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      24.59 ms /    64 runs   (    0.38 ms per token,  2602.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     183.50 ms /   230 tokens (    0.80 ms per token,  1253.41 tokens per second)\n",
      "llama_print_timings:        eval time =    1416.91 ms /    63 runs   (   22.49 ms per token,    44.46 tokens per second)\n",
      "llama_print_timings:       total time =    1845.58 ms /   293 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      64.34 ms /   173 runs   (    0.37 ms per token,  2688.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     291.28 ms /   272 tokens (    1.07 ms per token,   933.82 tokens per second)\n",
      "llama_print_timings:        eval time =    3726.85 ms /   172 runs   (   21.67 ms per token,    46.15 tokens per second)\n",
      "llama_print_timings:       total time =    4677.85 ms /   444 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      85.27 ms /   229 runs   (    0.37 ms per token,  2685.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.20 ms /   240 tokens (    0.83 ms per token,  1204.84 tokens per second)\n",
      "llama_print_timings:        eval time =    4950.36 ms /   228 runs   (   21.71 ms per token,    46.06 tokens per second)\n",
      "llama_print_timings:       total time =    6068.34 ms /   468 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2601.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     499.44 ms /   619 tokens (    0.81 ms per token,  1239.39 tokens per second)\n",
      "llama_print_timings:        eval time =      39.74 ms /     2 runs   (   19.87 ms per token,    50.33 tokens per second)\n",
      "llama_print_timings:       total time =     553.37 ms /   621 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.65 ms /    17 runs   (    0.39 ms per token,  2557.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     114.24 ms /    59 tokens (    1.94 ms per token,   516.44 tokens per second)\n",
      "llama_print_timings:        eval time =     351.13 ms /    16 runs   (   21.95 ms per token,    45.57 tokens per second)\n",
      "llama_print_timings:       total time =     550.94 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.23 ms /    83 runs   (    0.38 ms per token,  2657.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     177.76 ms /   130 tokens (    1.37 ms per token,   731.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1744.17 ms /    82 runs   (   21.27 ms per token,    47.01 tokens per second)\n",
      "llama_print_timings:       total time =    2237.16 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /     3 runs   (    0.38 ms per token,  2650.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     188.85 ms /   143 tokens (    1.32 ms per token,   757.21 tokens per second)\n",
      "llama_print_timings:        eval time =      45.71 ms /     2 runs   (   22.85 ms per token,    43.76 tokens per second)\n",
      "llama_print_timings:       total time =     245.85 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.76 ms /   143 runs   (    0.38 ms per token,  2659.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     129.84 ms /   107 tokens (    1.21 ms per token,   824.06 tokens per second)\n",
      "llama_print_timings:        eval time =    2974.14 ms /   142 runs   (   20.94 ms per token,    47.74 tokens per second)\n",
      "llama_print_timings:       total time =    3656.34 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.85 ms /   105 runs   (    0.38 ms per token,  2635.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     211.75 ms /   181 tokens (    1.17 ms per token,   854.76 tokens per second)\n",
      "llama_print_timings:        eval time =    2234.65 ms /   104 runs   (   21.49 ms per token,    46.54 tokens per second)\n",
      "llama_print_timings:       total time =    2852.85 ms /   285 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      71.60 ms /   195 runs   (    0.37 ms per token,  2723.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     289.04 ms /   295 tokens (    0.98 ms per token,  1020.63 tokens per second)\n",
      "llama_print_timings:        eval time =    4266.57 ms /   194 runs   (   21.99 ms per token,    45.47 tokens per second)\n",
      "llama_print_timings:       total time =    5311.80 ms /   489 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.55 ms /   121 runs   (    0.37 ms per token,  2716.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.09 ms /   172 tokens (    1.25 ms per token,   799.65 tokens per second)\n",
      "llama_print_timings:        eval time =    2565.98 ms /   120 runs   (   21.38 ms per token,    46.77 tokens per second)\n",
      "llama_print_timings:       total time =    3252.01 ms /   292 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.86 ms /   128 runs   (    0.37 ms per token,  2731.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.25 ms /   192 tokens (    1.13 ms per token,   887.87 tokens per second)\n",
      "llama_print_timings:        eval time =    2832.02 ms /   127 runs   (   22.30 ms per token,    44.84 tokens per second)\n",
      "llama_print_timings:       total time =    3529.48 ms /   319 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.02 ms /   105 runs   (    0.36 ms per token,  2761.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.67 ms /    94 tokens (    1.49 ms per token,   673.03 tokens per second)\n",
      "llama_print_timings:        eval time =    2102.39 ms /   104 runs   (   20.22 ms per token,    49.47 tokens per second)\n",
      "llama_print_timings:       total time =    2638.42 ms /   198 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      47.61 ms /   124 runs   (    0.38 ms per token,  2604.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.33 ms /   231 tokens (    0.95 ms per token,  1053.22 tokens per second)\n",
      "llama_print_timings:        eval time =    2618.83 ms /   123 runs   (   21.29 ms per token,    46.97 tokens per second)\n",
      "llama_print_timings:       total time =    3314.26 ms /   354 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      26.03 ms /    69 runs   (    0.38 ms per token,  2650.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     287.76 ms /   335 tokens (    0.86 ms per token,  1164.18 tokens per second)\n",
      "llama_print_timings:        eval time =    1466.45 ms /    68 runs   (   21.57 ms per token,    46.37 tokens per second)\n",
      "llama_print_timings:       total time =    2008.62 ms /   403 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.29 ms /    81 runs   (    0.39 ms per token,  2588.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.08 ms /    58 tokens (    2.14 ms per token,   467.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1703.57 ms /    80 runs   (   21.29 ms per token,    46.96 tokens per second)\n",
      "llama_print_timings:       total time =    2138.36 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.40 ms /   131 runs   (    0.37 ms per token,  2706.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.50 ms /   207 tokens (    0.97 ms per token,  1027.32 tokens per second)\n",
      "llama_print_timings:        eval time =    2866.60 ms /   130 runs   (   22.05 ms per token,    45.35 tokens per second)\n",
      "llama_print_timings:       total time =    3583.29 ms /   337 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.23 ms /   144 runs   (    0.37 ms per token,  2705.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     206.85 ms /   178 tokens (    1.16 ms per token,   860.51 tokens per second)\n",
      "llama_print_timings:        eval time =    3042.74 ms /   143 runs   (   21.28 ms per token,    47.00 tokens per second)\n",
      "llama_print_timings:       total time =    3805.65 ms /   321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2599.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     182.21 ms /   143 tokens (    1.27 ms per token,   784.83 tokens per second)\n",
      "llama_print_timings:        eval time =      47.87 ms /     2 runs   (   23.93 ms per token,    41.78 tokens per second)\n",
      "llama_print_timings:       total time =     241.02 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.82 ms /   127 runs   (    0.38 ms per token,  2601.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     186.15 ms /   133 tokens (    1.40 ms per token,   714.49 tokens per second)\n",
      "llama_print_timings:        eval time =    2651.78 ms /   126 runs   (   21.05 ms per token,    47.52 tokens per second)\n",
      "llama_print_timings:       total time =    3324.68 ms /   259 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.45 ms /    99 runs   (    0.38 ms per token,  2643.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     137.41 ms /    92 tokens (    1.49 ms per token,   669.55 tokens per second)\n",
      "llama_print_timings:        eval time =    2108.13 ms /    98 runs   (   21.51 ms per token,    46.49 tokens per second)\n",
      "llama_print_timings:       total time =    2629.12 ms /   190 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.40 ms /   114 runs   (    0.37 ms per token,  2688.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     204.31 ms /   186 tokens (    1.10 ms per token,   910.40 tokens per second)\n",
      "llama_print_timings:        eval time =    2448.43 ms /   113 runs   (   21.67 ms per token,    46.15 tokens per second)\n",
      "llama_print_timings:       total time =    3085.31 ms /   299 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.20 ms /     3 runs   (    0.40 ms per token,  2510.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     197.10 ms /   134 tokens (    1.47 ms per token,   679.85 tokens per second)\n",
      "llama_print_timings:        eval time =      56.62 ms /     2 runs   (   28.31 ms per token,    35.32 tokens per second)\n",
      "llama_print_timings:       total time =     267.58 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     3 runs   (    0.37 ms per token,  2676.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     127.42 ms /    45 tokens (    2.83 ms per token,   353.17 tokens per second)\n",
      "llama_print_timings:        eval time =      49.57 ms /     2 runs   (   24.79 ms per token,    40.35 tokens per second)\n",
      "llama_print_timings:       total time =     189.41 ms /    47 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      93.31 ms /   252 runs   (    0.37 ms per token,  2700.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     701.03 ms /   866 tokens (    0.81 ms per token,  1235.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5763.80 ms /   251 runs   (   22.96 ms per token,    43.55 tokens per second)\n",
      "llama_print_timings:       total time =    7458.73 ms /  1117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.27 ms /    21 runs   (    0.39 ms per token,  2539.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     306.24 ms /   290 tokens (    1.06 ms per token,   946.97 tokens per second)\n",
      "llama_print_timings:        eval time =     440.42 ms /    20 runs   (   22.02 ms per token,    45.41 tokens per second)\n",
      "llama_print_timings:       total time =     826.94 ms /   310 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.62 ms /   141 runs   (    0.38 ms per token,  2629.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     223.51 ms /   226 tokens (    0.99 ms per token,  1011.15 tokens per second)\n",
      "llama_print_timings:        eval time =    3051.34 ms /   140 runs   (   21.80 ms per token,    45.88 tokens per second)\n",
      "llama_print_timings:       total time =    3811.57 ms /   366 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.23 ms /    88 runs   (    0.38 ms per token,  2648.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     228.62 ms /   204 tokens (    1.12 ms per token,   892.31 tokens per second)\n",
      "llama_print_timings:        eval time =    1940.70 ms /    87 runs   (   22.31 ms per token,    44.83 tokens per second)\n",
      "llama_print_timings:       total time =    2499.14 ms /   291 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.76 ms /   132 runs   (    0.38 ms per token,  2652.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.76 ms /   102 tokens (    1.46 ms per token,   685.68 tokens per second)\n",
      "llama_print_timings:        eval time =    2767.27 ms /   131 runs   (   21.12 ms per token,    47.34 tokens per second)\n",
      "llama_print_timings:       total time =    3423.49 ms /   233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.80 ms /    21 runs   (    0.37 ms per token,  2693.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     136.29 ms /    35 tokens (    3.89 ms per token,   256.80 tokens per second)\n",
      "llama_print_timings:        eval time =     457.75 ms /    20 runs   (   22.89 ms per token,    43.69 tokens per second)\n",
      "llama_print_timings:       total time =     681.08 ms /    55 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.17 ms /   107 runs   (    0.38 ms per token,  2663.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     196.36 ms /   142 tokens (    1.38 ms per token,   723.15 tokens per second)\n",
      "llama_print_timings:        eval time =    2321.18 ms /   106 runs   (   21.90 ms per token,    45.67 tokens per second)\n",
      "llama_print_timings:       total time =    2928.17 ms /   248 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.19 ms /    95 runs   (    0.38 ms per token,  2625.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.61 ms /   110 tokens (    1.45 ms per token,   689.17 tokens per second)\n",
      "llama_print_timings:        eval time =    1986.17 ms /    94 runs   (   21.13 ms per token,    47.33 tokens per second)\n",
      "llama_print_timings:       total time =    2501.73 ms /   204 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.27 ms /   111 runs   (    0.38 ms per token,  2625.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     218.30 ms /   217 tokens (    1.01 ms per token,   994.03 tokens per second)\n",
      "llama_print_timings:        eval time =    2344.91 ms /   110 runs   (   21.32 ms per token,    46.91 tokens per second)\n",
      "llama_print_timings:       total time =    2988.98 ms /   327 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.35 ms /    19 runs   (    0.39 ms per token,  2583.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.96 ms /    36 tokens (    3.69 ms per token,   270.76 tokens per second)\n",
      "llama_print_timings:        eval time =     385.04 ms /    18 runs   (   21.39 ms per token,    46.75 tokens per second)\n",
      "llama_print_timings:       total time =     607.75 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.95 ms /    21 runs   (    0.43 ms per token,  2347.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.86 ms /    69 tokens (    1.94 ms per token,   515.47 tokens per second)\n",
      "llama_print_timings:        eval time =     485.66 ms /    20 runs   (   24.28 ms per token,    41.18 tokens per second)\n",
      "llama_print_timings:       total time =     737.40 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.05 ms /    25 runs   (    0.40 ms per token,  2487.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.49 ms /    97 tokens (    1.60 ms per token,   623.83 tokens per second)\n",
      "llama_print_timings:        eval time =     570.96 ms /    24 runs   (   23.79 ms per token,    42.03 tokens per second)\n",
      "llama_print_timings:       total time =     833.11 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.29 ms /   134 runs   (    0.38 ms per token,  2664.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.38 ms /   166 tokens (    1.29 ms per token,   777.94 tokens per second)\n",
      "llama_print_timings:        eval time =    2996.65 ms /   133 runs   (   22.53 ms per token,    44.38 tokens per second)\n",
      "llama_print_timings:       total time =    3761.65 ms /   299 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      95.92 ms /   255 runs   (    0.38 ms per token,  2658.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     401.71 ms /   485 tokens (    0.83 ms per token,  1207.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5931.28 ms /   254 runs   (   23.35 ms per token,    42.82 tokens per second)\n",
      "llama_print_timings:       total time =    7406.50 ms /   739 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.72 ms /   106 runs   (    0.38 ms per token,  2603.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     294.09 ms /   269 tokens (    1.09 ms per token,   914.69 tokens per second)\n",
      "llama_print_timings:        eval time =    2475.93 ms /   105 runs   (   23.58 ms per token,    42.41 tokens per second)\n",
      "llama_print_timings:       total time =    3197.85 ms /   374 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.75 ms /    72 runs   (    0.40 ms per token,  2504.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.28 ms /    96 tokens (    1.64 ms per token,   610.38 tokens per second)\n",
      "llama_print_timings:        eval time =    1552.95 ms /    71 runs   (   21.87 ms per token,    45.72 tokens per second)\n",
      "llama_print_timings:       total time =    2019.94 ms /   167 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      51.84 ms /   138 runs   (    0.38 ms per token,  2662.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     186.63 ms /   167 tokens (    1.12 ms per token,   894.81 tokens per second)\n",
      "llama_print_timings:        eval time =    3148.83 ms /   137 runs   (   22.98 ms per token,    43.51 tokens per second)\n",
      "llama_print_timings:       total time =    3905.42 ms /   304 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.29 ms /    98 runs   (    0.37 ms per token,  2700.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     241.62 ms /   250 tokens (    0.97 ms per token,  1034.70 tokens per second)\n",
      "llama_print_timings:        eval time =    2171.60 ms /    97 runs   (   22.39 ms per token,    44.67 tokens per second)\n",
      "llama_print_timings:       total time =    2796.18 ms /   347 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.96 ms /   111 runs   (    0.38 ms per token,  2645.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     192.13 ms /   155 tokens (    1.24 ms per token,   806.75 tokens per second)\n",
      "llama_print_timings:        eval time =    2467.12 ms /   110 runs   (   22.43 ms per token,    44.59 tokens per second)\n",
      "llama_print_timings:       total time =    3094.65 ms /   265 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.41 ms /   140 runs   (    0.38 ms per token,  2621.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     142.26 ms /    69 tokens (    2.06 ms per token,   485.01 tokens per second)\n",
      "llama_print_timings:        eval time =    2964.55 ms /   139 runs   (   21.33 ms per token,    46.89 tokens per second)\n",
      "llama_print_timings:       total time =    3669.43 ms /   208 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.33 ms /     3 runs   (    0.44 ms per token,  2259.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.45 ms /    80 tokens (    1.93 ms per token,   517.97 tokens per second)\n",
      "llama_print_timings:        eval time =      47.22 ms /     2 runs   (   23.61 ms per token,    42.36 tokens per second)\n",
      "llama_print_timings:       total time =     221.11 ms /    82 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.91 ms /    17 runs   (    0.41 ms per token,  2460.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.38 ms /    81 tokens (    1.62 ms per token,   616.54 tokens per second)\n",
      "llama_print_timings:        eval time =     355.10 ms /    16 runs   (   22.19 ms per token,    45.06 tokens per second)\n",
      "llama_print_timings:       total time =     571.31 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.65 ms /   125 runs   (    0.37 ms per token,  2738.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.61 ms /    88 tokens (    1.61 ms per token,   621.44 tokens per second)\n",
      "llama_print_timings:        eval time =    2576.85 ms /   124 runs   (   20.78 ms per token,    48.12 tokens per second)\n",
      "llama_print_timings:       total time =    3200.65 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.77 ms /    18 runs   (    0.43 ms per token,  2315.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     147.07 ms /    74 tokens (    1.99 ms per token,   503.17 tokens per second)\n",
      "llama_print_timings:        eval time =     391.14 ms /    17 runs   (   23.01 ms per token,    43.46 tokens per second)\n",
      "llama_print_timings:       total time =     632.35 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      75.14 ms /   198 runs   (    0.38 ms per token,  2635.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     189.19 ms /   181 tokens (    1.05 ms per token,   956.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4325.50 ms /   197 runs   (   21.96 ms per token,    45.54 tokens per second)\n",
      "llama_print_timings:       total time =    5344.07 ms /   378 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.24 ms /    18 runs   (    0.40 ms per token,  2485.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.63 ms /    78 tokens (    1.98 ms per token,   504.43 tokens per second)\n",
      "llama_print_timings:        eval time =     390.91 ms /    17 runs   (   22.99 ms per token,    43.49 tokens per second)\n",
      "llama_print_timings:       total time =     623.15 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.30 ms /    95 runs   (    0.37 ms per token,  2690.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.58 ms /   104 tokens (    1.39 ms per token,   719.33 tokens per second)\n",
      "llama_print_timings:        eval time =    1983.78 ms /    94 runs   (   21.10 ms per token,    47.38 tokens per second)\n",
      "llama_print_timings:       total time =    2509.11 ms /   198 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      54.26 ms /   140 runs   (    0.39 ms per token,  2580.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.17 ms /   246 tokens (    0.88 ms per token,  1137.99 tokens per second)\n",
      "llama_print_timings:        eval time =    2948.42 ms /   139 runs   (   21.21 ms per token,    47.14 tokens per second)\n",
      "llama_print_timings:       total time =    3737.17 ms /   385 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.29 ms /    93 runs   (    0.38 ms per token,  2635.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     189.16 ms /   140 tokens (    1.35 ms per token,   740.10 tokens per second)\n",
      "llama_print_timings:        eval time =    1931.84 ms /    92 runs   (   21.00 ms per token,    47.62 tokens per second)\n",
      "llama_print_timings:       total time =    2494.83 ms /   232 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.15 ms /    90 runs   (    0.39 ms per token,  2560.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.47 ms /   119 tokens (    1.32 ms per token,   755.69 tokens per second)\n",
      "llama_print_timings:        eval time =    1855.20 ms /    89 runs   (   20.84 ms per token,    47.97 tokens per second)\n",
      "llama_print_timings:       total time =    2365.69 ms /   208 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.23 ms /     3 runs   (    0.41 ms per token,  2435.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.11 ms /   107 tokens (    1.39 ms per token,   717.60 tokens per second)\n",
      "llama_print_timings:        eval time =      47.01 ms /     2 runs   (   23.50 ms per token,    42.55 tokens per second)\n",
      "llama_print_timings:       total time =     211.22 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.08 ms /   109 runs   (    0.38 ms per token,  2653.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.98 ms /   219 tokens (    1.00 ms per token,   995.54 tokens per second)\n",
      "llama_print_timings:        eval time =    2329.93 ms /   108 runs   (   21.57 ms per token,    46.35 tokens per second)\n",
      "llama_print_timings:       total time =    2984.98 ms /   327 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.61 ms /   103 runs   (    0.37 ms per token,  2667.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     183.41 ms /   177 tokens (    1.04 ms per token,   965.04 tokens per second)\n",
      "llama_print_timings:        eval time =    2138.70 ms /   102 runs   (   20.97 ms per token,    47.69 tokens per second)\n",
      "llama_print_timings:       total time =    2723.88 ms /   279 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      62.96 ms /   164 runs   (    0.38 ms per token,  2604.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     222.84 ms /   211 tokens (    1.06 ms per token,   946.88 tokens per second)\n",
      "llama_print_timings:        eval time =    3554.93 ms /   163 runs   (   21.81 ms per token,    45.85 tokens per second)\n",
      "llama_print_timings:       total time =    4415.68 ms /   374 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.81 ms /   116 runs   (    0.37 ms per token,  2709.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     256.77 ms /   268 tokens (    0.96 ms per token,  1043.74 tokens per second)\n",
      "llama_print_timings:        eval time =    2443.01 ms /   115 runs   (   21.24 ms per token,    47.07 tokens per second)\n",
      "llama_print_timings:       total time =    3130.28 ms /   383 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.61 ms /    92 runs   (    0.38 ms per token,  2658.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     210.72 ms /   207 tokens (    1.02 ms per token,   982.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1981.12 ms /    91 runs   (   21.77 ms per token,    45.93 tokens per second)\n",
      "llama_print_timings:       total time =    2552.02 ms /   298 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.70 ms /    21 runs   (    0.37 ms per token,  2728.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.47 ms /    66 tokens (    2.26 ms per token,   441.57 tokens per second)\n",
      "llama_print_timings:        eval time =     449.58 ms /    20 runs   (   22.48 ms per token,    44.49 tokens per second)\n",
      "llama_print_timings:       total time =     681.96 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.19 ms /     3 runs   (    0.40 ms per token,  2529.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.86 ms /   190 tokens (    1.13 ms per token,   888.45 tokens per second)\n",
      "llama_print_timings:        eval time =      42.99 ms /     2 runs   (   21.49 ms per token,    46.53 tokens per second)\n",
      "llama_print_timings:       total time =     269.71 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.78 ms /   126 runs   (    0.37 ms per token,  2693.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.37 ms /   111 tokens (    1.37 ms per token,   728.49 tokens per second)\n",
      "llama_print_timings:        eval time =    2661.86 ms /   125 runs   (   21.29 ms per token,    46.96 tokens per second)\n",
      "llama_print_timings:       total time =    3288.05 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.81 ms /   140 runs   (    0.38 ms per token,  2650.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.90 ms /    64 tokens (    1.97 ms per token,   508.33 tokens per second)\n",
      "llama_print_timings:        eval time =    2940.04 ms /   139 runs   (   21.15 ms per token,    47.28 tokens per second)\n",
      "llama_print_timings:       total time =    3605.71 ms /   203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.84 ms /    24 runs   (    0.37 ms per token,  2715.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.17 ms /    60 tokens (    2.20 ms per token,   453.97 tokens per second)\n",
      "llama_print_timings:        eval time =     512.31 ms /    23 runs   (   22.27 ms per token,    44.89 tokens per second)\n",
      "llama_print_timings:       total time =     737.77 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2622.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.25 ms /    57 tokens (    2.44 ms per token,   409.35 tokens per second)\n",
      "llama_print_timings:        eval time =      47.07 ms /     2 runs   (   23.54 ms per token,    42.49 tokens per second)\n",
      "llama_print_timings:       total time =     197.84 ms /    59 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      75.20 ms /   203 runs   (    0.37 ms per token,  2699.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     147.74 ms /    91 tokens (    1.62 ms per token,   615.95 tokens per second)\n",
      "llama_print_timings:        eval time =    4225.94 ms /   202 runs   (   20.92 ms per token,    47.80 tokens per second)\n",
      "llama_print_timings:       total time =    5166.05 ms /   293 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.25 ms /     3 runs   (    0.42 ms per token,  2400.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     198.86 ms /   162 tokens (    1.23 ms per token,   814.65 tokens per second)\n",
      "llama_print_timings:        eval time =      45.14 ms /     2 runs   (   22.57 ms per token,    44.31 tokens per second)\n",
      "llama_print_timings:       total time =     257.31 ms /   164 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.25 ms /   109 runs   (    0.38 ms per token,  2642.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     223.55 ms /   222 tokens (    1.01 ms per token,   993.08 tokens per second)\n",
      "llama_print_timings:        eval time =    2342.04 ms /   108 runs   (   21.69 ms per token,    46.11 tokens per second)\n",
      "llama_print_timings:       total time =    2982.70 ms /   330 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.15 ms /    74 runs   (    0.38 ms per token,  2629.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.69 ms /   234 tokens (    0.92 ms per token,  1089.96 tokens per second)\n",
      "llama_print_timings:        eval time =    1575.26 ms /    73 runs   (   21.58 ms per token,    46.34 tokens per second)\n",
      "llama_print_timings:       total time =    2065.40 ms /   307 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      44.32 ms /   118 runs   (    0.38 ms per token,  2662.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.77 ms /   117 tokens (    1.36 ms per token,   736.91 tokens per second)\n",
      "llama_print_timings:        eval time =    2466.01 ms /   117 runs   (   21.08 ms per token,    47.45 tokens per second)\n",
      "llama_print_timings:       total time =    3068.80 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      55.53 ms /   149 runs   (    0.37 ms per token,  2683.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.66 ms /    75 tokens (    2.00 ms per token,   501.15 tokens per second)\n",
      "llama_print_timings:        eval time =    3134.45 ms /   148 runs   (   21.18 ms per token,    47.22 tokens per second)\n",
      "llama_print_timings:       total time =    3862.38 ms /   223 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.74 ms /    73 runs   (    0.38 ms per token,  2631.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.35 ms /    73 tokens (    1.85 ms per token,   539.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1495.14 ms /    72 runs   (   20.77 ms per token,    48.16 tokens per second)\n",
      "llama_print_timings:       total time =    1902.76 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      96.64 ms /   256 runs   (    0.38 ms per token,  2649.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     294.41 ms /   345 tokens (    0.85 ms per token,  1171.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5462.31 ms /   255 runs   (   21.42 ms per token,    46.68 tokens per second)\n",
      "llama_print_timings:       total time =    6780.17 ms /   600 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.49 ms /   129 runs   (    0.38 ms per token,  2660.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     234.96 ms /   211 tokens (    1.11 ms per token,   898.04 tokens per second)\n",
      "llama_print_timings:        eval time =    2722.38 ms /   128 runs   (   21.27 ms per token,    47.02 tokens per second)\n",
      "llama_print_timings:       total time =    3474.62 ms /   339 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.59 ms /   114 runs   (    0.38 ms per token,  2615.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.04 ms /   174 tokens (    1.14 ms per token,   874.21 tokens per second)\n",
      "llama_print_timings:        eval time =    2433.39 ms /   113 runs   (   21.53 ms per token,    46.44 tokens per second)\n",
      "llama_print_timings:       total time =    3085.02 ms /   287 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.98 ms /   115 runs   (    0.38 ms per token,  2614.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     203.23 ms /   183 tokens (    1.11 ms per token,   900.48 tokens per second)\n",
      "llama_print_timings:        eval time =    2445.97 ms /   114 runs   (   21.46 ms per token,    46.61 tokens per second)\n",
      "llama_print_timings:       total time =    3094.32 ms /   297 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.86 ms /   117 runs   (    0.37 ms per token,  2667.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     189.59 ms /   181 tokens (    1.05 ms per token,   954.71 tokens per second)\n",
      "llama_print_timings:        eval time =    2459.32 ms /   116 runs   (   21.20 ms per token,    47.17 tokens per second)\n",
      "llama_print_timings:       total time =    3092.47 ms /   297 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.33 ms /    17 runs   (    0.37 ms per token,  2683.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.45 ms /    23 tokens (    7.32 ms per token,   136.54 tokens per second)\n",
      "llama_print_timings:        eval time =     325.54 ms /    16 runs   (   20.35 ms per token,    49.15 tokens per second)\n",
      "llama_print_timings:       total time =     560.46 ms /    39 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.80 ms /    75 runs   (    0.37 ms per token,  2698.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     167.80 ms /   120 tokens (    1.40 ms per token,   715.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1521.63 ms /    74 runs   (   20.56 ms per token,    48.63 tokens per second)\n",
      "llama_print_timings:       total time =    1971.39 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.62 ms /   116 runs   (    0.38 ms per token,  2659.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     534.79 ms /   576 tokens (    0.93 ms per token,  1077.06 tokens per second)\n",
      "llama_print_timings:        eval time =    2494.91 ms /   115 runs   (   21.69 ms per token,    46.09 tokens per second)\n",
      "llama_print_timings:       total time =    3469.61 ms /   691 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.16 ms /    87 runs   (    0.38 ms per token,  2623.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.23 ms /   103 tokens (    1.26 ms per token,   790.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1806.64 ms /    86 runs   (   21.01 ms per token,    47.60 tokens per second)\n",
      "llama_print_timings:       total time =    2264.15 ms /   189 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.00 ms /   110 runs   (    0.38 ms per token,  2618.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     200.71 ms /   195 tokens (    1.03 ms per token,   971.55 tokens per second)\n",
      "llama_print_timings:        eval time =    2318.52 ms /   109 runs   (   21.27 ms per token,    47.01 tokens per second)\n",
      "llama_print_timings:       total time =    2939.48 ms /   304 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.50 ms /    22 runs   (    0.39 ms per token,  2588.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     150.47 ms /   126 tokens (    1.19 ms per token,   837.35 tokens per second)\n",
      "llama_print_timings:        eval time =     442.12 ms /    21 runs   (   21.05 ms per token,    47.50 tokens per second)\n",
      "llama_print_timings:       total time =     677.48 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.27 ms /     3 runs   (    0.42 ms per token,  2367.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     183.21 ms /   138 tokens (    1.33 ms per token,   753.24 tokens per second)\n",
      "llama_print_timings:        eval time =      48.62 ms /     2 runs   (   24.31 ms per token,    41.13 tokens per second)\n",
      "llama_print_timings:       total time =     244.99 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      88.96 ms /   239 runs   (    0.37 ms per token,  2686.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     198.71 ms /   134 tokens (    1.48 ms per token,   674.36 tokens per second)\n",
      "llama_print_timings:        eval time =    5032.91 ms /   238 runs   (   21.15 ms per token,    47.29 tokens per second)\n",
      "llama_print_timings:       total time =    6174.19 ms /   372 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.34 ms /   109 runs   (    0.38 ms per token,  2636.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.10 ms /    70 tokens (    2.12 ms per token,   472.65 tokens per second)\n",
      "llama_print_timings:        eval time =    2277.23 ms /   108 runs   (   21.09 ms per token,    47.43 tokens per second)\n",
      "llama_print_timings:       total time =    2841.88 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.32 ms /    19 runs   (    0.39 ms per token,  2596.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.35 ms /   124 tokens (    1.22 ms per token,   819.28 tokens per second)\n",
      "llama_print_timings:        eval time =     397.74 ms /    18 runs   (   22.10 ms per token,    45.26 tokens per second)\n",
      "llama_print_timings:       total time =     626.97 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.33 ms /    23 runs   (    0.32 ms per token,  3139.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.07 ms /    80 tokens (    1.81 ms per token,   551.46 tokens per second)\n",
      "llama_print_timings:        eval time =     469.87 ms /    22 runs   (   21.36 ms per token,    46.82 tokens per second)\n",
      "llama_print_timings:       total time =     708.56 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      55.75 ms /   147 runs   (    0.38 ms per token,  2636.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     198.28 ms /   170 tokens (    1.17 ms per token,   857.36 tokens per second)\n",
      "llama_print_timings:        eval time =    3066.80 ms /   146 runs   (   21.01 ms per token,    47.61 tokens per second)\n",
      "llama_print_timings:       total time =    3836.30 ms /   316 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /     3 runs   (    0.38 ms per token,  2654.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.79 ms /    56 tokens (    2.37 ms per token,   421.71 tokens per second)\n",
      "llama_print_timings:        eval time =      48.39 ms /     2 runs   (   24.20 ms per token,    41.33 tokens per second)\n",
      "llama_print_timings:       total time =     193.48 ms /    58 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.93 ms /    18 runs   (    0.38 ms per token,  2598.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.18 ms /    72 tokens (    2.10 ms per token,   476.25 tokens per second)\n",
      "llama_print_timings:        eval time =     352.48 ms /    17 runs   (   20.73 ms per token,    48.23 tokens per second)\n",
      "llama_print_timings:       total time =     578.60 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.71 ms /    86 runs   (    0.38 ms per token,  2628.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     204.97 ms /   214 tokens (    0.96 ms per token,  1044.04 tokens per second)\n",
      "llama_print_timings:        eval time =    1805.80 ms /    85 runs   (   21.24 ms per token,    47.07 tokens per second)\n",
      "llama_print_timings:       total time =    2344.20 ms /   299 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /     3 runs   (    0.38 ms per token,  2650.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.93 ms /   147 tokens (    1.13 ms per token,   885.92 tokens per second)\n",
      "llama_print_timings:        eval time =      44.34 ms /     2 runs   (   22.17 ms per token,    45.11 tokens per second)\n",
      "llama_print_timings:       total time =     221.23 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.28 ms /   143 runs   (    0.37 ms per token,  2684.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.50 ms /    79 tokens (    1.78 ms per token,   562.29 tokens per second)\n",
      "llama_print_timings:        eval time =    3044.44 ms /   142 runs   (   21.44 ms per token,    46.64 tokens per second)\n",
      "llama_print_timings:       total time =    3732.47 ms /   221 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      91.76 ms /   254 runs   (    0.36 ms per token,  2768.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     929.07 ms /  1074 tokens (    0.87 ms per token,  1156.00 tokens per second)\n",
      "llama_print_timings:        eval time =    5754.86 ms /   253 runs   (   22.75 ms per token,    43.96 tokens per second)\n",
      "llama_print_timings:       total time =    7686.66 ms /  1327 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.34 ms /    25 runs   (    0.37 ms per token,  2677.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     276.87 ms /   266 tokens (    1.04 ms per token,   960.75 tokens per second)\n",
      "llama_print_timings:        eval time =     508.60 ms /    24 runs   (   21.19 ms per token,    47.19 tokens per second)\n",
      "llama_print_timings:       total time =     880.51 ms /   290 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.28 ms /    21 runs   (    0.39 ms per token,  2536.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     179.00 ms /   155 tokens (    1.15 ms per token,   865.94 tokens per second)\n",
      "llama_print_timings:        eval time =     446.07 ms /    20 runs   (   22.30 ms per token,    44.84 tokens per second)\n",
      "llama_print_timings:       total time =     708.75 ms /   175 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      67.20 ms /   191 runs   (    0.35 ms per token,  2842.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     307.36 ms /   297 tokens (    1.03 ms per token,   966.30 tokens per second)\n",
      "llama_print_timings:        eval time =    4145.51 ms /   190 runs   (   21.82 ms per token,    45.83 tokens per second)\n",
      "llama_print_timings:       total time =    5179.88 ms /   487 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.58 ms /   108 runs   (    0.38 ms per token,  2597.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     193.42 ms /   157 tokens (    1.23 ms per token,   811.69 tokens per second)\n",
      "llama_print_timings:        eval time =    2316.61 ms /   107 runs   (   21.65 ms per token,    46.19 tokens per second)\n",
      "llama_print_timings:       total time =    2948.92 ms /   264 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      54.14 ms /   146 runs   (    0.37 ms per token,  2696.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     345.22 ms /   399 tokens (    0.87 ms per token,  1155.77 tokens per second)\n",
      "llama_print_timings:        eval time =    3120.55 ms /   145 runs   (   21.52 ms per token,    46.47 tokens per second)\n",
      "llama_print_timings:       total time =    4022.17 ms /   544 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      96.67 ms /   256 runs   (    0.38 ms per token,  2648.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     563.05 ms /   580 tokens (    0.97 ms per token,  1030.10 tokens per second)\n",
      "llama_print_timings:        eval time =    5716.74 ms /   255 runs   (   22.42 ms per token,    44.61 tokens per second)\n",
      "llama_print_timings:       total time =    7293.23 ms /   835 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.39 ms /    74 runs   (    0.38 ms per token,  2607.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.60 ms /   182 tokens (    1.15 ms per token,   868.31 tokens per second)\n",
      "llama_print_timings:        eval time =    1648.01 ms /    73 runs   (   22.58 ms per token,    44.30 tokens per second)\n",
      "llama_print_timings:       total time =    2153.57 ms /   255 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      97.41 ms /   256 runs   (    0.38 ms per token,  2628.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     223.31 ms /   248 tokens (    0.90 ms per token,  1110.55 tokens per second)\n",
      "llama_print_timings:        eval time =    5513.26 ms /   255 runs   (   21.62 ms per token,    46.25 tokens per second)\n",
      "llama_print_timings:       total time =    6797.83 ms /   503 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.17 ms /    21 runs   (    0.39 ms per token,  2571.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.00 ms /   126 tokens (    1.14 ms per token,   875.02 tokens per second)\n",
      "llama_print_timings:        eval time =     433.51 ms /    20 runs   (   21.68 ms per token,    46.14 tokens per second)\n",
      "llama_print_timings:       total time =     661.69 ms /   146 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.83 ms /   130 runs   (    0.38 ms per token,  2609.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     188.61 ms /   225 tokens (    0.84 ms per token,  1192.91 tokens per second)\n",
      "llama_print_timings:        eval time =    2875.94 ms /   129 runs   (   22.29 ms per token,    44.85 tokens per second)\n",
      "llama_print_timings:       total time =    3570.46 ms /   354 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.17 ms /     3 runs   (    0.39 ms per token,  2572.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.68 ms /    81 tokens (    1.74 ms per token,   575.78 tokens per second)\n",
      "llama_print_timings:        eval time =      45.22 ms /     2 runs   (   22.61 ms per token,    44.23 tokens per second)\n",
      "llama_print_timings:       total time =     196.88 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.70 ms /    83 runs   (    0.38 ms per token,  2617.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     604.61 ms /   665 tokens (    0.91 ms per token,  1099.88 tokens per second)\n",
      "llama_print_timings:        eval time =    1830.48 ms /    82 runs   (   22.32 ms per token,    44.80 tokens per second)\n",
      "llama_print_timings:       total time =    2747.45 ms /   747 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.82 ms /    18 runs   (    0.38 ms per token,  2639.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     137.64 ms /    87 tokens (    1.58 ms per token,   632.10 tokens per second)\n",
      "llama_print_timings:        eval time =     372.71 ms /    17 runs   (   21.92 ms per token,    45.61 tokens per second)\n",
      "llama_print_timings:       total time =     581.50 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.26 ms /   115 runs   (    0.37 ms per token,  2721.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.08 ms /   202 tokens (    1.08 ms per token,   922.02 tokens per second)\n",
      "llama_print_timings:        eval time =    2470.37 ms /   114 runs   (   21.67 ms per token,    46.15 tokens per second)\n",
      "llama_print_timings:       total time =    3128.69 ms /   316 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.17 ms /    22 runs   (    0.37 ms per token,  2691.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     142.63 ms /    52 tokens (    2.74 ms per token,   364.57 tokens per second)\n",
      "llama_print_timings:        eval time =     468.25 ms /    21 runs   (   22.30 ms per token,    44.85 tokens per second)\n",
      "llama_print_timings:       total time =     694.79 ms /    73 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.54 ms /    19 runs   (    0.40 ms per token,  2519.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.23 ms /    67 tokens (    2.21 ms per token,   451.99 tokens per second)\n",
      "llama_print_timings:        eval time =     381.93 ms /    18 runs   (   21.22 ms per token,    47.13 tokens per second)\n",
      "llama_print_timings:       total time =     612.24 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      66.37 ms /   174 runs   (    0.38 ms per token,  2621.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     191.57 ms /   247 tokens (    0.78 ms per token,  1289.33 tokens per second)\n",
      "llama_print_timings:        eval time =    3679.76 ms /   173 runs   (   21.27 ms per token,    47.01 tokens per second)\n",
      "llama_print_timings:       total time =    4541.85 ms /   420 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.94 ms /    18 runs   (    0.39 ms per token,  2592.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     193.81 ms /   182 tokens (    1.06 ms per token,   939.05 tokens per second)\n",
      "llama_print_timings:        eval time =     364.27 ms /    17 runs   (   21.43 ms per token,    46.67 tokens per second)\n",
      "llama_print_timings:       total time =     626.61 ms /   199 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.82 ms /    18 runs   (    0.38 ms per token,  2640.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     195.91 ms /   133 tokens (    1.47 ms per token,   678.87 tokens per second)\n",
      "llama_print_timings:        eval time =     381.66 ms /    17 runs   (   22.45 ms per token,    44.54 tokens per second)\n",
      "llama_print_timings:       total time =     645.78 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.70 ms /    17 runs   (    0.39 ms per token,  2535.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.25 ms /   104 tokens (    1.46 ms per token,   683.09 tokens per second)\n",
      "llama_print_timings:        eval time =     374.36 ms /    16 runs   (   23.40 ms per token,    42.74 tokens per second)\n",
      "llama_print_timings:       total time =     596.28 ms /   120 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.39 ms /   120 runs   (    0.38 ms per token,  2643.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     188.47 ms /   143 tokens (    1.32 ms per token,   758.74 tokens per second)\n",
      "llama_print_timings:        eval time =    2549.26 ms /   119 runs   (   21.42 ms per token,    46.68 tokens per second)\n",
      "llama_print_timings:       total time =    3195.00 ms /   262 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      53.04 ms /   142 runs   (    0.37 ms per token,  2677.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.91 ms /   125 tokens (    1.17 ms per token,   856.68 tokens per second)\n",
      "llama_print_timings:        eval time =    3053.74 ms /   141 runs   (   21.66 ms per token,    46.17 tokens per second)\n",
      "llama_print_timings:       total time =    3747.41 ms /   266 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      58.93 ms /   156 runs   (    0.38 ms per token,  2647.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     137.05 ms /   100 tokens (    1.37 ms per token,   729.66 tokens per second)\n",
      "llama_print_timings:        eval time =    3455.90 ms /   155 runs   (   22.30 ms per token,    44.85 tokens per second)\n",
      "llama_print_timings:       total time =    4226.66 ms /   255 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.94 ms /   130 runs   (    0.38 ms per token,  2656.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.71 ms /   183 tokens (    1.15 ms per token,   872.63 tokens per second)\n",
      "llama_print_timings:        eval time =    2716.71 ms /   129 runs   (   21.06 ms per token,    47.48 tokens per second)\n",
      "llama_print_timings:       total time =    3433.21 ms /   312 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.31 ms /    92 runs   (    0.37 ms per token,  2681.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.60 ms /    56 tokens (    2.21 ms per token,   453.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1951.48 ms /    91 runs   (   21.44 ms per token,    46.63 tokens per second)\n",
      "llama_print_timings:       total time =    2432.72 ms /   147 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.71 ms /    22 runs   (    0.40 ms per token,  2526.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     143.89 ms /    92 tokens (    1.56 ms per token,   639.40 tokens per second)\n",
      "llama_print_timings:        eval time =     449.17 ms /    21 runs   (   21.39 ms per token,    46.75 tokens per second)\n",
      "llama_print_timings:       total time =     685.39 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     3 runs   (    0.39 ms per token,  2586.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.81 ms /    91 tokens (    1.47 ms per token,   680.06 tokens per second)\n",
      "llama_print_timings:        eval time =      45.02 ms /     2 runs   (   22.51 ms per token,    44.42 tokens per second)\n",
      "llama_print_timings:       total time =     190.60 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      31.97 ms /    88 runs   (    0.36 ms per token,  2752.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     271.62 ms /   357 tokens (    0.76 ms per token,  1314.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1822.06 ms /    87 runs   (   20.94 ms per token,    47.75 tokens per second)\n",
      "llama_print_timings:       total time =    2422.97 ms /   444 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      42.16 ms /   113 runs   (    0.37 ms per token,  2679.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     188.69 ms /   153 tokens (    1.23 ms per token,   810.84 tokens per second)\n",
      "llama_print_timings:        eval time =    2420.99 ms /   112 runs   (   21.62 ms per token,    46.26 tokens per second)\n",
      "llama_print_timings:       total time =    3045.54 ms /   265 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.96 ms /   107 runs   (    0.38 ms per token,  2612.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     178.54 ms /   148 tokens (    1.21 ms per token,   828.94 tokens per second)\n",
      "llama_print_timings:        eval time =    2431.28 ms /   106 runs   (   22.94 ms per token,    43.60 tokens per second)\n",
      "llama_print_timings:       total time =    3030.38 ms /   254 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      46.58 ms /   125 runs   (    0.37 ms per token,  2683.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     305.42 ms /   342 tokens (    0.89 ms per token,  1119.78 tokens per second)\n",
      "llama_print_timings:        eval time =    2690.83 ms /   124 runs   (   21.70 ms per token,    46.08 tokens per second)\n",
      "llama_print_timings:       total time =    3486.86 ms /   466 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.51 ms /   132 runs   (    0.38 ms per token,  2666.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.76 ms /   335 tokens (    0.80 ms per token,  1251.10 tokens per second)\n",
      "llama_print_timings:        eval time =    2789.14 ms /   131 runs   (   21.29 ms per token,    46.97 tokens per second)\n",
      "llama_print_timings:       total time =    3554.46 ms /   466 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.63 ms /    21 runs   (    0.36 ms per token,  2752.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.07 ms /    90 tokens (    1.69 ms per token,   591.82 tokens per second)\n",
      "llama_print_timings:        eval time =     470.99 ms /    20 runs   (   23.55 ms per token,    42.46 tokens per second)\n",
      "llama_print_timings:       total time =     707.93 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.65 ms /    17 runs   (    0.39 ms per token,  2557.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.28 ms /    86 tokens (    1.74 ms per token,   576.11 tokens per second)\n",
      "llama_print_timings:        eval time =     356.31 ms /    16 runs   (   22.27 ms per token,    44.91 tokens per second)\n",
      "llama_print_timings:       total time =     571.17 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      50.31 ms /   137 runs   (    0.37 ms per token,  2723.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     228.84 ms /   222 tokens (    1.03 ms per token,   970.13 tokens per second)\n",
      "llama_print_timings:        eval time =    2914.89 ms /   136 runs   (   21.43 ms per token,    46.66 tokens per second)\n",
      "llama_print_timings:       total time =    3665.01 ms /   358 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.15 ms /   129 runs   (    0.37 ms per token,  2679.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     261.79 ms /   273 tokens (    0.96 ms per token,  1042.82 tokens per second)\n",
      "llama_print_timings:        eval time =    2761.66 ms /   128 runs   (   21.58 ms per token,    46.35 tokens per second)\n",
      "llama_print_timings:       total time =    3514.64 ms /   401 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.74 ms /    26 runs   (    0.37 ms per token,  2669.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.38 ms /   124 tokens (    1.22 ms per token,   819.12 tokens per second)\n",
      "llama_print_timings:        eval time =     551.72 ms /    25 runs   (   22.07 ms per token,    45.31 tokens per second)\n",
      "llama_print_timings:       total time =     803.36 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      45.62 ms /   126 runs   (    0.36 ms per token,  2762.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     180.90 ms /   138 tokens (    1.31 ms per token,   762.85 tokens per second)\n",
      "llama_print_timings:        eval time =    2734.15 ms /   125 runs   (   21.87 ms per token,    45.72 tokens per second)\n",
      "llama_print_timings:       total time =    3420.58 ms /   263 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      64.21 ms /   174 runs   (    0.37 ms per token,  2709.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     260.29 ms /   316 tokens (    0.82 ms per token,  1214.02 tokens per second)\n",
      "llama_print_timings:        eval time =    3638.47 ms /   173 runs   (   21.03 ms per token,    47.55 tokens per second)\n",
      "llama_print_timings:       total time =    4568.97 ms /   489 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.54 ms /   105 runs   (    0.39 ms per token,  2589.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     184.80 ms /   135 tokens (    1.37 ms per token,   730.52 tokens per second)\n",
      "llama_print_timings:        eval time =    2215.95 ms /   104 runs   (   21.31 ms per token,    46.93 tokens per second)\n",
      "llama_print_timings:       total time =    2806.26 ms /   239 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.12 ms /    21 runs   (    0.39 ms per token,  2584.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.98 ms /   120 tokens (    1.37 ms per token,   731.80 tokens per second)\n",
      "llama_print_timings:        eval time =     432.17 ms /    20 runs   (   21.61 ms per token,    46.28 tokens per second)\n",
      "llama_print_timings:       total time =     677.75 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.09 ms /    92 runs   (    0.37 ms per token,  2698.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.05 ms /    58 tokens (    2.41 ms per token,   414.13 tokens per second)\n",
      "llama_print_timings:        eval time =    1851.04 ms /    91 runs   (   20.34 ms per token,    49.16 tokens per second)\n",
      "llama_print_timings:       total time =    2341.63 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.12 ms /    89 runs   (    0.37 ms per token,  2686.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.85 ms /    41 tokens (    3.05 ms per token,   328.38 tokens per second)\n",
      "llama_print_timings:        eval time =    1884.76 ms /    88 runs   (   21.42 ms per token,    46.69 tokens per second)\n",
      "llama_print_timings:       total time =    2344.00 ms /   129 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      29.54 ms /    78 runs   (    0.38 ms per token,  2640.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.22 ms /    89 tokens (    1.77 ms per token,   566.10 tokens per second)\n",
      "llama_print_timings:        eval time =    1628.83 ms /    77 runs   (   21.15 ms per token,    47.27 tokens per second)\n",
      "llama_print_timings:       total time =    2081.49 ms /   166 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.82 ms /    95 runs   (    0.38 ms per token,  2651.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     223.14 ms /   244 tokens (    0.91 ms per token,  1093.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1985.64 ms /    94 runs   (   21.12 ms per token,    47.34 tokens per second)\n",
      "llama_print_timings:       total time =    2575.48 ms /   338 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.22 ms /   109 runs   (    0.37 ms per token,  2710.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     208.72 ms /   163 tokens (    1.28 ms per token,   780.96 tokens per second)\n",
      "llama_print_timings:        eval time =    2296.66 ms /   108 runs   (   21.27 ms per token,    47.02 tokens per second)\n",
      "llama_print_timings:       total time =    2916.14 ms /   271 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      73.05 ms /   192 runs   (    0.38 ms per token,  2628.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     206.22 ms /   144 tokens (    1.43 ms per token,   698.28 tokens per second)\n",
      "llama_print_timings:        eval time =    4174.83 ms /   191 runs   (   21.86 ms per token,    45.75 tokens per second)\n",
      "llama_print_timings:       total time =    5158.85 ms /   335 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.49 ms /    22 runs   (    0.39 ms per token,  2591.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     184.89 ms /   183 tokens (    1.01 ms per token,   989.78 tokens per second)\n",
      "llama_print_timings:        eval time =     447.17 ms /    21 runs   (   21.29 ms per token,    46.96 tokens per second)\n",
      "llama_print_timings:       total time =     729.74 ms /   204 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      58.07 ms /   152 runs   (    0.38 ms per token,  2617.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.25 ms /   210 tokens (    1.00 ms per token,  1003.59 tokens per second)\n",
      "llama_print_timings:        eval time =    3245.40 ms /   151 runs   (   21.49 ms per token,    46.53 tokens per second)\n",
      "llama_print_timings:       total time =    4058.56 ms /   361 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.36 ms /    17 runs   (    0.37 ms per token,  2671.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.71 ms /   105 tokens (    1.48 ms per token,   674.32 tokens per second)\n",
      "llama_print_timings:        eval time =     356.19 ms /    16 runs   (   22.26 ms per token,    44.92 tokens per second)\n",
      "llama_print_timings:       total time =     576.14 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      43.66 ms /   116 runs   (    0.38 ms per token,  2656.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     230.01 ms /   225 tokens (    1.02 ms per token,   978.21 tokens per second)\n",
      "llama_print_timings:        eval time =    2472.61 ms /   115 runs   (   21.50 ms per token,    46.51 tokens per second)\n",
      "llama_print_timings:       total time =    3146.10 ms /   340 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.59 ms /    22 runs   (    0.39 ms per token,  2562.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.51 ms /    88 tokens (    1.57 ms per token,   635.33 tokens per second)\n",
      "llama_print_timings:        eval time =     459.53 ms /    21 runs   (   21.88 ms per token,    45.70 tokens per second)\n",
      "llama_print_timings:       total time =     681.54 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.56 ms /    22 runs   (    0.39 ms per token,  2570.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.56 ms /   124 tokens (    1.25 ms per token,   802.27 tokens per second)\n",
      "llama_print_timings:        eval time =     451.31 ms /    21 runs   (   21.49 ms per token,    46.53 tokens per second)\n",
      "llama_print_timings:       total time =     690.35 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      29.37 ms /    80 runs   (    0.37 ms per token,  2723.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     203.56 ms /   157 tokens (    1.30 ms per token,   771.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1604.62 ms /    79 runs   (   20.31 ms per token,    49.23 tokens per second)\n",
      "llama_print_timings:       total time =    2107.06 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /     3 runs   (    0.39 ms per token,  2533.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.80 ms /    57 tokens (    2.15 ms per token,   464.17 tokens per second)\n",
      "llama_print_timings:        eval time =      49.14 ms /     2 runs   (   24.57 ms per token,    40.70 tokens per second)\n",
      "llama_print_timings:       total time =     184.71 ms /    59 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      88.53 ms /   236 runs   (    0.38 ms per token,  2665.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.85 ms /   228 tokens (    0.94 ms per token,  1066.16 tokens per second)\n",
      "llama_print_timings:        eval time =    5088.26 ms /   235 runs   (   21.65 ms per token,    46.18 tokens per second)\n",
      "llama_print_timings:       total time =    6241.87 ms /   463 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      35.10 ms /    93 runs   (    0.38 ms per token,  2649.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.62 ms /    34 tokens (    3.90 ms per token,   256.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1980.58 ms /    92 runs   (   21.53 ms per token,    46.45 tokens per second)\n",
      "llama_print_timings:       total time =    2468.87 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.53 ms /    22 runs   (    0.39 ms per token,  2579.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     157.14 ms /   127 tokens (    1.24 ms per token,   808.21 tokens per second)\n",
      "llama_print_timings:        eval time =     459.79 ms /    21 runs   (   21.89 ms per token,    45.67 tokens per second)\n",
      "llama_print_timings:       total time =     702.37 ms /   148 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.06 ms /    17 runs   (    0.42 ms per token,  2407.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     205.21 ms /   161 tokens (    1.27 ms per token,   784.55 tokens per second)\n",
      "llama_print_timings:        eval time =     352.38 ms /    16 runs   (   22.02 ms per token,    45.41 tokens per second)\n",
      "llama_print_timings:       total time =     632.88 ms /   177 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.26 ms /    18 runs   (    0.40 ms per token,  2479.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     197.25 ms /   163 tokens (    1.21 ms per token,   826.35 tokens per second)\n",
      "llama_print_timings:        eval time =     390.60 ms /    17 runs   (   22.98 ms per token,    43.52 tokens per second)\n",
      "llama_print_timings:       total time =     670.14 ms /   180 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2629.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.39 ms /    74 tokens (    1.95 ms per token,   512.51 tokens per second)\n",
      "llama_print_timings:        eval time =      42.49 ms /     2 runs   (   21.24 ms per token,    47.07 tokens per second)\n",
      "llama_print_timings:       total time =     199.59 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.75 ms /    18 runs   (    0.37 ms per token,  2668.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     248.33 ms /    31 tokens (    8.01 ms per token,   124.83 tokens per second)\n",
      "llama_print_timings:        eval time =     374.66 ms /    17 runs   (   22.04 ms per token,    45.37 tokens per second)\n",
      "llama_print_timings:       total time =     692.64 ms /    48 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      65.01 ms /   175 runs   (    0.37 ms per token,  2691.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     198.04 ms /   187 tokens (    1.06 ms per token,   944.25 tokens per second)\n",
      "llama_print_timings:        eval time =    3727.93 ms /   174 runs   (   21.42 ms per token,    46.67 tokens per second)\n",
      "llama_print_timings:       total time =    4604.08 ms /   361 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      52.09 ms /   139 runs   (    0.37 ms per token,  2668.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.79 ms /   121 tokens (    1.37 ms per token,   729.83 tokens per second)\n",
      "llama_print_timings:        eval time =    2906.30 ms /   138 runs   (   21.06 ms per token,    47.48 tokens per second)\n",
      "llama_print_timings:       total time =    3602.44 ms /   259 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.64 ms /    22 runs   (    0.39 ms per token,  2547.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.21 ms /    73 tokens (    1.84 ms per token,   543.92 tokens per second)\n",
      "llama_print_timings:        eval time =     458.29 ms /    21 runs   (   21.82 ms per token,    45.82 tokens per second)\n",
      "llama_print_timings:       total time =     681.79 ms /    94 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       6.78 ms /    17 runs   (    0.40 ms per token,  2507.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.88 ms /   170 tokens (    1.18 ms per token,   850.50 tokens per second)\n",
      "llama_print_timings:        eval time =     394.15 ms /    16 runs   (   24.63 ms per token,    40.59 tokens per second)\n",
      "llama_print_timings:       total time =     662.44 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      27.58 ms /    71 runs   (    0.39 ms per token,  2574.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.75 ms /    63 tokens (    2.08 ms per token,   481.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1553.66 ms /    70 runs   (   22.20 ms per token,    45.05 tokens per second)\n",
      "llama_print_timings:       total time =    1963.27 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      34.65 ms /    95 runs   (    0.36 ms per token,  2741.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     220.79 ms /    30 tokens (    7.36 ms per token,   135.88 tokens per second)\n",
      "llama_print_timings:        eval time =    2027.85 ms /    94 runs   (   21.57 ms per token,    46.35 tokens per second)\n",
      "llama_print_timings:       total time =    2607.65 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.30 ms /    88 runs   (    0.37 ms per token,  2724.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.33 ms /   108 tokens (    1.45 ms per token,   690.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1838.70 ms /    87 runs   (   21.13 ms per token,    47.32 tokens per second)\n",
      "llama_print_timings:       total time =    2343.09 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      10.82 ms /    28 runs   (    0.39 ms per token,  2587.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.21 ms /   177 tokens (    1.22 ms per token,   818.66 tokens per second)\n",
      "llama_print_timings:        eval time =     622.44 ms /    27 runs   (   23.05 ms per token,    43.38 tokens per second)\n",
      "llama_print_timings:       total time =     943.51 ms /   204 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.28 ms /    83 runs   (    0.39 ms per token,  2571.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.67 ms /    59 tokens (    2.47 ms per token,   405.02 tokens per second)\n",
      "llama_print_timings:        eval time =    1801.96 ms /    82 runs   (   21.98 ms per token,    45.51 tokens per second)\n",
      "llama_print_timings:       total time =    2291.69 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      40.05 ms /   110 runs   (    0.36 ms per token,  2746.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     212.86 ms /   168 tokens (    1.27 ms per token,   789.26 tokens per second)\n",
      "llama_print_timings:        eval time =    2395.74 ms /   109 runs   (   21.98 ms per token,    45.50 tokens per second)\n",
      "llama_print_timings:       total time =    3037.34 ms /   277 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      39.70 ms /   105 runs   (    0.38 ms per token,  2644.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     137.00 ms /    79 tokens (    1.73 ms per token,   576.62 tokens per second)\n",
      "llama_print_timings:        eval time =    2186.67 ms /   104 runs   (   21.03 ms per token,    47.56 tokens per second)\n",
      "llama_print_timings:       total time =    2744.58 ms /   183 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      28.61 ms /    76 runs   (    0.38 ms per token,  2656.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.69 ms /   134 tokens (    1.36 ms per token,   737.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1507.79 ms /    75 runs   (   20.10 ms per token,    49.74 tokens per second)\n",
      "llama_print_timings:       total time =    1975.32 ms /   209 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.38 ms /   108 runs   (    0.38 ms per token,  2610.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     271.51 ms /   257 tokens (    1.06 ms per token,   946.56 tokens per second)\n",
      "llama_print_timings:        eval time =    2327.34 ms /   107 runs   (   21.75 ms per token,    45.98 tokens per second)\n",
      "llama_print_timings:       total time =    3020.02 ms /   364 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.80 ms /    22 runs   (    0.40 ms per token,  2500.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.68 ms /   119 tokens (    1.37 ms per token,   731.50 tokens per second)\n",
      "llama_print_timings:        eval time =     434.04 ms /    21 runs   (   20.67 ms per token,    48.38 tokens per second)\n",
      "llama_print_timings:       total time =     692.57 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      32.29 ms /    84 runs   (    0.38 ms per token,  2601.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.58 ms /   123 tokens (    1.15 ms per token,   868.74 tokens per second)\n",
      "llama_print_timings:        eval time =    1797.41 ms /    83 runs   (   21.66 ms per token,    46.18 tokens per second)\n",
      "llama_print_timings:       total time =    2283.36 ms /   206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      13.40 ms /    34 runs   (    0.39 ms per token,  2537.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     156.97 ms /   125 tokens (    1.26 ms per token,   796.34 tokens per second)\n",
      "llama_print_timings:        eval time =     727.81 ms /    33 runs   (   22.05 ms per token,    45.34 tokens per second)\n",
      "llama_print_timings:       total time =    1024.37 ms /   158 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      73.67 ms /   198 runs   (    0.37 ms per token,  2687.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     403.41 ms /   456 tokens (    0.88 ms per token,  1130.37 tokens per second)\n",
      "llama_print_timings:        eval time =    5042.48 ms /   197 runs   (   25.60 ms per token,    39.07 tokens per second)\n",
      "llama_print_timings:       total time =    6240.18 ms /   653 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      41.14 ms /   111 runs   (    0.37 ms per token,  2697.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.51 ms /   107 tokens (    1.88 ms per token,   530.99 tokens per second)\n",
      "llama_print_timings:        eval time =    3076.15 ms /   110 runs   (   27.96 ms per token,    35.76 tokens per second)\n",
      "llama_print_timings:       total time =    3718.68 ms /   217 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      54.94 ms /   146 runs   (    0.38 ms per token,  2657.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     443.10 ms /   375 tokens (    1.18 ms per token,   846.31 tokens per second)\n",
      "llama_print_timings:        eval time =    3265.43 ms /   145 runs   (   22.52 ms per token,    44.40 tokens per second)\n",
      "llama_print_timings:       total time =    4289.13 ms /   520 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      76.96 ms /   207 runs   (    0.37 ms per token,  2689.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     301.11 ms /   349 tokens (    0.86 ms per token,  1159.03 tokens per second)\n",
      "llama_print_timings:        eval time =    4631.53 ms /   206 runs   (   22.48 ms per token,    44.48 tokens per second)\n",
      "llama_print_timings:       total time =    5754.26 ms /   555 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      33.10 ms /    86 runs   (    0.38 ms per token,  2597.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     194.27 ms /   167 tokens (    1.16 ms per token,   859.65 tokens per second)\n",
      "llama_print_timings:        eval time =    1817.53 ms /    85 runs   (   21.38 ms per token,    46.77 tokens per second)\n",
      "llama_print_timings:       total time =    2339.09 ms /   252 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.74 ms /    21 runs   (    0.42 ms per token,  2402.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     150.03 ms /   104 tokens (    1.44 ms per token,   693.21 tokens per second)\n",
      "llama_print_timings:        eval time =     456.35 ms /    20 runs   (   22.82 ms per token,    43.83 tokens per second)\n",
      "llama_print_timings:       total time =     694.87 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.42 ms /   103 runs   (    0.37 ms per token,  2680.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.22 ms /   235 tokens (    0.86 ms per token,  1167.90 tokens per second)\n",
      "llama_print_timings:        eval time =    2202.37 ms /   102 runs   (   21.59 ms per token,    46.31 tokens per second)\n",
      "llama_print_timings:       total time =    2809.24 ms /   337 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.23 ms /    79 runs   (    0.38 ms per token,  2613.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     189.86 ms /   135 tokens (    1.41 ms per token,   711.04 tokens per second)\n",
      "llama_print_timings:        eval time =    1690.94 ms /    78 runs   (   21.68 ms per token,    46.13 tokens per second)\n",
      "llama_print_timings:       total time =    2177.14 ms /   213 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.74 ms /   130 runs   (    0.38 ms per token,  2613.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.17 ms /   112 tokens (    1.32 ms per token,   755.90 tokens per second)\n",
      "llama_print_timings:        eval time =    2717.50 ms /   129 runs   (   21.07 ms per token,    47.47 tokens per second)\n",
      "llama_print_timings:       total time =    3394.83 ms /   241 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      30.81 ms /    87 runs   (    0.35 ms per token,  2823.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.07 ms /   110 tokens (    1.21 ms per token,   826.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1784.77 ms /    86 runs   (   20.75 ms per token,    48.19 tokens per second)\n",
      "llama_print_timings:       total time =    2257.11 ms /   196 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.01 ms /    18 runs   (    0.39 ms per token,  2568.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.20 ms /    76 tokens (    1.82 ms per token,   549.92 tokens per second)\n",
      "llama_print_timings:        eval time =     368.77 ms /    17 runs   (   21.69 ms per token,    46.10 tokens per second)\n",
      "llama_print_timings:       total time =     579.56 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      48.14 ms /   124 runs   (    0.39 ms per token,  2575.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.09 ms /   199 tokens (    1.08 ms per token,   929.50 tokens per second)\n",
      "llama_print_timings:        eval time =    2622.67 ms /   123 runs   (   21.32 ms per token,    46.90 tokens per second)\n",
      "llama_print_timings:       total time =    3323.12 ms /   322 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.14 ms /     3 runs   (    0.38 ms per token,  2629.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     122.87 ms /    54 tokens (    2.28 ms per token,   439.48 tokens per second)\n",
      "llama_print_timings:        eval time =      46.86 ms /     2 runs   (   23.43 ms per token,    42.68 tokens per second)\n",
      "llama_print_timings:       total time =     181.59 ms /    56 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      36.52 ms /    98 runs   (    0.37 ms per token,  2683.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     287.63 ms /   267 tokens (    1.08 ms per token,   928.27 tokens per second)\n",
      "llama_print_timings:        eval time =    2100.23 ms /    97 runs   (   21.65 ms per token,    46.19 tokens per second)\n",
      "llama_print_timings:       total time =    2754.28 ms /   364 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.36 ms /    24 runs   (    0.39 ms per token,  2565.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     244.11 ms /   302 tokens (    0.81 ms per token,  1237.14 tokens per second)\n",
      "llama_print_timings:        eval time =     489.03 ms /    23 runs   (   21.26 ms per token,    47.03 tokens per second)\n",
      "llama_print_timings:       total time =     828.62 ms /   325 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.08 ms /    21 runs   (    0.38 ms per token,  2600.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     153.03 ms /    75 tokens (    2.04 ms per token,   490.09 tokens per second)\n",
      "llama_print_timings:        eval time =     462.07 ms /    20 runs   (   23.10 ms per token,    43.28 tokens per second)\n",
      "llama_print_timings:       total time =     692.11 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.29 ms /    22 runs   (    0.33 ms per token,  3019.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     148.37 ms /    94 tokens (    1.58 ms per token,   633.55 tokens per second)\n",
      "llama_print_timings:        eval time =     481.36 ms /    21 runs   (   22.92 ms per token,    43.63 tokens per second)\n",
      "llama_print_timings:       total time =     718.70 ms /   115 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       7.24 ms /    18 runs   (    0.40 ms per token,  2484.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.85 ms /    54 tokens (    2.35 ms per token,   425.71 tokens per second)\n",
      "llama_print_timings:        eval time =     381.75 ms /    17 runs   (   22.46 ms per token,    44.53 tokens per second)\n",
      "llama_print_timings:       total time =     596.38 ms /    71 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      80.85 ms /   217 runs   (    0.37 ms per token,  2684.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     195.17 ms /   151 tokens (    1.29 ms per token,   773.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4661.76 ms /   216 runs   (   21.58 ms per token,    46.33 tokens per second)\n",
      "llama_print_timings:       total time =    5718.65 ms /   367 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.23 ms /   101 runs   (    0.38 ms per token,  2642.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.26 ms /   143 tokens (    1.18 ms per token,   849.87 tokens per second)\n",
      "llama_print_timings:        eval time =    2269.60 ms /   100 runs   (   22.70 ms per token,    44.06 tokens per second)\n",
      "llama_print_timings:       total time =    2851.64 ms /   243 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       1.19 ms /     3 runs   (    0.40 ms per token,  2525.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.43 ms /   108 tokens (    1.49 ms per token,   669.02 tokens per second)\n",
      "llama_print_timings:        eval time =      47.22 ms /     2 runs   (   23.61 ms per token,    42.36 tokens per second)\n",
      "llama_print_timings:       total time =     223.11 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      19.07 ms /    49 runs   (    0.39 ms per token,  2569.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     203.41 ms /   165 tokens (    1.23 ms per token,   811.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1071.44 ms /    48 runs   (   22.32 ms per token,    44.80 tokens per second)\n",
      "llama_print_timings:       total time =    1458.96 ms /   213 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      29.11 ms /    77 runs   (    0.38 ms per token,  2645.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.31 ms /    63 tokens (    1.93 ms per token,   519.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1650.64 ms /    76 runs   (   21.72 ms per token,    46.04 tokens per second)\n",
      "llama_print_timings:       total time =    2071.16 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      57.44 ms /   151 runs   (    0.38 ms per token,  2629.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     149.18 ms /    97 tokens (    1.54 ms per token,   650.23 tokens per second)\n",
      "llama_print_timings:        eval time =    3304.61 ms /   150 runs   (   22.03 ms per token,    45.39 tokens per second)\n",
      "llama_print_timings:       total time =    4055.29 ms /   247 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      38.21 ms /   103 runs   (    0.37 ms per token,  2695.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     180.04 ms /   149 tokens (    1.21 ms per token,   827.58 tokens per second)\n",
      "llama_print_timings:        eval time =    2168.16 ms /   102 runs   (   21.26 ms per token,    47.04 tokens per second)\n",
      "llama_print_timings:       total time =    2742.47 ms /   251 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      49.86 ms /   131 runs   (    0.38 ms per token,  2627.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     626.36 ms /   701 tokens (    0.89 ms per token,  1119.16 tokens per second)\n",
      "llama_print_timings:        eval time =    2925.94 ms /   130 runs   (   22.51 ms per token,    44.43 tokens per second)\n",
      "llama_print_timings:       total time =    4057.95 ms /   831 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      84.91 ms /   232 runs   (    0.37 ms per token,  2732.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     351.18 ms /   435 tokens (    0.81 ms per token,  1238.69 tokens per second)\n",
      "llama_print_timings:        eval time =    5075.65 ms /   231 runs   (   21.97 ms per token,    45.51 tokens per second)\n",
      "llama_print_timings:       total time =    6334.25 ms /   666 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      16.29 ms /    43 runs   (    0.38 ms per token,  2639.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.54 ms /   117 tokens (    1.24 ms per token,   803.92 tokens per second)\n",
      "llama_print_timings:        eval time =     906.96 ms /    42 runs   (   21.59 ms per token,    46.31 tokens per second)\n",
      "llama_print_timings:       total time =    1212.70 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      37.54 ms /   100 runs   (    0.38 ms per token,  2663.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.94 ms /   109 tokens (    1.28 ms per token,   778.90 tokens per second)\n",
      "llama_print_timings:        eval time =    2070.86 ms /    99 runs   (   20.92 ms per token,    47.81 tokens per second)\n",
      "llama_print_timings:       total time =    2586.48 ms /   208 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       9.44 ms /    23 runs   (    0.41 ms per token,  2437.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.79 ms /    63 tokens (    1.93 ms per token,   517.30 tokens per second)\n",
      "llama_print_timings:        eval time =     472.44 ms /    22 runs   (   21.47 ms per token,    46.57 tokens per second)\n",
      "llama_print_timings:       total time =     694.22 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =       8.72 ms /    23 runs   (    0.38 ms per token,  2637.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.02 ms /   109 tokens (    1.28 ms per token,   784.05 tokens per second)\n",
      "llama_print_timings:        eval time =     462.90 ms /    22 runs   (   21.04 ms per token,    47.53 tokens per second)\n",
      "llama_print_timings:       total time =     690.65 ms /   131 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1313.54 ms\n",
      "llama_print_timings:      sample time =      71.26 ms /   189 runs   (    0.38 ms per token,  2652.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     284.38 ms /   303 tokens (    0.94 ms per token,  1065.49 tokens per second)\n",
      "llama_print_timings:        eval time =    4042.86 ms /   188 runs   (   21.50 ms per token,    46.50 tokens per second)\n",
      "llama_print_timings:       total time =    5062.93 ms /   491 tokens\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T21:23:08.140002Z",
     "start_time": "2024-06-16T17:32:45.924581Z"
    }
   },
   "cell_type": "code",
   "source": "answers_2_70b = make_prediction('Hotel reviews', df_train['review'].tolist()[:100], template, parser04, llm_2_70b)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.27 ms /     2 runs   (    1.14 ms per token,   881.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33967.82 ms /   142 tokens (  239.21 ms per token,     4.18 tokens per second)\n",
      "llama_print_timings:        eval time =   14698.07 ms /     1 runs   (14698.07 ms per token,     0.07 tokens per second)\n",
      "llama_print_timings:       total time =   48687.32 ms /   143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.00 ms /     2 runs   (    1.00 ms per token,   998.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32983.12 ms /   267 tokens (  123.53 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:        eval time =   14272.38 ms /     1 runs   (14272.38 ms per token,     0.07 tokens per second)\n",
      "llama_print_timings:       total time =   47265.20 ms /   268 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /     2 runs   (    1.09 ms per token,   917.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32654.99 ms /    90 tokens (  362.83 ms per token,     2.76 tokens per second)\n",
      "llama_print_timings:        eval time =   16139.30 ms /     1 runs   (16139.30 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   48804.12 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.37 ms /     2 runs   (    1.19 ms per token,   842.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35553.57 ms /    89 tokens (  399.48 ms per token,     2.50 tokens per second)\n",
      "llama_print_timings:        eval time =   15711.82 ms /     1 runs   (15711.82 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   51274.81 ms /    90 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /     2 runs   (    1.12 ms per token,   895.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36492.52 ms /    73 tokens (  499.90 ms per token,     2.00 tokens per second)\n",
      "llama_print_timings:        eval time =   15608.60 ms /     1 runs   (15608.60 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   52110.26 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.22 ms /     2 runs   (    1.11 ms per token,   899.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36714.05 ms /   101 tokens (  363.51 ms per token,     2.75 tokens per second)\n",
      "llama_print_timings:        eval time =   15611.47 ms /     1 runs   (15611.47 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   52335.07 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.06 ms /     2 runs   (    1.03 ms per token,   969.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36980.78 ms /   171 tokens (  216.26 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:        eval time =   15750.95 ms /     1 runs   (15750.95 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   52740.86 ms /   172 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /     2 runs   (    1.12 ms per token,   896.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38070.67 ms /   232 tokens (  164.10 ms per token,     6.09 tokens per second)\n",
      "llama_print_timings:        eval time =   15809.49 ms /     1 runs   (15809.49 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   53890.22 ms /   233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.34 ms /     2 runs   (    1.17 ms per token,   853.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39301.60 ms /   188 tokens (  209.05 ms per token,     4.78 tokens per second)\n",
      "llama_print_timings:        eval time =   16172.82 ms /     1 runs   (16172.82 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   55483.70 ms /   189 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.31 ms /     2 runs   (    1.15 ms per token,   867.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38377.07 ms /    88 tokens (  436.10 ms per token,     2.29 tokens per second)\n",
      "llama_print_timings:        eval time =   15976.34 ms /     1 runs   (15976.34 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   54362.75 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.31 ms /     2 runs   (    1.15 ms per token,   866.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39492.26 ms /   253 tokens (  156.10 ms per token,     6.41 tokens per second)\n",
      "llama_print_timings:        eval time =   16200.27 ms /     1 runs   (16200.27 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   55703.34 ms /   254 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.27 ms /     2 runs   (    1.14 ms per token,   879.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38867.21 ms /    87 tokens (  446.75 ms per token,     2.24 tokens per second)\n",
      "llama_print_timings:        eval time =   16139.91 ms /     1 runs   (16139.91 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   55018.34 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.20 ms /     2 runs   (    1.10 ms per token,   908.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39641.27 ms /   192 tokens (  206.46 ms per token,     4.84 tokens per second)\n",
      "llama_print_timings:        eval time =   16133.98 ms /     1 runs   (16133.98 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   55784.54 ms /   193 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.11 ms /     2 runs   (    1.05 ms per token,   949.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40286.78 ms /   336 tokens (  119.90 ms per token,     8.34 tokens per second)\n",
      "llama_print_timings:        eval time =   16389.67 ms /     1 runs   (16389.67 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   56686.78 ms /   337 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /     2 runs   (    1.10 ms per token,   905.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39571.99 ms /    71 tokens (  557.35 ms per token,     1.79 tokens per second)\n",
      "llama_print_timings:        eval time =   16191.12 ms /     1 runs   (16191.12 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   55772.54 ms /    72 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /     2 runs   (    1.09 ms per token,   915.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39390.58 ms /    50 tokens (  787.81 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:        eval time =   16405.22 ms /     1 runs   (16405.22 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   55805.33 ms /    51 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.86 ms /     2 runs   (    0.93 ms per token,  1075.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39739.12 ms /    33 tokens ( 1204.22 ms per token,     0.83 tokens per second)\n",
      "llama_print_timings:        eval time =   16439.84 ms /     1 runs   (16439.84 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   56187.93 ms /    34 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.16 ms /     2 runs   (    1.08 ms per token,   927.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40920.31 ms /    45 tokens (  909.34 ms per token,     1.10 tokens per second)\n",
      "llama_print_timings:        eval time =   16542.76 ms /     1 runs   (16542.76 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   57478.23 ms /    46 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /     2 runs   (    1.11 ms per token,   896.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40207.55 ms /    44 tokens (  913.81 ms per token,     1.09 tokens per second)\n",
      "llama_print_timings:        eval time =   16448.45 ms /     1 runs   (16448.45 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   56665.86 ms /    45 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.09 ms /     2 runs   (    1.05 ms per token,   955.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42845.87 ms /   346 tokens (  123.83 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:        eval time =   17165.95 ms /     1 runs   (17165.95 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60022.15 ms /   347 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.16 ms /     2 runs   (    1.08 ms per token,   925.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42090.41 ms /   210 tokens (  200.43 ms per token,     4.99 tokens per second)\n",
      "llama_print_timings:        eval time =   16840.45 ms /     1 runs   (16840.45 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   58941.13 ms /   211 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.04 ms /     2 runs   (    1.02 ms per token,   979.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41415.52 ms /    90 tokens (  460.17 ms per token,     2.17 tokens per second)\n",
      "llama_print_timings:        eval time =   17243.84 ms /     1 runs   (17243.84 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   58668.27 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.28 ms /     2 runs   (    1.14 ms per token,   876.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41666.52 ms /    93 tokens (  448.03 ms per token,     2.23 tokens per second)\n",
      "llama_print_timings:        eval time =   17263.65 ms /     1 runs   (17263.65 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   58939.78 ms /    94 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       3.29 ms /     2 runs   (    1.65 ms per token,   607.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41829.55 ms /   224 tokens (  186.74 ms per token,     5.36 tokens per second)\n",
      "llama_print_timings:        eval time =   17978.40 ms /     1 runs   (17978.40 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   59819.39 ms /   225 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.01 ms /     2 runs   (    1.00 ms per token,   996.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44175.80 ms /   180 tokens (  245.42 ms per token,     4.07 tokens per second)\n",
      "llama_print_timings:        eval time =   16166.86 ms /     1 runs   (16166.86 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60352.12 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /     2 runs   (    1.09 ms per token,   917.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40972.43 ms /    82 tokens (  499.66 ms per token,     2.00 tokens per second)\n",
      "llama_print_timings:        eval time =   16236.19 ms /     1 runs   (16236.19 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   57218.10 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.50 ms /     2 runs   (    1.25 ms per token,   800.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40890.64 ms /   142 tokens (  287.96 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:        eval time =   16776.14 ms /     1 runs   (16776.14 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   57677.07 ms /   143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       2.05 ms /     2 runs   (    1.02 ms per token,   976.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41785.04 ms /    49 tokens (  852.76 ms per token,     1.17 tokens per second)\n",
      "llama_print_timings:        eval time =   16436.25 ms /     1 runs   (16436.25 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   58230.33 ms /    50 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =     114.15 ms /   145 runs   (    0.79 ms per token,  1270.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   83235.47 ms /   591 tokens (  140.84 ms per token,     7.10 tokens per second)\n",
      "llama_print_timings:        eval time = 2531954.13 ms /   144 runs   (17583.01 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time = 2615878.56 ms /   735 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.59 ms per token,  1703.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47969.70 ms /   274 tokens (  175.07 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:        eval time =   16829.35 ms /     1 runs   (16829.35 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64807.14 ms /   275 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.53 ms /     2 runs   (    0.76 ms per token,  1308.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   48268.56 ms /    86 tokens (  561.26 ms per token,     1.78 tokens per second)\n",
      "llama_print_timings:        eval time =   16520.00 ms /     1 runs   (16520.00 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64796.62 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.39 ms /     2 runs   (    0.69 ms per token,  1444.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46374.77 ms /   144 tokens (  322.05 ms per token,     3.11 tokens per second)\n",
      "llama_print_timings:        eval time =   16259.95 ms /     1 runs   (16259.95 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   62643.68 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.49 ms /     2 runs   (    0.74 ms per token,  1342.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46047.88 ms /    96 tokens (  479.67 ms per token,     2.08 tokens per second)\n",
      "llama_print_timings:        eval time =   17016.90 ms /     1 runs   (17016.90 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   63075.80 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.25 ms /     2 runs   (    0.63 ms per token,  1598.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47213.60 ms /   166 tokens (  284.42 ms per token,     3.52 tokens per second)\n",
      "llama_print_timings:        eval time =   16565.46 ms /     1 runs   (16565.46 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   63787.38 ms /   167 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.44 ms /     2 runs   (    0.72 ms per token,  1386.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46683.16 ms /    98 tokens (  476.36 ms per token,     2.10 tokens per second)\n",
      "llama_print_timings:        eval time =   16032.81 ms /     1 runs   (16032.81 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   62724.45 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1517.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47883.85 ms /   197 tokens (  243.07 ms per token,     4.11 tokens per second)\n",
      "llama_print_timings:        eval time =   16916.34 ms /     1 runs   (16916.34 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64808.80 ms /   198 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /     2 runs   (    0.68 ms per token,  1469.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43666.45 ms /   150 tokens (  291.11 ms per token,     3.44 tokens per second)\n",
      "llama_print_timings:        eval time =   16461.24 ms /     1 runs   (16461.24 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60136.89 ms /   151 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1510.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44187.07 ms /   115 tokens (  384.24 ms per token,     2.60 tokens per second)\n",
      "llama_print_timings:        eval time =   16444.89 ms /     1 runs   (16444.89 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60641.30 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.40 ms /     2 runs   (    0.70 ms per token,  1429.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43777.95 ms /   167 tokens (  262.14 ms per token,     3.81 tokens per second)\n",
      "llama_print_timings:        eval time =   16953.31 ms /     1 runs   (16953.31 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60739.82 ms /   168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.28 ms /     2 runs   (    0.64 ms per token,  1564.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44231.49 ms /   134 tokens (  330.09 ms per token,     3.03 tokens per second)\n",
      "llama_print_timings:        eval time =   16444.86 ms /     1 runs   (16444.86 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60684.89 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.27 ms /     2 runs   (    0.64 ms per token,  1572.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43842.95 ms /    37 tokens ( 1184.94 ms per token,     0.84 tokens per second)\n",
      "llama_print_timings:        eval time =   16437.18 ms /     1 runs   (16437.18 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60288.74 ms /    38 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1509.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   89399.31 ms /   931 tokens (   96.03 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:        eval time =   17092.39 ms /     1 runs   (17092.39 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =  106505.50 ms /   932 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.41 ms /     2 runs   (    0.71 ms per token,  1414.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44151.61 ms /   213 tokens (  207.28 ms per token,     4.82 tokens per second)\n",
      "llama_print_timings:        eval time =   16780.91 ms /     1 runs   (16780.91 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60941.40 ms /   214 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.37 ms /     2 runs   (    0.68 ms per token,  1464.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45254.00 ms /   116 tokens (  390.12 ms per token,     2.56 tokens per second)\n",
      "llama_print_timings:        eval time =   16571.53 ms /     1 runs   (16571.53 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   61834.69 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.61 ms /     2 runs   (    0.81 ms per token,  1240.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43976.45 ms /   211 tokens (  208.42 ms per token,     4.80 tokens per second)\n",
      "llama_print_timings:        eval time =   16598.58 ms /     1 runs   (16598.58 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60585.81 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.28 ms /     2 runs   (    0.64 ms per token,  1556.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44549.79 ms /   168 tokens (  265.18 ms per token,     3.77 tokens per second)\n",
      "llama_print_timings:        eval time =   16949.89 ms /     1 runs   (16949.89 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   61508.62 ms /   169 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.45 ms /     2 runs   (    0.73 ms per token,  1377.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45179.65 ms /   258 tokens (  175.11 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:        eval time =   16528.84 ms /     1 runs   (16528.84 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   61718.76 ms /   259 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.29 ms /     2 runs   (    0.65 ms per token,  1545.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44502.50 ms /   162 tokens (  274.71 ms per token,     3.64 tokens per second)\n",
      "llama_print_timings:        eval time =   17783.59 ms /     1 runs   (17783.59 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   62294.95 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.27 ms /     2 runs   (    0.64 ms per token,  1572.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46096.28 ms /   113 tokens (  407.93 ms per token,     2.45 tokens per second)\n",
      "llama_print_timings:        eval time =   18056.40 ms /     1 runs   (18056.40 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64161.34 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.92 ms /     2 runs   (    0.96 ms per token,  1042.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45215.02 ms /    87 tokens (  519.71 ms per token,     1.92 tokens per second)\n",
      "llama_print_timings:        eval time =   16893.96 ms /     1 runs   (16893.96 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   62118.29 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.40 ms /     2 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44753.51 ms /   124 tokens (  360.92 ms per token,     2.77 tokens per second)\n",
      "llama_print_timings:        eval time =   17010.44 ms /     1 runs   (17010.44 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   61773.19 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.27 ms /     2 runs   (    0.64 ms per token,  1571.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44531.45 ms /    36 tokens ( 1236.98 ms per token,     0.81 tokens per second)\n",
      "llama_print_timings:        eval time =   17391.18 ms /     1 runs   (17391.18 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   61931.35 ms /    37 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.66 ms per token,  1524.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44658.08 ms /    91 tokens (  490.75 ms per token,     2.04 tokens per second)\n",
      "llama_print_timings:        eval time =   17909.15 ms /     1 runs   (17909.15 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   62576.06 ms /    92 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.47 ms /     2 runs   (    0.73 ms per token,  1362.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   48039.17 ms /   322 tokens (  149.19 ms per token,     6.70 tokens per second)\n",
      "llama_print_timings:        eval time =   19580.86 ms /     1 runs   (19580.86 ms per token,     0.05 tokens per second)\n",
      "llama_print_timings:       total time =   67630.13 ms /   323 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.41 ms /     2 runs   (    0.71 ms per token,  1414.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45978.40 ms /   144 tokens (  319.29 ms per token,     3.13 tokens per second)\n",
      "llama_print_timings:        eval time =   17081.07 ms /     1 runs   (17081.07 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   63068.93 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.27 ms /     2 runs   (    0.63 ms per token,  1581.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46531.68 ms /   103 tokens (  451.76 ms per token,     2.21 tokens per second)\n",
      "llama_print_timings:        eval time =   16149.64 ms /     1 runs   (16149.64 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   62690.12 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.43 ms /     2 runs   (    0.71 ms per token,  1402.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44147.77 ms /   124 tokens (  356.03 ms per token,     2.81 tokens per second)\n",
      "llama_print_timings:        eval time =   16407.69 ms /     1 runs   (16407.69 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60563.89 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.65 ms per token,  1531.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44602.58 ms /   235 tokens (  189.80 ms per token,     5.27 tokens per second)\n",
      "llama_print_timings:        eval time =   16268.90 ms /     1 runs   (16268.90 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60880.05 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.34 ms /     2 runs   (    0.67 ms per token,  1486.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44462.72 ms /   245 tokens (  181.48 ms per token,     5.51 tokens per second)\n",
      "llama_print_timings:        eval time =   16182.68 ms /     1 runs   (16182.68 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   60653.99 ms /   246 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.37 ms /     2 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47053.98 ms /   177 tokens (  265.84 ms per token,     3.76 tokens per second)\n",
      "llama_print_timings:        eval time =   18532.46 ms /     1 runs   (18532.46 ms per token,     0.05 tokens per second)\n",
      "llama_print_timings:       total time =   65595.54 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.54 ms /     2 runs   (    0.77 ms per token,  1297.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44978.26 ms /    69 tokens (  651.86 ms per token,     1.53 tokens per second)\n",
      "llama_print_timings:        eval time =   17410.57 ms /     1 runs   (17410.57 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   62398.77 ms /    70 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.34 ms /     2 runs   (    0.67 ms per token,  1491.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46034.17 ms /   383 tokens (  120.19 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:        eval time =   18211.31 ms /     1 runs   (18211.31 ms per token,     0.05 tokens per second)\n",
      "llama_print_timings:       total time =   64254.83 ms /   384 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.37 ms /     2 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45278.29 ms /   104 tokens (  435.37 ms per token,     2.30 tokens per second)\n",
      "llama_print_timings:        eval time =   17490.74 ms /     1 runs   (17490.74 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   62778.38 ms /   105 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /     2 runs   (    0.68 ms per token,  1474.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45986.44 ms /   162 tokens (  283.87 ms per token,     3.52 tokens per second)\n",
      "llama_print_timings:        eval time =   17957.33 ms /     1 runs   (17957.33 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   63952.03 ms /   163 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.43 ms /     2 runs   (    0.71 ms per token,  1403.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46137.89 ms /   115 tokens (  401.20 ms per token,     2.49 tokens per second)\n",
      "llama_print_timings:        eval time =   18706.68 ms /     1 runs   (18706.68 ms per token,     0.05 tokens per second)\n",
      "llama_print_timings:       total time =   64854.80 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.30 ms /     2 runs   (    0.65 ms per token,  1542.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44389.74 ms /    43 tokens ( 1032.32 ms per token,     0.97 tokens per second)\n",
      "llama_print_timings:        eval time =   17043.89 ms /     1 runs   (17043.89 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   61442.45 ms /    44 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.58 ms per token,  1739.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44715.73 ms /   192 tokens (  232.89 ms per token,     4.29 tokens per second)\n",
      "llama_print_timings:        eval time =   16443.12 ms /     1 runs   (16443.12 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   61168.13 ms /   193 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =     103.80 ms /   152 runs   (    0.68 ms per token,  1464.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45867.31 ms /   366 tokens (  125.32 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:        eval time = 2623650.27 ms /   151 runs   (17375.17 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time = 2670260.69 ms /   517 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.43 ms /     2 runs   (    0.71 ms per token,  1401.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47416.85 ms /   207 tokens (  229.07 ms per token,     4.37 tokens per second)\n",
      "llama_print_timings:        eval time =   18352.78 ms /     1 runs   (18352.78 ms per token,     0.05 tokens per second)\n",
      "llama_print_timings:       total time =   65779.63 ms /   208 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.41 ms /     2 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46992.90 ms /   175 tokens (  268.53 ms per token,     3.72 tokens per second)\n",
      "llama_print_timings:        eval time =   18050.55 ms /     1 runs   (18050.55 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   65052.21 ms /   176 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.30 ms /     2 runs   (    0.65 ms per token,  1534.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47479.45 ms /    89 tokens (  533.48 ms per token,     1.87 tokens per second)\n",
      "llama_print_timings:        eval time =   17426.42 ms /     1 runs   (17426.42 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64913.73 ms /    90 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.39 ms /     2 runs   (    0.70 ms per token,  1435.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46759.32 ms /   107 tokens (  437.00 ms per token,     2.29 tokens per second)\n",
      "llama_print_timings:        eval time =   17723.96 ms /     1 runs   (17723.96 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64491.40 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.29 ms /     2 runs   (    0.65 ms per token,  1545.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46503.44 ms /    69 tokens (  673.96 ms per token,     1.48 tokens per second)\n",
      "llama_print_timings:        eval time =   17294.39 ms /     1 runs   (17294.39 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   63805.61 ms /    70 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.60 ms /     2 runs   (    0.80 ms per token,  1251.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29310.49 ms /    31 tokens (  945.50 ms per token,     1.06 tokens per second)\n",
      "llama_print_timings:        eval time =   18929.08 ms /     1 runs   (18929.08 ms per token,     0.05 tokens per second)\n",
      "llama_print_timings:       total time =   48248.41 ms /    32 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.29 ms /     2 runs   (    0.64 ms per token,  1551.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46628.77 ms /   297 tokens (  157.00 ms per token,     6.37 tokens per second)\n",
      "llama_print_timings:        eval time =   17926.62 ms /     1 runs   (17926.62 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64563.81 ms /   298 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.27 ms /     2 runs   (    0.64 ms per token,  1574.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46828.83 ms /   244 tokens (  191.92 ms per token,     5.21 tokens per second)\n",
      "llama_print_timings:        eval time =   17012.67 ms /     1 runs   (17012.67 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   63849.62 ms /   245 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.27 ms /     2 runs   (    0.64 ms per token,  1572.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46981.15 ms /   102 tokens (  460.60 ms per token,     2.17 tokens per second)\n",
      "llama_print_timings:        eval time =   18194.88 ms /     1 runs   (18194.88 ms per token,     0.05 tokens per second)\n",
      "llama_print_timings:       total time =   65184.12 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.37 ms /     2 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47285.97 ms /   130 tokens (  363.74 ms per token,     2.75 tokens per second)\n",
      "llama_print_timings:        eval time =   16834.19 ms /     1 runs   (16834.19 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64128.68 ms /   131 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /     2 runs   (    0.68 ms per token,  1471.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46715.77 ms /   237 tokens (  197.11 ms per token,     5.07 tokens per second)\n",
      "llama_print_timings:        eval time =   17550.25 ms /     1 runs   (17550.25 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64274.22 ms /   238 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.28 ms /     2 runs   (    0.64 ms per token,  1557.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47341.13 ms /   246 tokens (  192.44 ms per token,     5.20 tokens per second)\n",
      "llama_print_timings:        eval time =   17285.94 ms /     1 runs   (17285.94 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64636.07 ms /   247 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /     2 runs   (    0.68 ms per token,  1467.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46456.11 ms /   135 tokens (  344.12 ms per token,     2.91 tokens per second)\n",
      "llama_print_timings:        eval time =   17104.08 ms /     1 runs   (17104.08 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   63568.38 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.38 ms /     2 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46735.43 ms /    61 tokens (  766.15 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:        eval time =   17452.58 ms /     1 runs   (17452.58 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64196.42 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.43 ms /     2 runs   (    0.72 ms per token,  1397.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46093.19 ms /   199 tokens (  231.62 ms per token,     4.32 tokens per second)\n",
      "llama_print_timings:        eval time =   17979.52 ms /     1 runs   (17979.52 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64080.64 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.21 ms /     2 runs   (    0.60 ms per token,  1654.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47974.86 ms /   374 tokens (  128.28 ms per token,     7.80 tokens per second)\n",
      "llama_print_timings:        eval time =   17990.06 ms /     1 runs   (17990.06 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   65974.54 ms /   375 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.39 ms /     2 runs   (    0.70 ms per token,  1437.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47253.48 ms /    69 tokens (  684.83 ms per token,     1.46 tokens per second)\n",
      "llama_print_timings:        eval time =   17865.19 ms /     1 runs   (17865.19 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   65126.84 ms /    70 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.35 ms /     2 runs   (    0.67 ms per token,  1484.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47589.35 ms /    96 tokens (  495.72 ms per token,     2.02 tokens per second)\n",
      "llama_print_timings:        eval time =   16263.06 ms /     1 runs   (16263.06 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   63860.70 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.47 ms /     2 runs   (    0.73 ms per token,  1365.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46708.39 ms /    69 tokens (  676.93 ms per token,     1.48 tokens per second)\n",
      "llama_print_timings:        eval time =   16733.23 ms /     1 runs   (16733.23 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   63449.69 ms /    70 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.44 ms /     2 runs   (    0.72 ms per token,  1386.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49995.79 ms /   192 tokens (  260.39 ms per token,     3.84 tokens per second)\n",
      "llama_print_timings:        eval time =   18262.59 ms /     1 runs   (18262.59 ms per token,     0.05 tokens per second)\n",
      "llama_print_timings:       total time =   68267.48 ms /   193 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.46 ms /     2 runs   (    0.73 ms per token,  1367.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   50574.98 ms /    49 tokens ( 1032.14 ms per token,     0.97 tokens per second)\n",
      "llama_print_timings:        eval time =   16143.46 ms /     1 runs   (16143.46 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   66726.50 ms /    50 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.66 ms per token,  1524.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47010.96 ms /   134 tokens (  350.83 ms per token,     2.85 tokens per second)\n",
      "llama_print_timings:        eval time =   17310.92 ms /     1 runs   (17310.92 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64330.44 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /     2 runs   (    0.68 ms per token,  1466.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47421.97 ms /   163 tokens (  290.93 ms per token,     3.44 tokens per second)\n",
      "llama_print_timings:        eval time =   16768.36 ms /     1 runs   (16768.36 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64198.94 ms /   164 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /     2 runs   (    0.68 ms per token,  1474.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49265.02 ms /    63 tokens (  781.98 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:        eval time =   17723.82 ms /     1 runs   (17723.82 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   66998.37 ms /    64 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /     2 runs   (    0.68 ms per token,  1468.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   48518.25 ms /   154 tokens (  315.05 ms per token,     3.17 tokens per second)\n",
      "llama_print_timings:        eval time =   17014.19 ms /     1 runs   (17014.19 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   65540.84 ms /   155 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.33 ms /     2 runs   (    0.67 ms per token,  1500.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   50333.64 ms /   101 tokens (  498.35 ms per token,     2.01 tokens per second)\n",
      "llama_print_timings:        eval time =   18811.35 ms /     1 runs   (18811.35 ms per token,     0.05 tokens per second)\n",
      "llama_print_timings:       total time =   69154.46 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.39 ms /     2 runs   (    0.69 ms per token,  1440.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47988.49 ms /   108 tokens (  444.34 ms per token,     2.25 tokens per second)\n",
      "llama_print_timings:        eval time =   16885.33 ms /     1 runs   (16885.33 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   64881.71 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.48 ms /     2 runs   (    0.74 ms per token,  1355.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49568.35 ms /    86 tokens (  576.38 ms per token,     1.73 tokens per second)\n",
      "llama_print_timings:        eval time =   16835.70 ms /     1 runs   (16835.70 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   66414.10 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.47 ms /     2 runs   (    0.73 ms per token,  1361.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   50109.86 ms /    85 tokens (  589.53 ms per token,     1.70 tokens per second)\n",
      "llama_print_timings:        eval time =   16618.37 ms /     1 runs   (16618.37 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   66736.21 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =     117.96 ms /   142 runs   (    0.83 ms per token,  1203.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74629.80 ms /   522 tokens (  142.97 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:        eval time = 2485153.06 ms /   141 runs   (17625.20 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time = 2560462.22 ms /   663 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1518.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47871.34 ms /   182 tokens (  263.03 ms per token,     3.80 tokens per second)\n",
      "llama_print_timings:        eval time =   17983.30 ms /     1 runs   (17983.30 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   65863.03 ms /   183 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33978.88 ms\n",
      "llama_print_timings:      sample time =       1.38 ms /     2 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47889.30 ms /   203 tokens (  235.91 ms per token,     4.24 tokens per second)\n",
      "llama_print_timings:        eval time =   17526.41 ms /     1 runs   (17526.41 ms per token,     0.06 tokens per second)\n",
      "llama_print_timings:       total time =   65423.83 ms /   204 tokens\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T17:28:07.924622Z",
     "start_time": "2024-06-16T17:28:07.737219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answers_true = df_train['rating'].tolist()[:1000]\n",
    "confusion_matrix_plot(answers_true, answers_2_13b)\n",
    "print(f\"Accuracy score: {accuracy_score(answers_true, answers_2_13b)}\")\n",
    "print(f\"Recall score: {recall_score(answers_true, answers_2_13b, average='weighted')}\")\n",
    "print(f\"Precision score: {precision_score(answers_true, answers_2_13b, average='weighted')}\")\n",
    "print(classification_report(answers_true, answers_2_13b))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.626\n",
      "Recall score: 0.626\n",
      "Precision score: 0.6555899401318349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.46      0.59        69\n",
      "           1       0.44      0.56      0.49        81\n",
      "           2       0.47      0.40      0.43       112\n",
      "           3       0.50      0.66      0.57       282\n",
      "           4       0.81      0.70      0.75       456\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.61      0.56      0.57      1000\n",
      "weighted avg       0.66      0.63      0.63      1000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T17:28:08.099719Z",
     "start_time": "2024-06-16T17:28:07.926065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answers_true = df_train['rating'].tolist()[:100]\n",
    "confusion_matrix_plot(answers_true, answers_2_13b[:100])\n",
    "print(f\"Accuracy score: {accuracy_score(answers_true, answers_2_13b[:100])}\")\n",
    "print(f\"Recall score: {recall_score(answers_true, answers_2_13b[:100], average='weighted')}\")\n",
    "print(f\"Precision score: {precision_score(answers_true, answers_2_13b[:100], average='weighted')}\")\n",
    "print(classification_report(answers_true, answers_2_13b[:100]))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.65\n",
      "Recall score: 0.65\n",
      "Precision score: 0.6835683728036669\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       0.86      0.50      0.63        12\n",
      "           2       0.30      0.38      0.33         8\n",
      "           3       0.57      0.74      0.64        34\n",
      "           4       0.76      0.63      0.69        41\n",
      "\n",
      "    accuracy                           0.65       100\n",
      "   macro avg       0.70      0.65      0.66       100\n",
      "weighted avg       0.68      0.65      0.65       100\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T21:23:09.002068Z",
     "start_time": "2024-06-16T21:23:08.145079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answers_true = df_train['rating'].tolist()[:100]\n",
    "confusion_matrix_plot(answers_true, answers_2_70b)\n",
    "print(f\"Accuracy score: {accuracy_score(answers_true, answers_2_70b)}\")\n",
    "print(f\"Recall score: {recall_score(answers_true, answers_2_70b, average='weighted')}\")\n",
    "print(f\"Precision score: {precision_score(answers_true, answers_2_70b, average='weighted')}\")\n",
    "print(classification_report(answers_true, answers_2_70b))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAGwCAYAAABSAee3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+/ElEQVR4nO3deVxU9f4/8NdhgBmWYWQREMEt3BCX0vJycytJs74uad/K7Bua6c3AVDSVSk1L8WffzExTb4tc+8q1VTNvaWaJmkuCormRLCYuuKEMoGxzzu8PcmzCZYaZ4ZyZ83o+Hudxm8NZXnxuzZvP53zOOYIkSRKIiIjIJXnIHYCIiIjqj4WciIjIhbGQExERuTAWciIiIhfGQk5EROTCWMiJiIhcGAs5ERGRC/OUO4A9RFHEmTNnoNfrIQiC3HGIiMhGkiShtLQUERER8PBwXt+yoqICVVVVdh/H29sbOp3OAYkcx6UL+ZkzZxAVFSV3DCIislNhYSEiIyOdcuyKigq0bO6PovMmu48VHh6OgoICRRVzly7ker0eABAXNw2enlqZ0yibZvsBuSMQEdVRg2rswLfm73NnqKqqQtF5E37PaoEAff17/cZSEc27nkBVVRULuaNcH0739NTC01M5japEGsFL7ghERHX98ZDwhrg86q8X4K+v/3lEKPMSrksXciIiImuZJBEmO94uYpJEx4VxIBZyIiJSBRESRNS/ktuzrzPx9jMiIiIXxh45ERGpgggR9gyO27e387CQExGRKpgkCSap/sPj9uzrTBxaJyIicmHskRMRkSq462Q3FnIiIlIFERJMbljIObRORETkBKmpqbj33nuh1+sRGhqKIUOGICcnx2KbPn36QBAEi+WFF16w6Tws5EREpArXh9btWWyRkZGBxMRE7N69G5s3b0Z1dTX69euH8vJyi+3GjBmDs2fPmpcFCxbYdB4OrRMRkSo4ata60Wi0WK/VaqHV1n3fx8aNGy0+p6WlITQ0FFlZWejVq5d5va+vL8LDw+udiz1yIiIiG0RFRcFgMJiX1NRUq/YrKSkBAAQFBVmsX716NUJCQhAbG4uUlBRcvXrVpjzskRMRkSqIfyz27A/UvnI1ICDAvP5mvfE6+4oiJk6ciPvvvx+xsbHm9U8//TSaN2+OiIgIHDx4ENOmTUNOTg6++uorq3OxkBMRkSqY7Jy1fn3fgIAAi0JujcTERBw6dAg7duywWD927FjzP3fs2BFNmjRB3759kZeXh7vuusuqY3NonYiIVMEk2b/UR1JSEjZs2ICffvoJkZGRt922e/fuAIDc3Fyrj88eORERkRNIkoTx48dj7dq12Lp1K1q2bHnHfbKzswEATZo0sfo8LORERKQKjrpGbq3ExESkp6fj66+/hl6vR1FREQDAYDDAx8cHeXl5SE9PxyOPPILg4GAcPHgQkyZNQq9evdCpUyerz8NCTkREqiBCgAmCXfvbYtmyZQBqH/ryZytXrsTIkSPh7e2NH374AYsWLUJ5eTmioqIwbNgwvPbaazadh4WciIjICaQ73LMeFRWFjIwMu8/DQk5ERKogSrWLPfsrEQs5ERGpgsnOoXV79nUm3n5GRETkwtgjJyIiVXDXHjkLORERqYIoCRAlO2at27GvM3FonYiIyIWxR05ERKrAoXUiIiIXZoIHTHYMRJscmMWRWMiJiEgVJDuvkUu8Rk5ERESOxh65gzw7bD+eHZZtse7kGQOemzJUnkAKN3DkRTw+7jyCGtcg/4gP3n+tKXKyfeWOpShsI+uwnazDdnLfa+SK6JEvXboULVq0gE6nQ/fu3fHLL7/IHaleCgob4b/HPWleJs5+RO5IitR70GWMnXUGqxeGI7F/G+Qf0WFuej4MwdVyR1MMtpF12E7WYTvVMkkedi9KJHuqTz/9FMnJyZg1axb27duHzp07o3///jh//rzc0WxmMnngcomveTGW6uSOpEhDx17ExvQgfP9pEE4e12HxtEhUXhPQf3ix3NEUg21kHbaTddhO7k32Qr5w4UKMGTMGo0aNQkxMDJYvXw5fX198/PHHckezWdNwI9YsXYNPFn2OlMQMhAaXyR1JcTy9RLTudBX7tuvN6yRJwP7tesR0vSpjMuVgG1mH7WQdttMNIgSI8LBj4dB6HVVVVcjKykJ8fLx5nYeHB+Lj47Fr164621dWVsJoNFosSnE0tzHeWtEDKfP74d2P/47wxqV4Z+a38NGpa+jqTgKCTNB4AlcuWE7PuHzRE4GNa2RKpSxsI+uwnazDdrrh+jVyexYlkrWQX7x4ESaTCWFhYRbrw8LCUFRUVGf71NRUGAwG8xIVFdVQUe9o74FIbNvTEgWFQcg82BSvLHgI/n5V6P23ArmjERGRG5N9aN0WKSkpKCkpMS+FhYVyR7ql8qtanDprQNMw5YwaKIGxWANTDdDoLz2BwJAaXL7AmygAtpG12E7WYTvdwMluThASEgKNRoNz585ZrD937hzCw8PrbK/VahEQEGCxKJVOW40mYUZcuqKu2zvupKbaA8cP+uLuHqXmdYIgoUuPMhzJYlsBbCNrsZ2sw3a6ofYauX2LEslayL29vdG1a1ds2bLFvE4URWzZsgVxcXEyJrPd2Kd/Qad2RQgLKUVM63OYnfwjRFHATztbyR1Ncb76ZwgGPF2M+P8uRlR0BcbPPwWdr4jv1wTJHU0x2EbWYTtZh+3k3mQfV0lOTkZCQgK6deuG++67D4sWLUJ5eTlGjRoldzSbNA6+ilfGb0WAfyVKjDoc+i0M42f+F0p4C1odGesDYQg24dmXixDYuAb5h33w6oiWuHLRS+5oisE2sg7byTpsp1qinc9aFyE5MI3jCJIkyZ5syZIleOutt1BUVIQuXbpg8eLF6N69+x33MxqNMBgM6NlzJjw9WTBvR7N1n9wRiIjqqJGqsRVfo6SkxGmXS6/XijXZMfDVa+p9nKulJjzV5YhTs9aH7D1yAEhKSkJSUpLcMYiIyI1dvx+8/vvL3u+9KWVOwSMiIiKrKKJHTkRE5GwmSYDJjleR2rOvM7GQExGRKpjsnOxm4tA6ERERORp75EREpAqi5AHRjqezifLf5HVTLORERKQKHFonIiIixWGPnIiIVEGEfTPPRcdFcSgWciIiUgX7HwijzEFsZaYiIiIiq7BHTkREqmDvO8WV+j5yFnIiIlIFe98prtT3kbOQExGRKrhrj1yZqYiIiMgq7JETEZEq2P9AGGX2fVnIiYhIFURJgGjPfeQKffuZMv+8ICIiIquwR05ERKog2jm0rtQHwrCQExGRKtj/9jNlFnJlpiIiIiKrsEdORESqYIIAkx0PdbFnX2diISciIlXg0DoREREpDnvkRESkCibYNzxuclwUh2IhJyIiVXDXoXUWciIiUgW+NIWIiIgUhz1yIiJSBcnO95FLvP2MiIhIPhxaJyIiIsVxix65V3YePAVvuWMomtQtVu4ILqG0pZ/cEVyC/+d75I5AZDN3fY2pWxRyIiKiOzHZ+fYze/Z1JmWmIiIiIquwR05ERKrAoXUiIiIXJsIDoh0D0fbs60zKTEVERERWYY+ciIhUwSQJMNkxPG7Pvs7EQk5ERKrgrtfIObRORESqIP3x9rP6LpKNT3ZLTU3FvffeC71ej9DQUAwZMgQ5OTkW21RUVCAxMRHBwcHw9/fHsGHDcO7cOZvOw0JORETkBBkZGUhMTMTu3buxefNmVFdXo1+/figvLzdvM2nSJHzzzTf4/PPPkZGRgTNnzmDo0KE2nYdD60REpAomCDDZ8eKT6/sajUaL9VqtFlqtts72GzdutPiclpaG0NBQZGVloVevXigpKcFHH32E9PR0PPjggwCAlStXon379ti9ezf+9re/WZWLPXIiIlIFUbpxnbx+S+1xoqKiYDAYzEtqaqpV5y8pKQEABAUFAQCysrJQXV2N+Ph48zbt2rVDs2bNsGvXLqt/L/bIiYiIbFBYWIiAgADz55v1xv9KFEVMnDgR999/P2Jja999UVRUBG9vbzRq1Mhi27CwMBQVFVmdh4WciIhU4fqkNXv2B4CAgACLQm6NxMREHDp0CDt27Kj3+W+FhZyIiFRBhADRjmvk9d03KSkJGzZswLZt2xAZGWleHx4ejqqqKly5csWiV37u3DmEh4dbfXxeIyciInICSZKQlJSEtWvX4scff0TLli0tft61a1d4eXlhy5Yt5nU5OTk4efIk4uLirD4Pe+RERKQKDf1kt8TERKSnp+Prr7+GXq83X/c2GAzw8fGBwWDA6NGjkZycjKCgIAQEBGD8+PGIi4uzesY6wEJOREQq4ahr5NZatmwZAKBPnz4W61euXImRI0cCAN555x14eHhg2LBhqKysRP/+/fH+++/bdB4WciIiIieQJOmO2+h0OixduhRLly6t93lYyImISBVE2PmsdTsmyjkTCzkREamCZOesdYmFnIiISD58+xkREREpDnvkRESkCg09a72hsJATEZEqcGidiIiIFIc9ciIiUgW5nrXubCzkRESkChxaJyIiIsVhj5yIiFTBXXvkLORERKQK7lrIObRORETkwtgjd5DYbiV4fPQpRMeWIzi0CnNebI9dW4LljqVoTww7jOcSsrF2fVus+LCb3HEU5YuZ6WgSXFZn/ZfbY7Dwix4yJFKugSMv4vFx5xHUuAb5R3zw/mtNkZPtK3csxWE7sUfuFNu2bcPAgQMREREBQRCwbt06OePYRedrQn6OP96f3UruKC6hTfQlPPLwceQXNJI7iiI9//ZjGPjaM+ZlwtJHAAA/ZfPfrz/rPegyxs46g9ULw5HYvw3yj+gwNz0fhuBquaMpCtuploQbt6DVZ7nzS0nlIWshLy8vR+fOne16D6tSZG4LwqpFzbHzhxC5oyieTleNqZN/xrtLuqOszFvuOIp0pdwHxaW+5uX+Didx6kIA9uc2kTuaogwdexEb04Pw/adBOHlch8XTIlF5TUD/4cVyR1MUtlOt6z1yexYlkrWQDxgwAG+++SYee+wxOWNQA0t8YS9+yWyK/QdYlKzhqTGhX7fj+M+etoBCH0ghB08vEa07XcW+7XrzOkkSsH+7HjFdr8qYTFnYTu7Ppa6RV1ZWorKy0vzZaDTKmIbqo3fPE4huVYyXJg+QO4rL6NXxBPx9qvDtnjZyR1GUgCATNJ7AlQuWX2OXL3oiKrryFnupD9vpBl4jV4DU1FQYDAbzEhUVJXckskFISDleGJOFBQvvR3W1Ru44LuO//paD3UejcNHoJ3cUIpfmrkPrLtUjT0lJQXJysvmz0WhkMXchre8qRmCjCix55zvzOo1GQmyH8xj06G8YOOwpiKJL/W3pdGGBpejW9jRe+eghuaMojrFYA1MN0KhxjcX6wJAaXL7gUl9tTsV2cn8u9f+iVquFVquVOwbVU/bBcPwj6VGLdZMn7ELhqQB89mUHFvGbeLR7Di6X6rDrSDO5oyhOTbUHjh/0xd09SrFrowEAIAgSuvQow/o03vp5HdvpBncdWnepQq5kOl8TIppdM38Oi6xAq3ZlKC3xxIWzOhmTKce1a174/WQji3UVFZ4wlmrrrKfaL9tHu/+G7/a2gYl/5NzUV/8MwZRFhfjtgC9y9vvisTEXoPMV8f2aILmjKQrbqZYkCZDsKMb27OtMshbysrIy5Obmmj8XFBQgOzsbQUFBaNbMtXogrWNLseCTQ+bP/3ilAACw+atQLEzhJCWy3b1tTiM8qAz/2d1W7iiKlbE+EIZgE559uQiBjWuQf9gHr45oiSsXveSOpihsJ/cmSJIk2z3uW7duxQMPPFBnfUJCAtLS0u64v9FohMFgwIP6EfAUeD/y7Uhtm8sdwSWUtuSEMmv4f75H7gjkJmqkamzF1ygpKUFAQIBTznG9VsR9PR6efvW/PFtTXoldg99zatb6kLVH3qdPH8j4dwQREamIu14j54U3IiIiF8bJbkREpAqc7EZEROTC3HVonYWciIhUwV175LxGTkRE5MLYIyciIlWQ7BxaV2qPnIWciIhUQQJgzx3PSr1ZmkPrRERELow9ciIiUgURAgTYMWvdjn2diYWciIhUgbPWiYiISHHYIyciIlUQJQECHwhDRETkmiTJzlnrCp22zqF1IiIiF8YeORERqYK7TnZjISciIlVgISciInJh7jrZjdfIiYiIXBh75EREpAruOmudhZyIiFShtpDbc43cgWEciEPrRERELow9ciIiUgXOWiciInJhEux7p7hCR9Y5tE5EROTK2CMnIiJV4NA6ERGRK3PTsXUWciIiUgc7e+RQaI+c18iJiIhcGHvkRESkCnyyGxERkQvjZDcFE0vLIApecsdQNI+c3+WO4BIMpWFyR3AJYrdYuSO4BCnzkNwRSAXcopATERHdkSTYN2GNPXIiIiL5uOs1cs5aJyIicmHskRMRkTq46QNh2CMnIiJVuD5r3Z7FFtu2bcPAgQMREREBQRCwbt06i5+PHDkSgiBYLA8//LDNv5dVPfL169dbfcBBgwbZHIKIiMjdlJeXo3PnznjuuecwdOjQm27z8MMPY+XKlebPWq3W5vNYVciHDBli1cEEQYDJZLI5BBERUYNwwPC40Wi0+KzVam9agAcMGIABAwbc9lharRbh4eF25bFqaF0URasWFnEiIlIqRw2tR0VFwWAwmJfU1NR6Z9q6dStCQ0PRtm1bjBs3DpcuXbL5GHZNdquoqIBOp7PnEERERA3DQZPdCgsLERAQYF5dn+FwoHZYfejQoWjZsiXy8vLwyiuvYMCAAdi1axc0Go3Vx7G5kJtMJsybNw/Lly/HuXPn8Ntvv6FVq1aYMWMGWrRogdGjR9t6SCIiIpcREBBgUcjr66mnnjL/c8eOHdGpUyfcdddd2Lp1K/r27Wv1cWyetT537lykpaVhwYIF8Pb2Nq+PjY3Fhx9+aOvhiIiIGojggMV5WrVqhZCQEOTm5tq0n82FfNWqVfjnP/+JESNGWHT9O3fujGPHjtl6OCIiooYhOWBxolOnTuHSpUto0qSJTfvZPLR++vRpREdH11kviiKqq6ttPRwREZFbKisrs+hdFxQUIDs7G0FBQQgKCsLs2bMxbNgwhIeHIy8vD1OnTkV0dDT69+9v03ls7pHHxMRg+/btddZ/8cUXuPvuu209HBERUcNo4B55ZmYm7r77bnNtTE5Oxt13342ZM2dCo9Hg4MGDGDRoENq0aYPRo0eja9eu2L59u82T52zukc+cORMJCQk4ffo0RFHEV199hZycHKxatQobNmyw9XBEREQNo4HfftanTx9It3nTyqZNm+qf5U9s7pEPHjwY33zzDX744Qf4+flh5syZOHr0KL755hs89NBDDglFRERE1qnXfeQ9e/bE5s2bHZ2FiIjIadz1Nab1fiBMZmYmjh49CqD2unnXrl0dFoqIiMjh3PTtZzYX8lOnTmH48OH4+eef0ahRIwDAlStX8Pe//x1r1qxBZGSkozMSERHRLdh8jfz5559HdXU1jh49iuLiYhQXF+Po0aMQRRHPP/+8MzISERHZ7/pkN3sWBbK5R56RkYGdO3eibdu25nVt27bFe++9h549ezo0HBERkaMIUu1iz/5KZHMhj4qKuumDX0wmEyIiIhwSioiIyOHc9Bq5zUPrb731FsaPH4/MzEzzuszMTEyYMAH/+7//69BwREREdHtW9cgDAwMhCDeuDZSXl6N79+7w9KzdvaamBp6ennjuuecwZMgQpwQlIiKySwM/EKahWFXIFy1a5OQYRERETuamQ+tWFfKEhARn5yAiIqJ6qPcDYQCgoqICVVVVFusc8bJ1IiIih3PTHrnNk93Ky8uRlJSE0NBQ+Pn5ITAw0GIhIiJSJIW/j7y+bC7kU6dOxY8//ohly5ZBq9Xiww8/xOzZsxEREYFVq1Y5IyMRERHdgs1D69988w1WrVqFPn36YNSoUejZsyeio6PRvHlzrF69GiNGjHBGTiIiIvu46ax1m3vkxcXFaNWqFYDa6+HFxcUAgB49emDbtm2OTUdEROQg15/sZs+iRDb3yFu1aoWCggI0a9YM7dq1w2effYb77rsP33zzjfklKmo2cORFPD7uPIIa1yD/iA/ef60pcrJ95Y6lGLHdSvD46FOIji1HcGgV5rzYHru2BMsdS3EeGZSHRwflIyz8KgDg9xMB+Peq9sj8JVzmZMr1xLDDeC4hG2vXt8WKD7vJHUdx+N3kvmzukY8aNQoHDhwAAEyfPh1Lly6FTqfDpEmT8PLLL9t0rNTUVNx7773Q6/UIDQ3FkCFDkJOTY2skxeg96DLGzjqD1QvDkdi/DfKP6DA3PR+G4LqPtFUrna8J+Tn+eH92K7mjKNrFCz5Y+UEsXvrHg5jwwoM4sL8xZry5E81aGOWOpkhtoi/hkYePI7+gkdxRFInfTX/gZLdakyZNwksvvQQAiI+Px7Fjx5Ceno79+/djwoQJNh0rIyMDiYmJ2L17NzZv3ozq6mr069cP5eXltsZShKFjL2JjehC+/zQIJ4/rsHhaJCqvCeg/vFjuaIqRuS0IqxY1x84fQuSOomi/7IpA5p4mOHNaj9On9Fj1USwqrnmiXcwluaMpjk5XjamTf8a7S7qjrMxb7jiKxO8m92bXfeQA0Lx5czRv3rxe+27cuNHic1paGkJDQ5GVlYVevXrZG61BeXqJaN3pKtYsCTWvkyQB+7frEdP1qozJyNV5eEjo0fsUdDoTjh7mZYi/SnxhL37JbIr9B5pg+BOH5I6jOPxuukGAnW8/c1gSx7KqkC9evNjqA17vrddHSUkJACAoKOimP6+srERlZaX5s9GonGHGgCATNJ7AlQuWTXr5oieioitvsRfRrbVoWYK3l/4Eb28R16554o2Zf0Ph73zg0p/17nkC0a2K8dLkAXJHUSx+N7k/qwr5O++8Y9XBBEGodyEXRRETJ07E/fffj9jY2Jtuk5qaitmzZ9fr+ESu5lShHknPx8PPvxo9ep3G5OmZmDqxN4v5H0JCyvHCmCy8MvNBVFdr5I5DrsBNbz+zqpAXFBQ4OwcSExNx6NAh7Nix45bbpKSkIDk52fzZaDQiKirK6dmsYSzWwFQDNGpcY7E+MKQGly/YfQWDVKimxgNnz/gDAHJ/C0TrdsUYPCwXSxbeI3MyZWh9VzECG1VgyTvfmddpNBJiO5zHoEd/w8BhT0EUbZ4G5Hb43fQnbvqIVkX8v5iUlIQNGzZg27ZtiIyMvOV2Wq0WWq22AZNZr6baA8cP+uLuHqXYtdEAABAECV16lGF9Gq9rkv08BMDLS5Q7hmJkHwzHP5IetVg3ecIuFJ4KwGdfdmAR/wO/m9yfrIVckiSMHz8ea9euxdatW9GyZUs549jtq3+GYMqiQvx2wBc5+33x2JgL0PmK+H7Nza/5q5HO14SIZtfMn8MiK9CqXRlKSzxx4axOxmTKMvL5Q8j8JQznz/nC17cGffoWomOXC5gxtYfc0RTj2jUv/H6ykcW6igpPGEu1ddarHb+b/sAeueMlJiYiPT0dX3/9NfR6PYqKigAABoMBPj4+ckarl4z1gTAEm/Dsy0UIbFyD/MM+eHVES1y56CV3NMVoHVuKBZ/cmFn8j1dqL9ts/ioUC1PayBVLcQyBlZickomgoAqUl3uhID8AM6b2wP6sMLmjkQvid1Mte5/OptQnuwmSJMkWTRBuPnFg5cqVGDly5B33NxqNMBgM6IPB8BTU9S+krTz0erkjuAQhgoXSGqKeoyfWkDJ5O9yd1EjV2IqvUVJS4rTXYF+vFS3mzoWHrv7/7ooVFTjx6qtOzVofsg+tExERNQg3HVqv12yQ7du345lnnkFcXBxOnz4NAPjkk09uO+OciIhIVnxEa60vv/wS/fv3h4+PD/bv329+QEtJSQnmzZvn8IBERER0azYX8jfffBPLly/HBx98AC+vG9el77//fuzbt8+h4YiIiByFrzH9Q05Ozk2fg24wGHDlyhVHZCIiInI8N32ym8098vDwcOTm5tZZv2PHDrRqxVdTEhGRQvEaea0xY8ZgwoQJ2LNnDwRBwJkzZ7B69WpMmTIF48aNc0ZGIiIiugWbh9anT58OURTRt29fXL16Fb169YJWq8WUKVMwfvx4Z2QkIiKym7s+EMbmQi4IAl599VW8/PLLyM3NRVlZGWJiYuDv7++MfERERI7hpveR1/uBMN7e3oiJiXFkFiIiIrKRzYX8gQceuOWjVQHgxx9/tCsQERGRU9h7C5m79Mi7dOli8bm6uhrZ2dk4dOgQEhISHJWLiIjIsTi0Xuudd9656frXX38dZWVldgciIiIi69XrWes388wzz+Djjz921OGIiIgcy03vI3fY28927doFnR2vhyMiInIm3n72h6FDh1p8liQJZ8+eRWZmJmbMmOGwYERERHRnNhdyg8Fg8dnDwwNt27bFnDlz0K9fP4cFIyIiojuzqZCbTCaMGjUKHTt2RGBgoLMyEREROZ6bzlq3abKbRqNBv379+JYzIiJyOe76GlObZ63HxsYiPz/fGVmIiIjIRjYX8jfffBNTpkzBhg0bcPbsWRiNRouFiIhIsdzs1jPAhmvkc+bMweTJk/HII48AAAYNGmTxqFZJkiAIAkwmk+NTEhER2ctNr5FbXchnz56NF154AT/99JMz8xAREZENrC7kklT7p0jv3r2dFoaIiMhZ+EAY4LZvPSMiIlI0tQ+tA0CbNm3uWMyLi4vtCkRERETWs6mQz549u86T3YiIiFwBh9YBPPXUUwgNDXVWFiIiIudx06F1q+8j5/VxIiIi5bF51joREZFLctMeudWFXBRFZ+YgIiJyKl4jJ5cmlpbKHcElaBAmdwSXsHH9/8kdwSXEP/2c3BEUz1RTAWz/umFO5qY9cpuftU5ERETKwUJORETqYM8LU+rRm9+2bRsGDhyIiIgICIKAdevWWcaRJMycORNNmjSBj48P4uPjcfz4cZt/LRZyIiJShYZ+H3l5eTk6d+6MpUuX3vTnCxYswOLFi7F8+XLs2bMHfn5+6N+/PyoqKmw6D6+RExEROcGAAQMwYMCAm/5MkiQsWrQIr732GgYPHgwAWLVqFcLCwrBu3To89dRTVp+HPXIiIlIHBw2tG41Gi6WystLmKAUFBSgqKkJ8fLx5ncFgQPfu3bFr1y6bjsVCTkREquCoofWoqCgYDAbzkpqaanOWoqIiAEBYmOWdMmFhYeafWYtD60RERDYoLCxEQECA+bNWq5UxDXvkRESkFg4aWg8ICLBY6lPIw8PDAQDnzp2zWH/u3Dnzz6zFQk5EROrQwLef3U7Lli0RHh6OLVu2mNcZjUbs2bMHcXFxNh2LQ+tEREROUFZWhtzcXPPngoICZGdnIygoCM2aNcPEiRPx5ptvonXr1mjZsiVmzJiBiIgIDBkyxKbzsJATEZEqCH8s9uxvi8zMTDzwwAPmz8nJyQCAhIQEpKWlYerUqSgvL8fYsWNx5coV9OjRAxs3boROp7PpPCzkRESkDg38rPU+ffrc9s2hgiBgzpw5mDNnjh2hWMiJiEgl3PXtZ5zsRkRE5MLYIyciInVw09eYspATEZF6KLQY24ND60RERC6MPXIiIlIFd53sxkJORETq4KbXyDm0TkRE5MLYIyciIlXg0DoREZEr49A6ERERKQ175EREpAocWiciInJlbjq0zkJORETq4KaFnNfIiYiIXBh75EREpAq8Rk5EROTKOLRORERESsMeORERqYIgSRCk+ner7dnXmVjIHWzgyIt4fNx5BDWuQf4RH7z/WlPkZPvKHUtx2E6398igPDw6KB9h4VcBAL+fCMC/V7VH5i/hMieTz5r3QvHzt41QmKuFt05ETLerGP3qGURFV1psdyTTF2n/rwmO7fOFRgO06nAN89LzoPVR5pewsz07bD+eHZZtse7kGQOemzJUnkByctOhdVkL+bJly7Bs2TKcOHECANChQwfMnDkTAwYMkDNWvfUedBljZ53Be9MjcWyfLx4bcwFz0/MxumdblFzykjueYrCd7uziBR+s/CAWZ075QxCAvv1/x4w3d2L82HicPBEgdzxZHNzlj4EjL6JNl6sw1QBp85vgleF34YOMY9D5igBqi/irI+7CU0nn8OKbp6HRSMg/4gNB5RcRCwobYeq8/ubPJlHlDeJmZP1/MzIyEvPnz0dWVhYyMzPx4IMPYvDgwTh8+LCcsept6NiL2JgehO8/DcLJ4zosnhaJymsC+g8vljuaorCd7uyXXRHI3NMEZ07rcfqUHqs+ikXFNU+0i7kkdzTZzEvPR78ni9GibQXu6lCByYtO4vxpbxw/6GPeZsXrTTFk9AU8Of48WrStQFR0JXoPugJvrUK7Ug3EZPLA5RJf82Is1ckdSRbXZ63bsyiRrD3ygQMHWnyeO3culi1bht27d6NDhw4ypaofTy8RrTtdxZoloeZ1kiRg/3Y9YrpelTGZsrCdbOfhIaFH71PQ6Uw4ejhY7jiKUW7UAAD0jUwAgCsXPXFsnx8efOwyJg5sjbO/eyMquhIjp51FbPdyOaPKrmm4EWuWrkF1tQZHjofiozVdcf6Sv9yxGh6H1p3LZDLh888/R3l5OeLi4m66TWVlJSorb1wPMxqNDRXvjgKCTNB4AlcuWDbp5Yueda7hqRnbyXotWpbg7aU/wdtbxLVrnnhj5t9Q+Ls6h9X/ShSB5bOaosO9ZWjRrgIAcPZ3bwDAJwvDMWbGGdzV4Rp++CIQ05+8Cyt+PIamrarkjCybo7mN8daKHig8Y0Bw4DX8z9D9eGfmt3h+2mO4VsFLWe5A9kL+66+/Ii4uDhUVFfD398fatWsRExNz021TU1Mxe/bsBk5IJI9ThXokPR8PP/9q9Oh1GpOnZ2LqxN4s5gCWvBKJ34/54O11x83rxNrL5HjkmUvo/1TtZZrojteQvUOPTWuC8dwrZ+WIKru9ByLN/1xQCBzNDUH64s/R+28F2Li1jYzJGp67PhBG9hkPbdu2RXZ2Nvbs2YNx48YhISEBR44cuem2KSkpKCkpMS+FhYUNnPbWjMUamGqARo1rLNYHhtTg8gXZ/15SDLaT9WpqPHD2jD9yfwtE2oexyM8zYPCwXLljyW7JK02xZ3MAFnyRi8YR1eb1wWG1/041b1NhsX1UdAXOn2bP87ryq1qcOmtA0zDljGg2GMkBiwLJXsi9vb0RHR2Nrl27IjU1FZ07d8a777570221Wi0CAgIsFqWoqfbA8YO+uLtHqXmdIEjo0qMMR7J4W9V1bKf68xAALy9R7hiykaTaIr5zowELPs9FeDPLofKwqCoEh1fhVJ7WYv3pfC1CI6tBtXTaajQJM+LSFfX998bJbg1EFEWL6+Cu5Kt/hmDKokL8dsAXOftrb6vS+Yr4fk2Q3NEUhe10ZyOfP4TMX8Jw/pwvfH1r0KdvITp2uYAZU3vIHU02S16JxE9rA/H6ynz4+IsoPl/79eWnN0HrI0EQgMfHXcAn/xuOVjHX0KrDNfzweRAK83R47YMT8oaX0dinf8Hufc1w7qIfggOvIuHxbIiigJ92tpI7GjmIrIU8JSUFAwYMQLNmzVBaWor09HRs3boVmzZtkjNWvWWsD4Qh2IRnXy5CYOMa5B/2wasjWuLKRQ7r/Rnb6c4MgZWYnJKJoKAKlJd7oSA/ADOm9sD+rDC5o8lmw79CAAAvD2ttsX7yOyfR78naa+JDx1xAdYWA5bOaovSKBq1iKpD67zxEtFDnRDcAaBx8Fa+M34oA/0qUGHU49FsYxs/8L5So8RY0zlp3vPPnz+PZZ5/F2bNnYTAY0KlTJ2zatAkPPfSQnLHssn5lCNavDJE7huKxnW7v3be6yh1BcTadybZquyfHn8eT4887N4wLmfteH7kjKIpSh8ftIWsh/+ijj+Q8PRERkctT3DVyIiIip5Ck2sWe/RWIhZyIiFSB95ETERGR4rBHTkRE6sBZ60RERK5LEGsXe/ZXIg6tExERuTD2yImISB04tE5EROS63HXWOgs5ERGpg5veR85r5ERERC6MPXIiIlIFDq0TERG5Mjed7MahdSIiIhfGHjkREakCh9aJiIhcGWetExERkdKwR05ERKrAoXUiIiJXxlnrREREpDTskRMRkSpwaJ2IiMiViVLtYs/+CsRCTkRE6sBr5ERERKQ07JETEZEqCLDzGrnDkjgWCzkREakDn+xGRERESsMeORERqQJvPyMiInJlnLVORERESsNCTkREqiBIkt2LLV5//XUIgmCxtGvXzuG/F4fWif5EOnNO7ggu4Yn8vnJHcAk9390tdwTFqyyrxva/N9DJxD8We/a3UYcOHfDDDz+YP3t6Or7sspATERE5iaenJ8LDw516Dg6tExGRKjhqaN1oNFoslZWVtzzn8ePHERERgVatWmHEiBE4efKkw38vFnIiIlIHyQELgKioKBgMBvOSmpp609N1794daWlp2LhxI5YtW4aCggL07NkTpaWlDv21OLRORETq4KAnuxUWFiIgIMC8WqvV3nTzAQMGmP+5U6dO6N69O5o3b47PPvsMo0ePrn+Ov2AhJyIiskFAQIBFIbdWo0aN0KZNG+Tm5jo0D4fWiYhIFa4/2c2exR5lZWXIy8tDkyZNHPML/YGFnIiI1OH60Lo9iw2mTJmCjIwMnDhxAjt37sRjjz0GjUaD4cOHO/TX4tA6ERGRE5w6dQrDhw/HpUuX0LhxY/To0QO7d+9G48aNHXoeFnIiIlIFQaxd7NnfFmvWrKn/yWzAQk5EROrA95ETERGR0rBHTkRE6uCmrzFlISciIlWozxvM/rq/EnFonYiIyIWxR05EROrgppPdWMiJiEgdJNj3PnJl1nEWciIiUgdeIyciIiLFYY+ciIjUQYKd18gdlsShWMiJiEgd3HSyG4fWiYiIXBh75EREpA4iAMHO/RWIhZyIiFSBs9aJiIhIcdgjJyIidXDTyW4s5EREpA5uWsg5tE5EROTC2CMnIiJ1cNMeOQs5ERGpA28/IyIicl28/YyIiIgUhz1yBxs48iIeH3ceQY1rkH/EB++/1hQ52b5yx1IcttPtxXYrweOjTyE6thzBoVWY82J77NoSLHcs2dVkV6My/RpMOTWQLknwnaeHVy9v88/FYhEVy66i5pcqSGUSPDt7QTfJD5oojYypG9apjzxwaYsHrhUI8NACAV0kNJ9YA58WN7Yp+sIDF7/zQPlRAaZyAfdtr4JngGyRG46bXiNXTI98/vz5EAQBEydOlDtKvfUedBljZ53B6oXhSOzfBvlHdJibng9DcLXc0RSF7XRnOl8T8nP88f7sVnJHURTpmgRNtCd8kv3q/kyScDWlFOIZE3znB8B/ZSN4hHugfKIR0jVlfgE7gzHTA02eFNHpkxp0WFEDsQY4/IIXTFdvbCNWAI3+LqLpaJN8QeUgSvYvCqSIQr53716sWLECnTp1kjuKXYaOvYiN6UH4/tMgnDyuw+Jpkai8JqD/8GK5oykK2+nOMrcFYdWi5tj5Q4jcURTFK84burG+8OqtrfMzsVCE6XANfCb7wbO9JzTNNNBN8QMqJVT/UClDWnnELKtB6GARvtES/NpKaD2nBlVnBZQdvTHLK+IZEZGjReg7KbMwkW1kL+RlZWUYMWIEPvjgAwQGBsodp948vUS07nQV+7brzeskScD+7XrEdL16mz3Vhe1ETlP9R1HS3ihYgocAeAuoOVgjUyj51ZTV/q8qhs7v5PrQuj2LAsleyBMTE/Hoo48iPj7+jttWVlbCaDRaLEoREGSCxhO4csFy2sHli54IbKzeL5G/YjuRs3g010AI80Dl8quQjCKkagmV/3cN0nkR0iWF3jfkZJIInFjgCX0XEX6tlVmEGpa9RVyZbSjrZLc1a9Zg37592Lt3r1Xbp6amYvbs2U5ORUSuSPAU4DdXj6vzy2B85DKgATy7esHzb15K/f51uvx5GlzNExCbxvkn7ky2Ql5YWIgJEyZg8+bN0Ol0Vu2TkpKC5ORk82ej0YioqChnRbSJsVgDUw3Q6C+9ysCQGly+wJsDrmM7kTNp2nlCn9YIUpkIqRrwCPRA2ZgSaNqpZ9b6dfnzNLi8zQOxH1dDGyZ3GoXgrHXHysrKwvnz53HPPffA09MTnp6eyMjIwOLFi+Hp6QmTqe5sSq1Wi4CAAItFKWqqPXD8oC/u7lFqXicIErr0KMORLN5WdR3biRqC4O8Bj0APmApNMOXUwLOn9513chOSVFvEi3/0QIcPqqGLlDuRgrjprHXZukB9+/bFr7/+arFu1KhRaNeuHaZNmwaNxvX+gv7qnyGYsqgQvx3wRc5+Xzw25gJ0viK+XxMkdzRFYTvdmc7XhIhm18yfwyIr0KpdGUpLPHHhrHUjWO5IuipBPH3jj3zxrAmm4zUQ9AI8wjWo/rESQiMPeIR5wJRvwrV3y+HZ0xte96mnkOfP0+Didx5ot6gGGj+g6mLteo0/oPnjX52qi0D1RQEVhbUTA6/mCtD4At5NJHgZZApO9SZbIdfr9YiNjbVY5+fnh+Dg4DrrXUXG+kAYgk149uUiBDauQf5hH7w6oiWuXPSSO5qisJ3urHVsKRZ8csj8+R+vFAAANn8VioUpbeSKJTvTsRqUv3RjkmvFe7V3OngN0ML3VX+Il0RULrkKqViEEOwB74e10I70kSuuLM59VtsJOjza8r+n6Dm1t6UBQNHnGpxafqOzdGiUV51t3JIk1i727K9AvCjpYOtXhmD9St77eydsp9v79ZdGGNC2h9wxFMfzHi8Ydtz6CXfa//aB9r/VVbj/6u8Hqu64TbNxJjQbp7KHwQBue41cUYV869atckcgIiJ3Jdp5C5lCr5HLfh85ERER1Z+ieuREREROw6F1IiIiFybBzkLusCQOxaF1IiIiF8YeORERqQOH1omIiFyYKAKw415wUZn3kXNonYiIyIWxR05EROrAoXUiIiIX5qaFnEPrRERELow9ciIiUgc3fUQrCzkREamCJImQ7HiDmT37OhMLORERqYMk2der5jVyIiIicjT2yImISB0kO6+RK7RHzkJORETqIIqAYMd1boVeI+fQOhERkQtjj5yIiNSBQ+tERESuSxJFSHYMrSv19jMOrRMREbkw9siJiEgdOLRORETkwkQJENyvkHNonYiIyIWxR05EROogSQDsuY9cmT1yFnIiIlIFSZQg2TG0LrGQExERyUgSYV+PnLefERERqc7SpUvRokUL6HQ6dO/eHb/88otDj89CTkREqiCJkt2LrT799FMkJydj1qxZ2LdvHzp37oz+/fvj/PnzDvu9WMiJiEgdJNH+xUYLFy7EmDFjMGrUKMTExGD58uXw9fXFxx9/7LBfy6WvkV+feFCDarvu8Se6zkOqkjuCS6gulzuBa6gUquWOoHiV5bVt1BATyeytFTWozWo0Gi3Wa7VaaLXaOttXVVUhKysLKSkp5nUeHh6Ij4/Hrl276h/kL1y6kJeWlgIAduBbmZOQ2yiVO4CL6Cd3ANewXu4ALqS0tBQGg8Epx/b29kZ4eDh2FNlfK/z9/REVFWWxbtasWXj99dfrbHvx4kWYTCaEhYVZrA8LC8OxY8fsznKdSxfyiIgIFBYWQq/XQxAEueMAqP1LLSoqCoWFhQgICJA7jmKxnazDdrIO28k6SmwnSZJQWlqKiIgIp51Dp9OhoKAAVVX2j7hJklSn3tysN96QXLqQe3h4IDIyUu4YNxUQEKCY/1CUjO1kHbaTddhO1lFaOzmrJ/5nOp0OOp3O6ef5s5CQEGg0Gpw7d85i/blz5xAeHu6w83CyGxERkRN4e3uja9eu2LJli3mdKIrYsmUL4uLiHHYel+6RExERKVlycjISEhLQrVs33HfffVi0aBHKy8sxatQoh52DhdzBtFotZs2aJfs1E6VjO1mH7WQdtpN12E4N78knn8SFCxcwc+ZMFBUVoUuXLti4cWOdCXD2ECSlPjyWiIiI7ojXyImIiFwYCzkREZELYyEnIiJyYSzkRERELoyF3MGc/bo6V7dt2zYMHDgQEREREAQB69atkzuSIqWmpuLee++FXq9HaGgohgwZgpycHLljKc6yZcvQqVMn8wNO4uLi8N1338kdS9Hmz58PQRAwceJEuaOQg7CQO1BDvK7O1ZWXl6Nz585YunSp3FEULSMjA4mJidi9ezc2b96M6upq9OvXD+XlfFvJn0VGRmL+/PnIyspCZmYmHnzwQQwePBiHDx+WO5oi7d27FytWrECnTp3kjkIOxNvPHKh79+649957sWTJEgC1T/CJiorC+PHjMX36dJnTKY8gCFi7di2GDBkidxTFu3DhAkJDQ5GRkYFevXrJHUfRgoKC8NZbb2H06NFyR1GUsrIy3HPPPXj//ffx5ptvokuXLli0aJHcscgB2CN3kOuvq4uPjzevc8br6kidSkpKANQWKbo5k8mENWvWoLy83KGPv3QXiYmJePTRRy2+o8g98MluDtJQr6sj9RFFERMnTsT999+P2NhYueMozq+//oq4uDhUVFTA398fa9euRUxMjNyxFGXNmjXYt28f9u7dK3cUcgIWciKFS0xMxKFDh7Bjxw65oyhS27ZtkZ2djZKSEnzxxRdISEhARkYGi/kfCgsLMWHCBGzevLnB3/5FDYOF3EEa6nV1pC5JSUnYsGEDtm3bpthX9srN29sb0dHRAICuXbti7969ePfdd7FixQqZkylDVlYWzp8/j3vuuce8zmQyYdu2bViyZAkqKyuh0WhkTEj24jVyB2mo19WROkiShKSkJKxduxY//vgjWrZsKXcklyGKIiorK+WOoRh9+/bFr7/+iuzsbPPSrVs3jBgxAtnZ2SziboA9cgdqiNfVubqysjLk5uaaPxcUFCA7OxtBQUFo1qyZjMmUJTExEenp6fj666+h1+tRVFQEADAYDPDx8ZE5nXKkpKRgwIABaNasGUpLS5Geno6tW7di06ZNckdTDL1eX2duhZ+fH4KDgznnwk2wkDtQQ7yuztVlZmbigQceMH9OTk4GACQkJCAtLU2mVMqzbNkyAECfPn0s1q9cuRIjR45s+EAKdf78eTz77LM4e/YsDAYDOnXqhE2bNuGhhx6SOxpRg+F95ERERC6M18iJiIhcGAs5ERGRC2MhJyIicmEs5ERERC6MhZyIiMiFsZATERG5MBZyIiIiF8ZCTkRE5MJYyInsNHLkSAwZMsT8uU+fPpg4cWKD59i6dSsEQcCVK1duuY0gCFi3bp3Vx3z99dfRpUsXu3KdOHECgiAgOzvbruMQ0c2xkJNbGjlyJARBgCAI5rdjzZkzBzU1NU4/91dffYU33njDqm2tKb5ERLfDZ62T23r44YexcuVKVFZW4ttvv0ViYiK8vLyQkpJSZ9uqqip4e3s75LxBQUEOOQ4RkTXYIye3pdVqER4ejubNm2PcuHGIj4/H+vXrAdwYDp87dy4iIiLQtm1bAEBhYSGeeOIJNGrUCEFBQRg8eDBOnDhhPqbJZEJycjIaNWqE4OBgTJ06FX99XcFfh9YrKysxbdo0REVFQavVIjo6Gh999BFOnDhhfoFMYGAgBEEwvxBFFEWkpqaiZcuW8PHxQefOnfHFF19YnOfbb79FmzZt4OPjgwceeMAip7WmTZuGNm3awNfXF61atcKMGTNQXV1dZ7sVK1YgKioKvr6+eOKJJ1BSUmLx8w8//BDt27eHTqdDu3bt8P7779uchYjqh4WcVMPHxwdVVVXmz1u2bEFOTg42b96MDRs2oLq6Gv3794der8f27dvx888/w9/fHw8//LB5v7fffhtpaWn4+OOPsWPHDhQXF2Pt2rW3Pe+zzz6Lf//731i8eDGOHj2KFStWwN/fH1FRUfjyyy8BADk5OTh79izeffddAEBqaipWrVqF5cuX4/Dhw5g0aRKeeeYZZGRkAKj9g2Po0KEYOHAgsrOz8fzzz2P69Ok2t4ler0daWhqOHDmCd999Fx988AHeeecdi21yc3Px2Wef4ZtvvsHGjRuxf/9+vPjii+afr169GjNnzsTcuXNx9OhRzJs3DzNmzMC//vUvm/MQUT1IRG4oISFBGjx4sCRJkiSKorR582ZJq9VKU6ZMMf88LCxMqqysNO/zySefSG3btpVEUTSvq6yslHx8fKRNmzZJkiRJTZo0kRYsWGD+eXV1tRQZGWk+lyRJUu/evaUJEyZIkiRJOTk5EgBp8+bNN835008/SQCky5cvm9dVVFRIvr6+0s6dOy22HT16tDR8+HBJkiQpJSVFiomJsfj5tGnT6hzrrwBIa9euveXP33rrLalr167mz7NmzZI0Go106tQp87rvvvtO8vDwkM6ePStJkiTdddddUnp6usVx3njjDSkuLk6SJEkqKCiQAEj79++/5XmJqP54jZzc1oYNG+Dv74/q6mqIooinn34ar7/+uvnnHTt2tLgufuDAAeTm5kKv11scp6KiAnl5eSgpKcHZs2fRvXt38888PT3RrVu3OsPr12VnZ0Oj0aB3795W587NzcXVq1frvFO7qqoKd999NwDg6NGjFjkAIC4uzupzXPfpp59i8eLFyMvLQ1lZGWpqahAQEGCxTbNmzdC0aVOL84iiiJycHOj1euTl5WH06NEYM2aMeZuamhoYDAab8xCR7VjIyW098MADWLZsGby9vREREQFPT8t/3f38/Cw+l5WVoWvXrli9enWdYzVu3LheGXx8fGzep6ysDADwn//8x6KAArXX/R1l165dGDFiBGbPno3+/fvDYDBgzZo1ePvtt23O+sEHH9T5w0Kj0TgsKxHdGgs5uS0/Pz9ER0dbvf0999yDTz/9FKGhoXV6pdc1adIEe/bsQa9evQDU9jyzsrJwzz333HT7jh07QhRFZGRkID4+vs7Pr48ImEwm87qYmBhotVqcPHnylj359u3bmyfuXbd79+47/5J/snPnTjRv3hyvvvqqed3vv/9eZ7uTJ0/izJkziIiIMJ/Hw8MDbdu2RVhYGCIiIpCfn48RI0bYdH4icgxOdiP6w4gRIxASEoLBgwdj+/btKCgowNatW/HSSy/h1KlTAIAJEyZg/vz5WLduHY4dO4YXX3zxtveAt2jRAgkJCXjuueewbt068zE/++wzAEDz5s0hCAI2bNiACxcuoKysDHq9HlOmTMGkSZPwr3/9C3l5edi3bx/ee+898wSyF154AcePH8fLL7+MnJwcpKenIy0tzabft3Xr1jh58iTWrFmDvLw8LF68+KYT93Q6HRISEnDgwAFs374dL730Ep544gmEh4cDAGbPno3U1FQsXrwYv/32G3799VesXLkSCxcutCkPEdUPCznRH3x9fbFt2zY0a9YMQ4cORfv27TF69GhUVFSYe+iTJ0/G//zP/yAhIQFxcXHQ6/V47LHHbnvcZcuW4fHHH8eLL76Idu3aYcyYMSgvLwcANG3aFLNnz8b06dMRFhaGpKQkAMAbb7yBGTNmIDU1Fe3bt8fDDz+M//znP2jZsiWA2uvWX375JdatW4fOnTtj+fLlmDdvnk2/76BBgzBp0iQkJSWhS5cu2LlzJ2bMmFFnu+joaAwdOhSPPPII+vXrh06dOlncXvb888/jww8/xMqVK9GxY0f07t0baWlp5qxE5FyCdKtZOkRERKR47JETERG5MBZyIiIiF8ZCTkRE5MJYyImIiFwYCzkREZELYyEnIiJyYSzkRERELoyFnIiIyIWxkBMREbkwFnIiIiIXxkJORETkwv4/wOc57ACtO30AAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.59\n",
      "Recall score: 0.59\n",
      "Precision score: 0.6663715332286761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91         5\n",
      "           1       0.80      0.33      0.47        12\n",
      "           2       0.21      0.38      0.27         8\n",
      "           3       0.53      0.76      0.63        34\n",
      "           4       0.81      0.51      0.63        41\n",
      "\n",
      "    accuracy                           0.59       100\n",
      "   macro avg       0.64      0.60      0.58       100\n",
      "weighted avg       0.67      0.59      0.59       100\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Klasyfikacja za pomocą dostrojonego modelu BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Szybkie przeuczenie, ale na ogół dobre wyniki\n",
    "\n",
    "Testowane modyfikacje:\n",
    "- Mniejszy model - przeuczenie następuje wolniej, ale nie osiąga lepszych wyników\n",
    "- Regularyzacja L2 - nie jest w stanie zapobiec przeuczeniu\n",
    "- Regresja zamiast klasyfikacji - dużo wolniejsze przeuczenie, wyniki podobne bądź nieco gorsze\n",
    "- Model RoBERTa - podobne wyniki\n",
    "- Preferencje dla klas mniejszościowych w `DataLoader` - początkowo lepszy balans accuracy między klasami, ale w późnych epokach zawsze dąży rozkładu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_t = str | bytes | os.PathLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = Path(\"./data/train_data.csv\")\n",
    "NLTK_DIR = Path(\"./nltk\")\n",
    "MODEL_DIR = Path(\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(TRAIN_PATH)\n",
    "sns.histplot(df_raw[\"rating\"])\n",
    "cls_counts = np.histogram(df_raw[\"rating\"], bins=5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Path_t,\n",
    "        tokenizer: BertTokenizer,\n",
    "        batch_size: int,\n",
    "        val_fraction: float = 0.0,\n",
    "        truncation_strategy: Literal[\"truncate\"] = \"truncate\",\n",
    "        max_seq_len: int = 512,\n",
    "        clean_fn: Callable[[str], str] = lambda x: x,\n",
    "        n_first: int | None = None,  # just for testing purposes\n",
    "    ):\n",
    "        super(TextClassificationDataModule, self).__init__()\n",
    "        assert 0 <= val_fraction <= 1\n",
    "\n",
    "        self._data_path = data_path\n",
    "        self._clean_fn = clean_fn\n",
    "        self._tokenizer = tokenizer\n",
    "        self._encode_fn = partial(\n",
    "            tokenizer.encode_plus,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_seq_len,\n",
    "            truncation=True if truncation_strategy == \"truncate\" else False,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        self.val_fraction = val_fraction\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self._n_first = n_first\n",
    "\n",
    "        self._encoded: list[dict[str, Tensor]]\n",
    "        self._train: Dataset\n",
    "        self._val: Dataset\n",
    "        self._test: Dataset\n",
    "\n",
    "    def setup(self, stage: Literal[\"fit\", \"test\"]) -> None:\n",
    "        if stage == \"fit\":\n",
    "            df_raw = pd.read_csv(self._data_path)\n",
    "            reviews = df_raw[\"review\"].apply(self._clean_fn).values[: self._n_first]\n",
    "            self._encoded = [self._encode_fn(review) for review in reviews]\n",
    "            ratings = df_raw[\"rating\"].values[: self._n_first]\n",
    "            dataset = TensorDataset(\n",
    "                *(\n",
    "                    torch.cat([item[k] for item in self._encoded], dim=0)\n",
    "                    for k in (\"input_ids\", \"attention_mask\")\n",
    "                ),\n",
    "                torch.tensor(ratings),\n",
    "            )\n",
    "            val_size = int(len(dataset) * self.val_fraction)\n",
    "            train_size = len(dataset) - val_size\n",
    "\n",
    "            self._train, self._val = torch.utils.data.random_split(\n",
    "                dataset, [train_size, val_size]\n",
    "            )\n",
    "\n",
    "        if stage == \"test\":\n",
    "            df_raw = pd.read_csv(self._data_path, usecols=[0], names=['review'], header=None)\n",
    "            reviews = df_raw[\"review\"].apply(self._clean_fn).values[: self._n_first]\n",
    "            self._encoded = [self._encode_fn(review) for review in reviews]\n",
    "            self._test = TensorDataset(\n",
    "                *(\n",
    "                    torch.cat([item[k] for item in self._encoded], dim=0)\n",
    "                    for k in (\"input_ids\", \"attention_mask\")\n",
    "                )\n",
    "            )\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            # self._train, batch_size=self.batch_size, pin_memory=True, sampler=WeightedRandomSampler(weights=1/cls_counts, num_samples=len(self._train), replacement=True)\n",
    "            self._train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self._val, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self._test, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self._test, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BertForSequenceClassification,\n",
    "        lr: float,\n",
    "        weight_decay: float = 0.0,\n",
    "        warmup_steps: int = 0,\n",
    "        freeze_encoder: bool = False,\n",
    "    ):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        num_classes = model.num_labels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.model = model\n",
    "        self._lr = lr\n",
    "        self._weight_decay = weight_decay\n",
    "        self._warmup_steps = warmup_steps\n",
    "        self._freeze_encoder = freeze_encoder\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for param in self.model.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.flat_metrics = {\n",
    "            \"accuracy\": MulticlassAccuracy(num_classes=num_classes, average=\"micro\").to(\n",
    "                device\n",
    "            ),\n",
    "            \"precision\": MulticlassPrecision(\n",
    "                num_classes=num_classes, average=\"micro\"\n",
    "            ).to(device),\n",
    "            \"recall\": MulticlassRecall(num_classes=num_classes, average=\"micro\").to(\n",
    "                device\n",
    "            ),\n",
    "            \"f1\": MulticlassF1Score(num_classes=num_classes, average=\"micro\").to(\n",
    "                device\n",
    "            ),\n",
    "        }\n",
    "        self.class_metrics = {\n",
    "            \"accuracy\": MulticlassAccuracy(num_classes=num_classes, average=\"none\").to(\n",
    "                device\n",
    "            ),\n",
    "            \"precision\": MulticlassPrecision(\n",
    "                num_classes=num_classes, average=\"none\"\n",
    "            ).to(device),\n",
    "            \"recall\": MulticlassRecall(num_classes=num_classes, average=\"none\").to(\n",
    "                device\n",
    "            ),\n",
    "            \"f1\": MulticlassF1Score(num_classes=num_classes, average=\"none\").to(device),\n",
    "        }\n",
    "        self.confusion_matrix = MulticlassConfusionMatrix(\n",
    "            num_classes=num_classes, normalize=\"pred\"\n",
    "        ).to(device)\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        self.trainer.fit_loop.setup_data()\n",
    "        total_devices = getattr(self.hparams, \"n_gpus\", 1) * getattr(\n",
    "            self.hparams, \"n_nodes\", 1\n",
    "        )\n",
    "        train_batches = len(self.trainer.train_dataloader) // total_devices\n",
    "        train_steps = (\n",
    "            self.trainer.max_epochs * train_batches\n",
    "        ) // self.trainer.accumulate_grad_batches\n",
    "        # will not work with AdamW (plus it is deprecated anyway)\n",
    "        # https://discuss.huggingface.co/t/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-grad-fn/47965/2\n",
    "        optimizer = Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self._lr,\n",
    "            eps=1e-8,\n",
    "            weight_decay=self._weight_decay,\n",
    "        )\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=self._warmup_steps,\n",
    "            num_training_steps=train_steps,\n",
    "        )\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> dict[str, Tensor]:\n",
    "        input_ids, input_mask, labels = batch\n",
    "        output = self.model(\n",
    "            input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels\n",
    "        )\n",
    "        loss = output.loss\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> dict[str, Tensor]:\n",
    "        with torch.no_grad():\n",
    "            input_ids, input_mask, labels = batch\n",
    "            output = self.model(\n",
    "                input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels\n",
    "            )\n",
    "            loss = output.loss\n",
    "\n",
    "            preds = output.logits\n",
    "            self.log(\"val_loss\", loss, prog_bar=True)\n",
    "            self.log_dict(\n",
    "                {\n",
    "                    name: metric(preds, labels)\n",
    "                    for name, metric in self.flat_metrics.items()\n",
    "                },\n",
    "                prog_bar=True,\n",
    "            )\n",
    "            self.log_dict(\n",
    "                {\n",
    "                    f\"{name}_{c}\": v\n",
    "                    for name, metric_values in {\n",
    "                        _name: metric(preds, labels)\n",
    "                        for _name, metric in self.class_metrics.items()\n",
    "                    }.items()\n",
    "                    for c, v in enumerate(metric_values)\n",
    "                },\n",
    "                prog_bar=True,\n",
    "            )\n",
    "            self.confusion_matrix(preds, labels)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        with torch.no_grad():\n",
    "            confusion_matrix = self.confusion_matrix.compute()\n",
    "\n",
    "            df_cm = pd.DataFrame(\n",
    "                confusion_matrix.cpu().numpy(), index=range(5), columns=range(5)\n",
    "            )\n",
    "            fig, ax = plt.subplots(figsize=(10, 7))\n",
    "            sns.heatmap(\n",
    "                df_cm,\n",
    "                ax=ax,\n",
    "                annot=True,\n",
    "                cmap=\"Greens\",\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                annot_kws={\"size\": 6},\n",
    "                fmt=\".3f\",\n",
    "            )\n",
    "            ax.set_xlabel(\"True class\")\n",
    "            ax.set_ylabel(\"Predicted class\")\n",
    "            self.logger.experiment.add_figure(\n",
    "                \"Confusion matrix\", fig, self.current_epoch\n",
    "            )\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any:\n",
    "        with torch.no_grad():\n",
    "            input_ids, input_mask = batch\n",
    "            return self.model(\n",
    "                input_ids, token_type_ids=None, attention_mask=input_mask\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"distilbert/distilbert-base-uncased\",\n",
    "# \"prajjwal1/bert-medium\",\n",
    "# \"prajjwal1/bert-small\",\n",
    "# \"prajjwal1/bert-mini\",\n",
    "# \"bert-large-uncased-whole-word-masking\",\n",
    "# \"bert-base-uncased\",\n",
    "# \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "# \"bert-large-uncased\",\n",
    "\n",
    "bert_version = \"bert-base-uncased\"\n",
    "batch_size = 32\n",
    "lr = 5e-5\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    bert_version, do_lower_case=True, cache_dir=MODEL_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\", download_dir=NLTK_DIR)\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
    "    punctuations = \"@#!?+&*[]-%.:/();$=><|{}^\" + \"'`\" + \"_\"\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, \"\")  # Removing punctuations\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TextClassificationDataModule(\n",
    "    data_path=Path(TRAIN_PATH),\n",
    "    tokenizer=tokenizer,\n",
    "    val_fraction=0.2,\n",
    "    batch_size=batch_size,\n",
    "    clean_fn=clean_text,\n",
    "    max_seq_len=512,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (model): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  )\n",
       "  (confusion_matrix): MulticlassConfusionMatrix()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertClassifier(\n",
    "    model=BertForSequenceClassification.from_pretrained(\n",
    "        bert_version,\n",
    "        num_labels=5,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        cache_dir=MODEL_DIR,\n",
    "    ),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=0,\n",
    "    freeze_encoder=False,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model            │ BertForSequenceClassification │  109 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ confusion_matrix │ MulticlassConfusionMatrix     │      0 │\n",
       "└───┴──────────────────┴───────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001B[1;35m \u001B[0m\u001B[1;35m \u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35mName            \u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35mType                         \u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35mParams\u001B[0m\u001B[1;35m \u001B[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001B[2m \u001B[0m\u001B[2m0\u001B[0m\u001B[2m \u001B[0m│ model            │ BertForSequenceClassification │  109 M │\n",
       "│\u001B[2m \u001B[0m\u001B[2m1\u001B[0m\u001B[2m \u001B[0m│ confusion_matrix │ MulticlassConfusionMatrix     │      0 │\n",
       "└───┴──────────────────┴───────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 109 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 109 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 437                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1mTrainable params\u001B[0m: 109 M                                                                                            \n",
       "\u001B[1mNon-trainable params\u001B[0m: 0                                                                                            \n",
       "\u001B[1mTotal params\u001B[0m: 109 M                                                                                                \n",
       "\u001B[1mTotal estimated model params size (MB)\u001B[0m: 437                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a58ec9e2eb499fa2bbed1d572f1633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connecto\n",
       "r.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a \n",
       "bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on \n",
       "this machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connecto\n",
       "r.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a \n",
       "bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on \n",
       "this machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "20 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "20 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning:\n",
       "Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning:\n",
       "Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_val_acc_callback = ModelCheckpoint(\n",
    "    monitor=\"accuracy\",\n",
    "    filename=\"checkpoint_best_acc-{epoch:02d}-{step:03d}-{accuracy:.5f}\",\n",
    "    save_top_k=3,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    gradient_clip_val=1,\n",
    "    callbacks=[\n",
    "        best_val_acc_callback,\n",
    "        RichProgressBar()\n",
    "    ],\n",
    "    precision=\"bf16-mixed\",\n",
    "    val_check_interval=0.25,\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = Path(\"./data/test_data.csv\")\n",
    "test_dm = TextClassificationDataModule(\n",
    "    data_path=TEST_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=1,\n",
    "    clean_fn=clean_text\n",
    ")\n",
    "test_dm.prepare_data()\n",
    "test_dm.setup(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb3a2326574473799c3428092813537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.9219, -1.1562,  3.2656,  3.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.9219, -1.1562,  3.2656,  3.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.1250,  2.4531,  1.1016, -1.7578, -3.0938]], dtype=torch.bfloat16))]), logits=tensor([[ 2.1250,  2.4531,  1.1016, -1.7578, -3.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3125, -0.9688,  1.4688,  2.4844, -0.2070]], dtype=torch.bfloat16))]), logits=tensor([[-3.3125, -0.9688,  1.4688,  2.4844, -0.2070]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5156, -2.9688, -1.5000,  3.0156,  3.8594]], dtype=torch.bfloat16))]), logits=tensor([[-3.5156, -2.9688, -1.5000,  3.0156,  3.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -3.1562, -1.9219,  2.9062,  4.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -3.1562, -1.9219,  2.9062,  4.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 4.0312,  2.0312, -0.1768, -2.2656, -2.4219]], dtype=torch.bfloat16))]), logits=tensor([[ 4.0312,  2.0312, -0.1768, -2.2656, -2.4219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.0029,  0.3086, -0.5977, -0.5312, -0.0518]], dtype=torch.bfloat16))]), logits=tensor([[-0.0029,  0.3086, -0.5977, -0.5312, -0.0518]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4062, -3.0625, -1.8984,  2.9219,  4.3438]], dtype=torch.bfloat16))]), logits=tensor([[-3.4062, -3.0625, -1.8984,  2.9219,  4.3438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.8672,  2.1406,  1.2812, -1.0391, -2.8750]], dtype=torch.bfloat16))]), logits=tensor([[ 0.8672,  2.1406,  1.2812, -1.0391, -2.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -2.5469, -0.9727,  2.8906,  3.1094]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -2.5469, -0.9727,  2.8906,  3.1094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -0.8555,  1.0547,  2.1094,  0.1006]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -0.8555,  1.0547,  2.1094,  0.1006]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0938, -0.5508,  1.7031,  1.9531, -0.6445]], dtype=torch.bfloat16))]), logits=tensor([[-3.0938, -0.5508,  1.7031,  1.9531, -0.6445]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.6289,  1.8047,  0.9375, -0.7930, -2.2969]], dtype=torch.bfloat16))]), logits=tensor([[ 0.6289,  1.8047,  0.9375, -0.7930, -2.2969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -0.9609,  1.5938,  2.2969, -0.3262]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -0.9609,  1.5938,  2.2969, -0.3262]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0000, -2.7031, -2.1875,  2.0000,  4.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.0000, -2.7031, -2.1875,  2.0000,  4.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -3.0625, -1.7344,  3.0469,  4.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -3.0625, -1.7344,  3.0469,  4.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.2812,  2.4375,  0.3281, -2.2031, -2.8438]], dtype=torch.bfloat16))]), logits=tensor([[ 3.2812,  2.4375,  0.3281, -2.2031, -2.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.6211,  1.5859,  1.5312, -0.0376, -2.3594]], dtype=torch.bfloat16))]), logits=tensor([[-0.6211,  1.5859,  1.5312, -0.0376, -2.3594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.9688, -1.5469,  2.9531,  3.9531]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.9688, -1.5469,  2.9531,  3.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0781, -1.0156,  0.7539,  2.0469,  0.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.0781, -1.0156,  0.7539,  2.0469,  0.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7500, -3.0000, -2.4375,  2.2031,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.7500, -3.0000, -2.4375,  2.2031,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.4531,  2.2656,  0.5781, -1.7578, -2.7656]], dtype=torch.bfloat16))]), logits=tensor([[ 2.4531,  2.2656,  0.5781, -1.7578, -2.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -2.6406, -1.3906,  2.4375,  3.8594]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -2.6406, -1.3906,  2.4375,  3.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.6562,  2.5781,  0.9375, -2.0156, -3.1875]], dtype=torch.bfloat16))]), logits=tensor([[ 2.6562,  2.5781,  0.9375, -2.0156, -3.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8906, -3.0000, -2.4219,  2.2812,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.8906, -3.0000, -2.4219,  2.2812,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9219, -0.8984,  0.1338,  1.4922,  1.1719]], dtype=torch.bfloat16))]), logits=tensor([[-2.9219, -0.8984,  0.1338,  1.4922,  1.1719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5469, -1.0938, -0.7070,  1.1562,  2.0938]], dtype=torch.bfloat16))]), logits=tensor([[-2.5469, -1.0938, -0.7070,  1.1562,  2.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6250, -2.9062, -2.4844,  2.0000,  5.2500]], dtype=torch.bfloat16))]), logits=tensor([[-2.6250, -2.9062, -2.4844,  2.0000,  5.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.8398,  1.6797,  1.9609, -0.0933, -2.6406]], dtype=torch.bfloat16))]), logits=tensor([[-0.8398,  1.6797,  1.9609, -0.0933, -2.6406]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.3203,  0.9844,  1.3516,  0.3613, -1.6484]], dtype=torch.bfloat16))]), logits=tensor([[-1.3203,  0.9844,  1.3516,  0.3613, -1.6484]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -2.9844, -2.3281,  2.3438,  5.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -2.9844, -2.3281,  2.3438,  5.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.1338,  2.1562,  1.7891, -0.6523, -3.0625]], dtype=torch.bfloat16))]), logits=tensor([[ 0.1338,  2.1562,  1.7891, -0.6523, -3.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -2.9375, -1.6406,  2.6250,  4.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -2.9375, -1.6406,  2.6250,  4.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.1328,  1.6406,  2.2031,  0.0552, -2.8594]], dtype=torch.bfloat16))]), logits=tensor([[-1.1328,  1.6406,  2.2031,  0.0552, -2.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -1.4453,  0.7344,  2.5938,  0.8633]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -1.4453,  0.7344,  2.5938,  0.8633]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8125, -1.5547, -0.8164,  1.4688,  2.4844]], dtype=torch.bfloat16))]), logits=tensor([[-2.8125, -1.5547, -0.8164,  1.4688,  2.4844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -2.9375, -2.2656,  2.3906,  4.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -2.9375, -2.2656,  2.3906,  4.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.6875,  2.3438,  1.1875, -1.5078, -3.0625]], dtype=torch.bfloat16))]), logits=tensor([[ 1.6875,  2.3438,  1.1875, -1.5078, -3.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7969, -2.7969, -2.3750,  1.9766,  5.0938]], dtype=torch.bfloat16))]), logits=tensor([[-2.7969, -2.7969, -2.3750,  1.9766,  5.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -2.0469, -0.5625,  2.3438,  2.5156]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -2.0469, -0.5625,  2.3438,  2.5156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7031, -0.3027,  1.0469,  1.4844, -0.2578]], dtype=torch.bfloat16))]), logits=tensor([[-2.7031, -0.3027,  1.0469,  1.4844, -0.2578]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8281, -3.0312, -2.4531,  2.3281,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.8281, -3.0312, -2.4531,  2.3281,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -3.2031, -1.8594,  3.1250,  4.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -3.2031, -1.8594,  3.1250,  4.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -1.1406,  1.3594,  2.5625,  0.0171]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -1.1406,  1.3594,  2.5625,  0.0171]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.4062,  2.2188,  0.0767, -2.1094, -2.6094]], dtype=torch.bfloat16))]), logits=tensor([[ 3.4062,  2.2188,  0.0767, -2.1094, -2.6094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -3.0469, -2.0781,  2.6875,  4.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -3.0469, -2.0781,  2.6875,  4.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.3574,  2.4219,  2.0312, -1.0703, -3.5625]], dtype=torch.bfloat16))]), logits=tensor([[ 0.3574,  2.4219,  2.0312, -1.0703, -3.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6719, -2.7969, -1.3516,  2.9219,  3.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.6719, -2.7969, -1.3516,  2.9219,  3.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.7188, -0.8516,  3.2500,  3.0781]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.7188, -0.8516,  3.2500,  3.0781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -3.0156, -1.8047,  2.8594,  4.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -3.0156, -1.8047,  2.8594,  4.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8594, -1.0547, -0.0217,  1.5703,  1.3281]], dtype=torch.bfloat16))]), logits=tensor([[-2.8594, -1.0547, -0.0217,  1.5703,  1.3281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9375, -0.5664,  1.2656,  1.9688, -0.3457]], dtype=torch.bfloat16))]), logits=tensor([[-2.9375, -0.5664,  1.2656,  1.9688, -0.3457]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8906, -2.4375, -0.5312,  3.0938,  2.7031]], dtype=torch.bfloat16))]), logits=tensor([[-3.8906, -2.4375, -0.5312,  3.0938,  2.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.4590,  1.9844,  1.3672, -0.6875, -2.7031]], dtype=torch.bfloat16))]), logits=tensor([[ 0.4590,  1.9844,  1.3672, -0.6875, -2.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -2.8281, -1.2344,  2.9375,  3.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -2.8281, -1.2344,  2.9375,  3.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.6328,  1.7500,  1.8516, -0.1777, -2.6719]], dtype=torch.bfloat16))]), logits=tensor([[-0.6328,  1.7500,  1.8516, -0.1777, -2.6719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7500, -1.5312, -0.7422,  1.8594,  2.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7500, -1.5312, -0.7422,  1.8594,  2.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4531, -3.0469, -2.0625,  2.7344,  4.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.4531, -3.0469, -2.0625,  2.7344,  4.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.0859,  1.6641,  2.2188,  0.0493, -2.8438]], dtype=torch.bfloat16))]), logits=tensor([[-1.0859,  1.6641,  2.2188,  0.0493, -2.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0156, -1.8984, -0.8477,  1.8438,  2.7344]], dtype=torch.bfloat16))]), logits=tensor([[-3.0156, -1.8984, -0.8477,  1.8438,  2.7344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5938, -2.8750, -2.5000,  1.9922,  5.2500]], dtype=torch.bfloat16))]), logits=tensor([[-2.5938, -2.8750, -2.5000,  1.9922,  5.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3438,  0.5703,  2.2500,  0.8867, -1.8047]], dtype=torch.bfloat16))]), logits=tensor([[-2.3438,  0.5703,  2.2500,  0.8867, -1.8047]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -3.1250, -2.0625,  2.8281,  4.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -3.1250, -2.0625,  2.8281,  4.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1719, -3.0469, -2.2344,  2.5156,  4.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.1719, -3.0469, -2.2344,  2.5156,  4.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -2.4219, -1.4844,  2.0625,  3.9531]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -2.4219, -1.4844,  2.0625,  3.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.2500,  0.3301,  1.7812,  1.1484, -1.5469]], dtype=torch.bfloat16))]), logits=tensor([[-2.2500,  0.3301,  1.7812,  1.1484, -1.5469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8906, -0.6484,  1.2422,  2.0000, -0.3105]], dtype=torch.bfloat16))]), logits=tensor([[-2.8906, -0.6484,  1.2422,  2.0000, -0.3105]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9375, -3.0312, -2.3906,  2.3281,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.9375, -3.0312, -2.3906,  2.3281,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1719, -3.0781, -2.1875,  2.5781,  4.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.1719, -3.0781, -2.1875,  2.5781,  4.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -1.6875,  0.6289,  2.7656,  1.1172]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -1.6875,  0.6289,  2.7656,  1.1172]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -2.9219, -2.0156,  2.6250,  4.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -2.9219, -2.0156,  2.6250,  4.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.1562,  2.4219,  1.6016, -1.2891, -3.3594]], dtype=torch.bfloat16))]), logits=tensor([[ 1.1562,  2.4219,  1.6016, -1.2891, -3.3594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.4609,  1.5469,  2.2969,  0.1729, -2.7812]], dtype=torch.bfloat16))]), logits=tensor([[-1.4609,  1.5469,  2.2969,  0.1729, -2.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7031, -3.0938, -1.3828,  3.2656,  3.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.7031, -3.0938, -1.3828,  3.2656,  3.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9531, -1.1797,  0.1318,  2.0000,  1.1328]], dtype=torch.bfloat16))]), logits=tensor([[-2.9531, -1.1797,  0.1318,  2.0000,  1.1328]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.1562,  2.2031,  0.6992, -1.6328, -2.7188]], dtype=torch.bfloat16))]), logits=tensor([[ 2.1562,  2.2031,  0.6992, -1.6328, -2.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9375, -2.7969, -2.2500,  2.1406,  4.7812]], dtype=torch.bfloat16))]), logits=tensor([[-2.9375, -2.7969, -2.2500,  2.1406,  4.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7031, -2.8125, -0.8047,  3.3906,  2.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.7031, -2.8125, -0.8047,  3.3906,  2.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0781, -0.7344,  1.3594,  1.9375, -0.2441]], dtype=torch.bfloat16))]), logits=tensor([[-3.0781, -0.7344,  1.3594,  1.9375, -0.2441]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.7500, -0.5234,  3.4844,  2.7969]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.7500, -0.5234,  3.4844,  2.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -2.8281, -2.1875,  2.3906,  4.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -2.8281, -2.1875,  2.3906,  4.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7812, -0.2871,  1.3906,  1.8438, -0.7148]], dtype=torch.bfloat16))]), logits=tensor([[-2.7812, -0.2871,  1.3906,  1.8438, -0.7148]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.3184,  1.9141,  1.3984, -0.5898, -2.6094]], dtype=torch.bfloat16))]), logits=tensor([[ 0.3184,  1.9141,  1.3984, -0.5898, -2.6094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5469, -3.0469, -1.7109,  3.0469,  4.1562]], dtype=torch.bfloat16))]), logits=tensor([[-3.5469, -3.0469, -1.7109,  3.0469,  4.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.9531, -1.0078,  3.3906,  3.3594]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.9531, -1.0078,  3.3906,  3.3594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.3906, -0.1738,  3.2656,  2.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.3906, -0.1738,  3.2656,  2.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -3.2031, -2.0312,  2.8750,  4.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -3.2031, -2.0312,  2.8750,  4.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -3.0469, -1.9062,  2.8594,  4.4375]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -3.0469, -1.9062,  2.8594,  4.4375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.5703,  1.3984,  2.2344,  0.4062, -2.6406]], dtype=torch.bfloat16))]), logits=tensor([[-1.5703,  1.3984,  2.2344,  0.4062, -2.6406]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -3.1406, -1.9609,  2.9844,  4.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -3.1406, -1.9609,  2.9844,  4.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.9219,  1.6094, -0.3672, -2.0000, -1.9375]], dtype=torch.bfloat16))]), logits=tensor([[ 3.9219,  1.6094, -0.3672, -2.0000, -1.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.6250, -0.8711,  3.0469,  3.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.6250, -0.8711,  3.0469,  3.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -3.1406, -2.1875,  2.7812,  4.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -3.1406, -2.1875,  2.7812,  4.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.1875,  0.7227,  1.9844,  0.9570, -1.9375]], dtype=torch.bfloat16))]), logits=tensor([[-2.1875,  0.7227,  1.9844,  0.9570, -1.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 4.0312,  2.0938, -0.1455, -2.2969, -2.4844]], dtype=torch.bfloat16))]), logits=tensor([[ 4.0312,  2.0938, -0.1455, -2.2969, -2.4844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2031, -3.1562, -2.2656,  2.7344,  4.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.2031, -3.1562, -2.2656,  2.7344,  4.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -3.1094, -1.9062,  2.7812,  4.5312]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -3.1094, -1.9062,  2.7812,  4.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -3.1406, -2.2500,  2.6875,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -3.1406, -2.2500,  2.6875,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.2656,  2.3125,  0.2656, -2.1562, -2.7031]], dtype=torch.bfloat16))]), logits=tensor([[ 3.2656,  2.3125,  0.2656, -2.1562, -2.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -3.0000, -2.2656,  2.3750,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -3.0000, -2.2656,  2.3750,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.9062, -2.4844, -0.4434,  3.2188,  2.6406]], dtype=torch.bfloat16))]), logits=tensor([[-3.9062, -2.4844, -0.4434,  3.2188,  2.6406]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3594, -3.1562, -1.9844,  2.7969,  4.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.3594, -3.1562, -1.9844,  2.7969,  4.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7031, -2.3438, -0.0393,  3.1875,  2.0938]], dtype=torch.bfloat16))]), logits=tensor([[-3.7031, -2.3438, -0.0393,  3.1875,  2.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.1250,  0.6172,  1.6250,  0.8945, -1.4609]], dtype=torch.bfloat16))]), logits=tensor([[-2.1250,  0.6172,  1.6250,  0.8945, -1.4609]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.0469,  2.3594,  0.3867, -2.0625, -2.7656]], dtype=torch.bfloat16))]), logits=tensor([[ 3.0469,  2.3594,  0.3867, -2.0625, -2.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.1670,  1.9141,  1.7109, -0.4219, -2.7812]], dtype=torch.bfloat16))]), logits=tensor([[-0.1670,  1.9141,  1.7109, -0.4219, -2.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.8828, -0.2793,  0.0430,  0.8164,  0.4668]], dtype=torch.bfloat16))]), logits=tensor([[-1.8828, -0.2793,  0.0430,  0.8164,  0.4668]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.5000,  1.0547,  1.9297,  0.7109, -2.4531]], dtype=torch.bfloat16))]), logits=tensor([[-1.5000,  1.0547,  1.9297,  0.7109, -2.4531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.1108,  2.0312,  1.9297, -0.4336, -3.0625]], dtype=torch.bfloat16))]), logits=tensor([[-0.1108,  2.0312,  1.9297, -0.4336, -3.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8281, -2.5156, -0.1621,  3.4531,  2.3281]], dtype=torch.bfloat16))]), logits=tensor([[-3.8281, -2.5156, -0.1621,  3.4531,  2.3281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.2500,  0.7109,  2.1406,  0.9375, -1.9844]], dtype=torch.bfloat16))]), logits=tensor([[-2.2500,  0.7109,  2.1406,  0.9375, -1.9844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5156, -1.3750,  1.2891,  2.6719,  0.3867]], dtype=torch.bfloat16))]), logits=tensor([[-3.5156, -1.3750,  1.2891,  2.6719,  0.3867]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.8828,  0.8086,  1.7969,  0.7539, -1.8984]], dtype=torch.bfloat16))]), logits=tensor([[-1.8828,  0.8086,  1.7969,  0.7539, -1.8984]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.4531,  2.2656,  0.0708, -2.1719, -2.6250]], dtype=torch.bfloat16))]), logits=tensor([[ 3.4531,  2.2656,  0.0708, -2.1719, -2.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.8906, -1.0312,  3.3281,  3.4062]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.8906, -1.0312,  3.3281,  3.4062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8594, -2.7188, -0.4141,  3.5469,  2.6094]], dtype=torch.bfloat16))]), logits=tensor([[-3.8594, -2.7188, -0.4141,  3.5469,  2.6094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4062, -2.8125, -2.5000,  1.8125,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.4062, -2.8125, -2.5000,  1.8125,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.3594,  2.0469,  2.0625, -0.4043, -3.1250]], dtype=torch.bfloat16))]), logits=tensor([[-0.3594,  2.0469,  2.0625, -0.4043, -3.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1719, -3.0781, -2.2656,  2.6250,  4.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.1719, -3.0781, -2.2656,  2.6250,  4.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -2.9844, -1.4766,  2.9688,  3.9531]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -2.9844, -1.4766,  2.9688,  3.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3906, -3.1094, -2.0781,  2.7656,  4.7188]], dtype=torch.bfloat16))]), logits=tensor([[-3.3906, -3.1094, -2.0781,  2.7656,  4.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3594, -3.1719, -2.1406,  2.8438,  4.8750]], dtype=torch.bfloat16))]), logits=tensor([[-3.3594, -3.1719, -2.1406,  2.8438,  4.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.8516,  2.2500,  0.8906, -1.5234, -2.8438]], dtype=torch.bfloat16))]), logits=tensor([[ 1.8516,  2.2500,  0.8906, -1.5234, -2.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7188, -2.9375, -2.4688,  2.1094,  5.2500]], dtype=torch.bfloat16))]), logits=tensor([[-2.7188, -2.9375, -2.4688,  2.1094,  5.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2969, -3.1094, -2.1406,  2.6250,  4.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.2969, -3.1094, -2.1406,  2.6250,  4.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.9219,  2.3906,  0.4473, -2.0469, -2.8281]], dtype=torch.bfloat16))]), logits=tensor([[ 2.9219,  2.3906,  0.4473, -2.0469, -2.8281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.5547,  1.3750,  2.1250,  0.2949, -2.4844]], dtype=torch.bfloat16))]), logits=tensor([[-1.5547,  1.3750,  2.1250,  0.2949, -2.4844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.8906, -1.1562,  3.0625,  3.5781]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.8906, -1.1562,  3.0625,  3.5781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -3.1250, -2.2656,  2.6250,  5.0000]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -3.1250, -2.2656,  2.6250,  5.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 4.0000,  1.2500, -0.4824, -1.9219, -1.7031]], dtype=torch.bfloat16))]), logits=tensor([[ 4.0000,  1.2500, -0.4824, -1.9219, -1.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.2305,  2.0938,  1.7344, -0.6758, -3.0156]], dtype=torch.bfloat16))]), logits=tensor([[ 0.2305,  2.0938,  1.7344, -0.6758, -3.0156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.7969, -1.2266,  2.7812,  3.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.7969, -1.2266,  2.7812,  3.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.9844,  0.7344,  1.7734,  0.9766, -1.8672]], dtype=torch.bfloat16))]), logits=tensor([[-1.9844,  0.7344,  1.7734,  0.9766, -1.8672]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.0469,  2.2188,  0.7422, -1.6094, -2.7812]], dtype=torch.bfloat16))]), logits=tensor([[ 2.0469,  2.2188,  0.7422, -1.6094, -2.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.8438,  2.1719,  0.8672, -1.4531, -2.7656]], dtype=torch.bfloat16))]), logits=tensor([[ 1.8438,  2.1719,  0.8672, -1.4531, -2.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4062, -1.3594,  1.2188,  2.8125,  0.1914]], dtype=torch.bfloat16))]), logits=tensor([[-3.4062, -1.3594,  1.2188,  2.8125,  0.1914]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3125,  0.6055,  2.0156,  0.8789, -1.7188]], dtype=torch.bfloat16))]), logits=tensor([[-2.3125,  0.6055,  2.0156,  0.8789, -1.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -2.1250,  0.2129,  2.9531,  1.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -2.1250,  0.2129,  2.9531,  1.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -2.7969, -1.8672,  2.3594,  4.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -2.7969, -1.8672,  2.3594,  4.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7969,  1.3203, -0.4746, -1.8203, -1.6641]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7969,  1.3203, -0.4746, -1.8203, -1.6641]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5625, -3.1094, -1.8047,  3.0312,  4.2812]], dtype=torch.bfloat16))]), logits=tensor([[-3.5625, -3.1094, -1.8047,  3.0312,  4.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.3711,  2.5312,  2.2188, -1.1875, -3.8438]], dtype=torch.bfloat16))]), logits=tensor([[ 0.3711,  2.5312,  2.2188, -1.1875, -3.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.9688, -1.6172,  2.9375,  4.0312]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.9688, -1.6172,  2.9375,  4.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0938, -2.9375, -2.1562,  2.3438,  4.7812]], dtype=torch.bfloat16))]), logits=tensor([[-3.0938, -2.9375, -2.1562,  2.3438,  4.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -3.0625, -1.9375,  2.5469,  4.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -3.0625, -1.9375,  2.5469,  4.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.6406, -0.8516,  3.1875,  2.9844]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.6406, -0.8516,  3.1875,  2.9844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -3.0938, -2.1562,  2.5938,  4.8750]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -3.0938, -2.1562,  2.5938,  4.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8125, -2.6875, -0.8047,  3.1406,  3.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.8125, -2.6875, -0.8047,  3.1406,  3.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -3.1406, -1.9688,  2.8750,  4.5938]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -3.1406, -1.9688,  2.8750,  4.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.5156,  2.4375,  0.2539, -2.2812, -2.8438]], dtype=torch.bfloat16))]), logits=tensor([[ 3.5156,  2.4375,  0.2539, -2.2812, -2.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3906,  0.0138,  1.3125,  1.1719, -0.6758]], dtype=torch.bfloat16))]), logits=tensor([[-2.3906,  0.0138,  1.3125,  1.1719, -0.6758]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9688, -3.0312, -2.3750,  2.3281,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.9688, -3.0312, -2.3750,  2.3281,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6094, -2.8594, -2.4531,  1.9531,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.6094, -2.8594, -2.4531,  1.9531,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -3.1250, -1.7344,  3.1094,  4.2812]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -3.1250, -1.7344,  3.1094,  4.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -2.0625, -0.7031,  2.5312,  2.5938]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -2.0625, -0.7031,  2.5312,  2.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3125,  0.6445,  2.3906,  1.0078, -2.1406]], dtype=torch.bfloat16))]), logits=tensor([[-2.3125,  0.6445,  2.3906,  1.0078, -2.1406]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2188, -3.1094, -2.2031,  2.6875,  4.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.2188, -3.1094, -2.2031,  2.6875,  4.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5625, -2.7812, -2.4062,  1.9062,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-2.5625, -2.7812, -2.4062,  1.9062,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0781, -0.6641,  1.3906,  2.0938, -0.3613]], dtype=torch.bfloat16))]), logits=tensor([[-3.0781, -0.6641,  1.3906,  2.0938, -0.3613]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.5469,  2.3438,  0.6719, -1.8516, -2.8750]], dtype=torch.bfloat16))]), logits=tensor([[ 2.5469,  2.3438,  0.6719, -1.8516, -2.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.1318,  2.3438,  2.1406, -0.9414, -3.6719]], dtype=torch.bfloat16))]), logits=tensor([[ 0.1318,  2.3438,  2.1406, -0.9414, -3.6719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.9531, -1.1875,  3.0000,  3.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.9531, -1.1875,  3.0000,  3.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.1406,  0.3965,  1.4219,  0.9727, -1.2344]], dtype=torch.bfloat16))]), logits=tensor([[-2.1406,  0.3965,  1.4219,  0.9727, -1.2344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.8320,  1.7812,  2.2656, -0.3145, -3.2812]], dtype=torch.bfloat16))]), logits=tensor([[-0.8320,  1.7812,  2.2656, -0.3145, -3.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.1221,  2.0156,  1.8438, -0.3965, -2.9688]], dtype=torch.bfloat16))]), logits=tensor([[-0.1221,  2.0156,  1.8438, -0.3965, -2.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.8750, -0.9922,  3.2500,  3.2969]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.8750, -0.9922,  3.2500,  3.2969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -2.9688, -1.8672,  2.5781,  4.3750]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -2.9688, -1.8672,  2.5781,  4.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.1406,  0.7344,  2.4062,  0.7383, -2.2500]], dtype=torch.bfloat16))]), logits=tensor([[-2.1406,  0.7344,  2.4062,  0.7383, -2.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0000, -2.9844, -2.2344,  2.3125,  4.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.0000, -2.9844, -2.2344,  2.3125,  4.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -3.0781, -2.2188,  2.6094,  4.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -3.0781, -2.2188,  2.6094,  4.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.6562,  2.3281,  0.0991, -2.2812, -2.7031]], dtype=torch.bfloat16))]), logits=tensor([[ 3.6562,  2.3281,  0.0991, -2.2812, -2.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7188, -2.9375, -2.4219,  2.1094,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7188, -2.9375, -2.4219,  2.1094,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.8125,  1.2734,  2.3750,  0.4082, -2.5938]], dtype=torch.bfloat16))]), logits=tensor([[-1.8125,  1.2734,  2.3750,  0.4082, -2.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8125, -2.6094, -0.9297,  2.9844,  3.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.8125, -2.6094, -0.9297,  2.9844,  3.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.9062,  2.1094, -0.1445, -2.2656, -2.4531]], dtype=torch.bfloat16))]), logits=tensor([[ 3.9062,  2.1094, -0.1445, -2.2656, -2.4531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -3.1406, -2.2500,  2.6406,  4.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -3.1406, -2.2500,  2.6406,  4.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6406, -1.5391,  0.7344,  2.7031,  0.9766]], dtype=torch.bfloat16))]), logits=tensor([[-3.6406, -1.5391,  0.7344,  2.7031,  0.9766]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6406,  0.1016,  2.0312,  1.4844, -1.4922]], dtype=torch.bfloat16))]), logits=tensor([[-2.6406,  0.1016,  2.0312,  1.4844, -1.4922]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7188, -2.9375, -2.4531,  2.0781,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7188, -2.9375, -2.4531,  2.0781,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.4531,  2.0000, -0.1367, -2.0625, -2.2812]], dtype=torch.bfloat16))]), logits=tensor([[ 3.4531,  2.0000, -0.1367, -2.0625, -2.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6406, -2.0000,  0.0554,  2.7812,  1.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.6406, -2.0000,  0.0554,  2.7812,  1.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -3.0312, -1.6953,  3.0000,  4.1875]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -3.0312, -1.6953,  3.0000,  4.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2656, -3.0938, -2.1406,  2.5938,  4.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.2656, -3.0938, -2.1406,  2.5938,  4.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3906, -3.0469, -2.0469,  2.7500,  4.5938]], dtype=torch.bfloat16))]), logits=tensor([[-3.3906, -3.0469, -2.0469,  2.7500,  4.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8906, -2.9688, -2.3750,  2.2969,  4.9062]], dtype=torch.bfloat16))]), logits=tensor([[-2.8906, -2.9688, -2.3750,  2.2969,  4.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.6406,  2.4062,  0.1895, -2.3125, -2.8125]], dtype=torch.bfloat16))]), logits=tensor([[ 3.6406,  2.4062,  0.1895, -2.3125, -2.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -3.1250, -2.2344,  2.5469,  5.0000]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -3.1250, -2.2344,  2.5469,  5.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7031,  2.3281,  0.0850, -2.3125, -2.7031]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7031,  2.3281,  0.0850, -2.3125, -2.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1250, -3.0781, -2.2500,  2.5469,  4.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.1250, -3.0781, -2.2500,  2.5469,  4.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5938, -2.9844, -1.1875,  3.2188,  3.5781]], dtype=torch.bfloat16))]), logits=tensor([[-3.5938, -2.9844, -1.1875,  3.2188,  3.5781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0000, -0.4629,  1.9219,  1.8984, -0.9180]], dtype=torch.bfloat16))]), logits=tensor([[-3.0000, -0.4629,  1.9219,  1.8984, -0.9180]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 4.8065e-04,  1.9844e+00,  1.7031e+00, -5.7422e-01, -2.9375e+00]],\n",
       "        dtype=torch.bfloat16))]), logits=tensor([[ 4.8065e-04,  1.9844e+00,  1.7031e+00, -5.7422e-01, -2.9375e+00]],\n",
       "        dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.6992,  1.7500,  0.7891, -0.8477, -2.0781]], dtype=torch.bfloat16))]), logits=tensor([[ 0.6992,  1.7500,  0.7891, -0.8477, -2.0781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8438, -0.2207,  1.4531,  1.8125, -0.7656]], dtype=torch.bfloat16))]), logits=tensor([[-2.8438, -0.2207,  1.4531,  1.8125, -0.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -3.0469, -1.8203,  2.9688,  4.2812]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -3.0469, -1.8203,  2.9688,  4.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -3.0781, -2.2500,  2.5938,  4.8750]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -3.0781, -2.2500,  2.5938,  4.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.2969,  0.0815,  3.3125,  1.9141]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.2969,  0.0815,  3.3125,  1.9141]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.9062, -0.9766,  3.2969,  3.3281]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.9062, -0.9766,  3.2969,  3.3281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.0216,  1.2812,  0.7266, -0.4141, -1.6641]], dtype=torch.bfloat16))]), logits=tensor([[ 0.0216,  1.2812,  0.7266, -0.4141, -1.6641]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7812, -2.3750, -0.5195,  2.9531,  2.6406]], dtype=torch.bfloat16))]), logits=tensor([[-3.7812, -2.3750, -0.5195,  2.9531,  2.6406]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5938, -0.9375,  0.2354,  1.4844,  0.9336]], dtype=torch.bfloat16))]), logits=tensor([[-2.5938, -0.9375,  0.2354,  1.4844,  0.9336]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7969, -2.5312, -0.1641,  3.4375,  2.2812]], dtype=torch.bfloat16))]), logits=tensor([[-3.7969, -2.5312, -0.1641,  3.4375,  2.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.5156, -0.2656,  3.2656,  2.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.5156, -0.2656,  3.2656,  2.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4375, -0.0674,  1.3203,  1.3984, -0.9180]], dtype=torch.bfloat16))]), logits=tensor([[-2.4375, -0.0674,  1.3203,  1.3984, -0.9180]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.5938,  1.6094, -0.4219, -1.9219, -1.8047]], dtype=torch.bfloat16))]), logits=tensor([[ 3.5938,  1.6094, -0.4219, -1.9219, -1.8047]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.7969, -0.0391, -0.0260,  0.5625,  0.5195]], dtype=torch.bfloat16))]), logits=tensor([[-1.7969, -0.0391, -0.0260,  0.5625,  0.5195]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.6094,  2.3438,  0.7070, -1.8672, -2.9062]], dtype=torch.bfloat16))]), logits=tensor([[ 2.6094,  2.3438,  0.7070, -1.8672, -2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -3.0938, -2.3281,  2.4531,  5.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -3.0938, -2.3281,  2.4531,  5.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.9219,  2.3438,  0.4160, -2.0469, -2.7656]], dtype=torch.bfloat16))]), logits=tensor([[ 2.9219,  2.3438,  0.4160, -2.0469, -2.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.9375,  2.4219,  0.5664, -2.0781, -2.9062]], dtype=torch.bfloat16))]), logits=tensor([[ 2.9375,  2.4219,  0.5664, -2.0781, -2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.3906,  2.2031,  0.1069, -2.1406, -2.5625]], dtype=torch.bfloat16))]), logits=tensor([[ 3.3906,  2.2031,  0.1069, -2.1406, -2.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.7500, -1.3047,  2.9062,  3.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.7500, -1.3047,  2.9062,  3.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.0156,  2.2188,  0.2344, -2.0000, -2.6094]], dtype=torch.bfloat16))]), logits=tensor([[ 3.0156,  2.2188,  0.2344, -2.0000, -2.6094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5781, -2.7969, -2.4688,  1.8359,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.5781, -2.7969, -2.4688,  1.8359,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5938, -1.5312,  0.9336,  2.7812,  0.6953]], dtype=torch.bfloat16))]), logits=tensor([[-3.5938, -1.5312,  0.9336,  2.7812,  0.6953]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -3.2031, -1.9609,  2.9688,  4.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -3.2031, -1.9609,  2.9688,  4.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9062, -0.4316,  1.4062,  1.6719, -0.4277]], dtype=torch.bfloat16))]), logits=tensor([[-2.9062, -0.4316,  1.4062,  1.6719, -0.4277]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7969, -2.2188,  0.2695,  3.2500,  1.7656]], dtype=torch.bfloat16))]), logits=tensor([[-3.7969, -2.2188,  0.2695,  3.2500,  1.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5625, -2.8438, -2.4531,  1.8828,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.5625, -2.8438, -2.4531,  1.8828,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9844, -3.0938, -2.4219,  2.4531,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.9844, -3.0938, -2.4219,  2.4531,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -1.7969,  0.0447,  2.4531,  1.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -1.7969,  0.0447,  2.4531,  1.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.2812,  2.3281,  1.3438, -1.2891, -3.1250]], dtype=torch.bfloat16))]), logits=tensor([[ 1.2812,  2.3281,  1.3438, -1.2891, -3.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -2.8750, -2.0000,  2.2031,  4.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -2.8750, -2.0000,  2.2031,  4.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.8906,  2.2344,  1.5078, -1.0078, -3.1719]], dtype=torch.bfloat16))]), logits=tensor([[ 0.8906,  2.2344,  1.5078, -1.0078, -3.1719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.4531,  2.1719,  1.7656, -0.7227, -3.2500]], dtype=torch.bfloat16))]), logits=tensor([[ 0.4531,  2.1719,  1.7656, -0.7227, -3.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0000, -0.4707,  1.5078,  1.8047, -0.5312]], dtype=torch.bfloat16))]), logits=tensor([[-3.0000, -0.4707,  1.5078,  1.8047, -0.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8438, -2.8750, -0.8359,  3.3125,  3.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.8438, -2.8750, -0.8359,  3.3125,  3.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -2.7188, -1.6016,  2.6250,  3.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -2.7188, -1.6016,  2.6250,  3.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -3.1250, -1.8047,  2.9688,  4.3438]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -3.1250, -1.8047,  2.9688,  4.3438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -1.7812, -0.0156,  2.7344,  1.6953]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -1.7812, -0.0156,  2.7344,  1.6953]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6406, -2.7656, -0.9688,  3.1094,  3.2344]], dtype=torch.bfloat16))]), logits=tensor([[-3.6406, -2.7656, -0.9688,  3.1094,  3.2344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.5195,  1.7344,  1.7969, -0.1758, -2.6406]], dtype=torch.bfloat16))]), logits=tensor([[-0.5195,  1.7344,  1.7969, -0.1758, -2.6406]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.5977,  1.7734,  1.9219, -0.1719, -2.7969]], dtype=torch.bfloat16))]), logits=tensor([[-0.5977,  1.7734,  1.9219, -0.1719, -2.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.5078,  1.7266,  0.3633, -1.2344, -2.0312]], dtype=torch.bfloat16))]), logits=tensor([[ 1.5078,  1.7266,  0.3633, -1.2344, -2.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.9219,  2.1250, -0.1416, -2.2812, -2.4375]], dtype=torch.bfloat16))]), logits=tensor([[ 3.9219,  2.1250, -0.1416, -2.2812, -2.4375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7188, -2.3906, -0.6797,  2.8281,  2.8750]], dtype=torch.bfloat16))]), logits=tensor([[-3.7188, -2.3906, -0.6797,  2.8281,  2.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7344, -2.9219, -2.4531,  2.1094,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7344, -2.9219, -2.4531,  2.1094,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0469, -3.0312, -2.2812,  2.3125,  5.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.0469, -3.0312, -2.2812,  2.3125,  5.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -3.1719, -1.9844,  2.9531,  4.5625]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -3.1719, -1.9844,  2.9531,  4.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -3.1562, -2.0938,  2.8594,  4.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -3.1562, -2.0938,  2.8594,  4.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.6406,  2.2812,  1.1406, -1.4375, -2.9531]], dtype=torch.bfloat16))]), logits=tensor([[ 1.6406,  2.2812,  1.1406, -1.4375, -2.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.8750,  2.2969,  1.0156, -1.5547, -2.9062]], dtype=torch.bfloat16))]), logits=tensor([[ 1.8750,  2.2969,  1.0156, -1.5547, -2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5469, -2.9062, -2.5000,  2.0000,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.5469, -2.9062, -2.5000,  2.0000,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -1.7656, -0.0879,  2.5469,  1.7734]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -1.7656, -0.0879,  2.5469,  1.7734]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1094, -3.1250, -2.3438,  2.6094,  5.0312]], dtype=torch.bfloat16))]), logits=tensor([[-3.1094, -3.1250, -2.3438,  2.6094,  5.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -2.6562, -1.2656,  2.6875,  3.5938]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -2.6562, -1.2656,  2.6875,  3.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5625, -1.8750,  0.5430,  2.9844,  1.2109]], dtype=torch.bfloat16))]), logits=tensor([[-3.5625, -1.8750,  0.5430,  2.9844,  1.2109]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6250, -0.0791,  1.1641,  1.3359, -0.5820]], dtype=torch.bfloat16))]), logits=tensor([[-2.6250, -0.0791,  1.1641,  1.3359, -0.5820]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -3.1562, -2.1094,  2.9062,  4.7188]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -3.1562, -2.1094,  2.9062,  4.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -3.0156, -1.6250,  2.8438,  4.1875]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -3.0156, -1.6250,  2.8438,  4.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6719, -2.9531, -2.4844,  2.1094,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.6719, -2.9531, -2.4844,  2.1094,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1719, -3.1406, -2.3281,  2.6562,  5.0312]], dtype=torch.bfloat16))]), logits=tensor([[-3.1719, -3.1406, -2.3281,  2.6562,  5.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -3.0156, -1.6797,  2.9219,  4.1875]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -3.0156, -1.6797,  2.9219,  4.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.7188, -0.8086,  3.2969,  3.0000]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.7188, -0.8086,  3.2969,  3.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6719, -3.2031, -1.6797,  3.3125,  4.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.6719, -3.2031, -1.6797,  3.3125,  4.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.6875,  0.7383,  1.3672,  0.5742, -1.4141]], dtype=torch.bfloat16))]), logits=tensor([[-1.6875,  0.7383,  1.3672,  0.5742, -1.4141]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -1.8672, -1.1484,  1.8984,  2.9844]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -1.8672, -1.1484,  1.8984,  2.9844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.0000,  1.5000,  1.7969,  0.0184, -2.4531]], dtype=torch.bfloat16))]), logits=tensor([[-1.0000,  1.5000,  1.7969,  0.0184, -2.4531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.9062, -1.2891,  3.1250,  3.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.9062, -1.2891,  3.1250,  3.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0000, -2.0156, -1.3438,  1.8906,  3.2812]], dtype=torch.bfloat16))]), logits=tensor([[-3.0000, -2.0156, -1.3438,  1.8906,  3.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.3750,  2.4375,  0.2988, -2.2188, -2.8438]], dtype=torch.bfloat16))]), logits=tensor([[ 3.3750,  2.4375,  0.2988, -2.2188, -2.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6406, -1.7734,  0.7461,  3.0000,  1.0234]], dtype=torch.bfloat16))]), logits=tensor([[-3.6406, -1.7734,  0.7461,  3.0000,  1.0234]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8438, -2.7969, -0.8750,  3.2812,  3.2031]], dtype=torch.bfloat16))]), logits=tensor([[-3.8438, -2.7969, -0.8750,  3.2812,  3.2031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0156, -3.0781, -2.3906,  2.4531,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-3.0156, -3.0781, -2.3906,  2.4531,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6406, -2.8750, -1.0391,  3.2031,  3.3125]], dtype=torch.bfloat16))]), logits=tensor([[-3.6406, -2.8750, -1.0391,  3.2031,  3.3125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6406, -2.3594, -0.5273,  2.9844,  2.5469]], dtype=torch.bfloat16))]), logits=tensor([[-3.6406, -2.3594, -0.5273,  2.9844,  2.5469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.1250,  2.2188,  1.2656, -1.2188, -3.0156]], dtype=torch.bfloat16))]), logits=tensor([[ 1.1250,  2.2188,  1.2656, -1.2188, -3.0156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.6094,  2.4844,  0.8281, -1.9609, -3.0625]], dtype=torch.bfloat16))]), logits=tensor([[ 2.6094,  2.4844,  0.8281, -1.9609, -3.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.9609,  2.3438,  0.9219, -1.6406, -2.9375]], dtype=torch.bfloat16))]), logits=tensor([[ 1.9609,  2.3438,  0.9219, -1.6406, -2.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.4746,  1.9844,  1.3594, -0.7227, -2.6875]], dtype=torch.bfloat16))]), logits=tensor([[ 0.4746,  1.9844,  1.3594, -0.7227, -2.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6719, -2.8281, -1.3516,  2.7344,  3.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.6719, -2.8281, -1.3516,  2.7344,  3.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.3281,  2.1406,  0.4688, -1.6797, -2.6094]], dtype=torch.bfloat16))]), logits=tensor([[ 2.3281,  2.1406,  0.4688, -1.6797, -2.6094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -2.4375, -0.7617,  2.8750,  2.8281]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -2.4375, -0.7617,  2.8750,  2.8281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5625, -1.5938,  0.7344,  2.8281,  0.9336]], dtype=torch.bfloat16))]), logits=tensor([[-3.5625, -1.5938,  0.7344,  2.8281,  0.9336]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8125, -2.4531, -0.4121,  3.1562,  2.5781]], dtype=torch.bfloat16))]), logits=tensor([[-3.8125, -2.4531, -0.4121,  3.1562,  2.5781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.1406,  2.4062,  0.9727, -1.7109, -3.0000]], dtype=torch.bfloat16))]), logits=tensor([[ 2.1406,  2.4062,  0.9727, -1.7109, -3.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.6562, -1.1094,  2.9219,  3.3906]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.6562, -1.1094,  2.9219,  3.3906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -1.9922, -0.3984,  2.2344,  2.3125]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -1.9922, -0.3984,  2.2344,  2.3125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.4609,  1.3281,  1.9844,  0.3066, -2.4219]], dtype=torch.bfloat16))]), logits=tensor([[-1.4609,  1.3281,  1.9844,  0.3066, -2.4219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -2.6094, -0.6758,  3.0781,  2.8750]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -2.6094, -0.6758,  3.0781,  2.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.8438,  1.4062, -0.4434, -1.8984, -1.7188]], dtype=torch.bfloat16))]), logits=tensor([[ 3.8438,  1.4062, -0.4434, -1.8984, -1.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2969, -1.0781,  1.0625,  2.3750,  0.2793]], dtype=torch.bfloat16))]), logits=tensor([[-3.2969, -1.0781,  1.0625,  2.3750,  0.2793]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.0625,  0.8242,  2.3594,  0.8242, -2.3281]], dtype=torch.bfloat16))]), logits=tensor([[-2.0625,  0.8242,  2.3594,  0.8242, -2.3281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1719, -3.1094, -2.2812,  2.6406,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.1719, -3.1094, -2.2812,  2.6406,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -3.0469, -1.2656,  3.2656,  3.7188]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -3.0469, -1.2656,  3.2656,  3.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0938, -2.9219, -2.3438,  2.3594,  4.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.0938, -2.9219, -2.3438,  2.3594,  4.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5625, -2.7969, -1.3672,  2.5938,  3.7969]], dtype=torch.bfloat16))]), logits=tensor([[-3.5625, -2.7969, -1.3672,  2.5938,  3.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -3.1406, -1.4219,  3.3125,  4.0000]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -3.1406, -1.4219,  3.3125,  4.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.1406, -0.1416,  2.9062,  2.1562]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.1406, -0.1416,  2.9062,  2.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9219, -2.9531, -2.3906,  2.2031,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.9219, -2.9531, -2.3906,  2.2031,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.4902,  1.9375,  2.0938, -0.1719, -3.0312]], dtype=torch.bfloat16))]), logits=tensor([[-0.4902,  1.9375,  2.0938, -0.1719, -3.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.6875, -1.1016,  2.9844,  3.3906]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.6875, -1.1016,  2.9844,  3.3906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0156, -0.8438,  0.7734,  1.7812,  0.4414]], dtype=torch.bfloat16))]), logits=tensor([[-3.0156, -0.8438,  0.7734,  1.7812,  0.4414]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -1.3359,  1.1719,  2.5781,  0.4102]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -1.3359,  1.1719,  2.5781,  0.4102]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8594, -2.6094, -0.6875,  3.1094,  2.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.8594, -2.6094, -0.6875,  3.1094,  2.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -3.1094, -2.0781,  2.8125,  4.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -3.1094, -2.0781,  2.8125,  4.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2344, -0.8945,  1.7422,  2.1406, -0.3496]], dtype=torch.bfloat16))]), logits=tensor([[-3.2344, -0.8945,  1.7422,  2.1406, -0.3496]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.1016,  2.3750,  1.5312, -1.2656, -3.2656]], dtype=torch.bfloat16))]), logits=tensor([[ 1.1016,  2.3750,  1.5312, -1.2656, -3.2656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.2188,  2.3438,  0.2988, -2.1719, -2.7500]], dtype=torch.bfloat16))]), logits=tensor([[ 3.2188,  2.3438,  0.2988, -2.1719, -2.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.4219,  2.2031,  0.5352, -1.7500, -2.6562]], dtype=torch.bfloat16))]), logits=tensor([[ 2.4219,  2.2031,  0.5352, -1.7500, -2.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8906, -3.0312, -2.4219,  2.3125,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.8906, -3.0312, -2.4219,  2.3125,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.8594,  2.0938, -0.1367, -2.2188, -2.4062]], dtype=torch.bfloat16))]), logits=tensor([[ 3.8594,  2.0938, -0.1367, -2.2188, -2.4062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5156, -1.4688,  0.9844,  2.7812,  0.5195]], dtype=torch.bfloat16))]), logits=tensor([[-3.5156, -1.4688,  0.9844,  2.7812,  0.5195]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9688, -2.4531, -1.8828,  2.0156,  4.0938]], dtype=torch.bfloat16))]), logits=tensor([[-2.9688, -2.4531, -1.8828,  2.0156,  4.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -2.6719, -1.1953,  2.8594,  3.4844]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -2.6719, -1.1953,  2.8594,  3.4844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.6172,  2.1094,  0.8789, -1.3672, -2.6406]], dtype=torch.bfloat16))]), logits=tensor([[ 1.6172,  2.1094,  0.8789, -1.3672, -2.6406]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.9688,  2.1719, -0.0913, -2.3125, -2.5469]], dtype=torch.bfloat16))]), logits=tensor([[ 3.9688,  2.1719, -0.0913, -2.3125, -2.5469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -2.1719, -0.5508,  2.7500,  2.4531]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -2.1719, -0.5508,  2.7500,  2.4531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1562, -3.0469, -2.2188,  2.6875,  4.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.1562, -3.0469, -2.2188,  2.6875,  4.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -2.4688, -0.7617,  2.7656,  2.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -2.4688, -0.7617,  2.7656,  2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.7852,  1.9688,  1.1094, -0.9141, -2.5156]], dtype=torch.bfloat16))]), logits=tensor([[ 0.7852,  1.9688,  1.1094, -0.9141, -2.5156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -1.2109,  1.2422,  2.5938,  0.1533]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -1.2109,  1.2422,  2.5938,  0.1533]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8438, -2.5625, -0.4336,  3.3281,  2.5781]], dtype=torch.bfloat16))]), logits=tensor([[-3.8438, -2.5625, -0.4336,  3.3281,  2.5781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -2.9844, -2.2812,  2.4062,  4.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -2.9844, -2.2812,  2.4062,  4.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.9688,  1.2578, -0.4902, -1.8984, -1.6719]], dtype=torch.bfloat16))]), logits=tensor([[ 3.9688,  1.2578, -0.4902, -1.8984, -1.6719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8125, -2.6719, -2.0312,  1.9609,  4.4062]], dtype=torch.bfloat16))]), logits=tensor([[-2.8125, -2.6719, -2.0312,  1.9609,  4.4062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.3594, -0.8516,  2.7969,  2.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.3594, -0.8516,  2.7969,  2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6406, -2.6875, -0.5859,  3.3438,  2.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.6406, -2.6875, -0.5859,  3.3438,  2.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5625, -1.4141,  0.7930,  2.7500,  0.7148]], dtype=torch.bfloat16))]), logits=tensor([[-3.5625, -1.4141,  0.7930,  2.7500,  0.7148]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5156, -1.3984,  1.0781,  2.7500,  0.4551]], dtype=torch.bfloat16))]), logits=tensor([[-3.5156, -1.3984,  1.0781,  2.7500,  0.4551]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6406, -2.8594, -2.4531,  1.9766,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.6406, -2.8594, -2.4531,  1.9766,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.2031,  1.5938,  2.2656,  0.1982, -2.9219]], dtype=torch.bfloat16))]), logits=tensor([[-1.2031,  1.5938,  2.2656,  0.1982, -2.9219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -1.8359,  0.6758,  2.9688,  1.2031]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -1.8359,  0.6758,  2.9688,  1.2031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.1094,  2.3125,  1.4375, -1.1719, -3.1406]], dtype=torch.bfloat16))]), logits=tensor([[ 1.1094,  2.3125,  1.4375, -1.1719, -3.1406]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7656e+00, -1.8997e-03,  1.8281e+00,  1.5391e+00, -1.1797e+00]],\n",
       "        dtype=torch.bfloat16))]), logits=tensor([[-2.7656e+00, -1.8997e-03,  1.8281e+00,  1.5391e+00, -1.1797e+00]],\n",
       "        dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -1.7422,  0.5430,  2.8750,  1.1328]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -1.7422,  0.5430,  2.8750,  1.1328]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0938, -0.9414,  0.6406,  1.8125,  0.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.0938, -0.9414,  0.6406,  1.8125,  0.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -1.5000,  0.9219,  2.8438,  0.7344]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -1.5000,  0.9219,  2.8438,  0.7344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4531, -2.6406, -1.3359,  2.5938,  3.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.4531, -2.6406, -1.3359,  2.5938,  3.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -0.9883,  0.9648,  2.1562,  0.1221]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -0.9883,  0.9648,  2.1562,  0.1221]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -3.1562, -2.2500,  2.7188,  4.8750]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -3.1562, -2.2500,  2.7188,  4.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2656, -3.1562, -2.2500,  2.6875,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.2656, -3.1562, -2.2500,  2.6875,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.5938,  2.1562,  0.3672, -1.8203, -2.5469]], dtype=torch.bfloat16))]), logits=tensor([[ 2.5938,  2.1562,  0.3672, -1.8203, -2.5469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -3.0156, -1.2578,  3.2031,  3.6406]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -3.0156, -1.2578,  3.2031,  3.6406]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -1.0312,  0.5000,  2.1250,  0.5820]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -1.0312,  0.5000,  2.1250,  0.5820]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4531, -2.9375, -1.6094,  2.8906,  3.9531]], dtype=torch.bfloat16))]), logits=tensor([[-3.4531, -2.9375, -1.6094,  2.8906,  3.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.1089,  2.1094,  1.7969, -0.6211, -2.9844]], dtype=torch.bfloat16))]), logits=tensor([[ 0.1089,  2.1094,  1.7969, -0.6211, -2.9844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.4375,  0.6914,  2.0938,  0.3574, -2.2656]], dtype=torch.bfloat16))]), logits=tensor([[-1.4375,  0.6914,  2.0938,  0.3574, -2.2656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8438, -2.2031,  0.3555,  3.2969,  1.7109]], dtype=torch.bfloat16))]), logits=tensor([[-3.8438, -2.2031,  0.3555,  3.2969,  1.7109]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -3.0000, -2.0000,  2.4219,  4.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -3.0000, -2.0000,  2.4219,  4.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -2.6875, -1.7734,  2.2344,  4.2812]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -2.6875, -1.7734,  2.2344,  4.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8594, -3.0156, -2.3594,  2.3125,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.8594, -3.0156, -2.3594,  2.3125,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.0781,  2.3906,  0.4023, -2.0781, -2.8438]], dtype=torch.bfloat16))]), logits=tensor([[ 3.0781,  2.3906,  0.4023, -2.0781, -2.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6250, -2.9531, -1.5469,  3.1250,  3.9219]], dtype=torch.bfloat16))]), logits=tensor([[-3.6250, -2.9531, -1.5469,  3.1250,  3.9219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.9375,  0.8984,  2.0156,  0.7773, -2.0938]], dtype=torch.bfloat16))]), logits=tensor([[-1.9375,  0.8984,  2.0156,  0.7773, -2.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -1.4688,  0.7852,  2.5781,  0.8867]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -1.4688,  0.7852,  2.5781,  0.8867]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -2.5781, -1.0547,  2.7969,  3.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -2.5781, -1.0547,  2.7969,  3.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8750, -3.0312, -2.4375,  2.2969,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.8750, -3.0312, -2.4375,  2.2969,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4688, -2.8438, -2.4844,  1.8672,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.4688, -2.8438, -2.4844,  1.8672,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8438, -2.9062, -2.4062,  2.1094,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.8438, -2.9062, -2.4062,  2.1094,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5938, -2.6719, -0.9922,  2.8594,  3.2656]], dtype=torch.bfloat16))]), logits=tensor([[-3.5938, -2.6719, -0.9922,  2.8594,  3.2656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5938, -2.9062, -2.4844,  2.0000,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.5938, -2.9062, -2.4844,  2.0000,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2969, -0.8906,  1.2422,  2.3750, -0.0742]], dtype=torch.bfloat16))]), logits=tensor([[-3.2969, -0.8906,  1.2422,  2.3750, -0.0742]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -3.0469, -2.1875,  2.6719,  4.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -3.0469, -2.1875,  2.6719,  4.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0469, -0.5430,  1.7031,  2.1250, -0.8008]], dtype=torch.bfloat16))]), logits=tensor([[-3.0469, -0.5430,  1.7031,  2.1250, -0.8008]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.9297,  2.0312,  1.1641, -0.9648, -2.7031]], dtype=torch.bfloat16))]), logits=tensor([[ 0.9297,  2.0312,  1.1641, -0.9648, -2.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8750, -2.4688, -0.4746,  3.0312,  2.7188]], dtype=torch.bfloat16))]), logits=tensor([[-3.8750, -2.4688, -0.4746,  3.0312,  2.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.6875,  1.1875,  2.1094,  0.5898, -2.3906]], dtype=torch.bfloat16))]), logits=tensor([[-1.6875,  1.1875,  2.1094,  0.5898, -2.3906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9062, -3.0000, -2.3594,  2.2500,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.9062, -3.0000, -2.3594,  2.2500,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7969, -2.6562, -0.6719,  3.2656,  2.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.7969, -2.6562, -0.6719,  3.2656,  2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4219, -2.8125, -2.5000,  1.8359,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.4219, -2.8125, -2.5000,  1.8359,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -3.0469, -2.3594,  2.4062,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -3.0469, -2.3594,  2.4062,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -1.6719,  0.8320,  2.9375,  0.8555]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -1.6719,  0.8320,  2.9375,  0.8555]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -2.6094, -1.8750,  2.2031,  4.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -2.6094, -1.8750,  2.2031,  4.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8281, -2.9844, -2.4531,  2.2031,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.8281, -2.9844, -2.4531,  2.2031,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -1.5000,  0.9570,  2.7969,  0.6641]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -1.5000,  0.9570,  2.7969,  0.6641]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -1.3203,  0.4707,  2.3906,  0.9805]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -1.3203,  0.4707,  2.3906,  0.9805]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2031, -2.9844, -2.1250,  2.5000,  4.7188]], dtype=torch.bfloat16))]), logits=tensor([[-3.2031, -2.9844, -2.1250,  2.5000,  4.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7031, -2.6719, -1.0156,  3.0781,  3.2344]], dtype=torch.bfloat16))]), logits=tensor([[-3.7031, -2.6719, -1.0156,  3.0781,  3.2344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9375, -0.8711,  0.8359,  2.0469,  0.1572]], dtype=torch.bfloat16))]), logits=tensor([[-2.9375, -0.8711,  0.8359,  2.0469,  0.1572]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7969, -0.1602,  1.4453,  1.4062, -0.5938]], dtype=torch.bfloat16))]), logits=tensor([[-2.7969, -0.1602,  1.4453,  1.4062, -0.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9375, -3.0469, -2.3750,  2.3594,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.9375, -3.0469, -2.3750,  2.3594,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.8594,  1.1406,  2.3594,  0.6797, -2.5938]], dtype=torch.bfloat16))]), logits=tensor([[-1.8594,  1.1406,  2.3594,  0.6797, -2.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2188, -1.7578, -0.5859,  2.0469,  2.4375]], dtype=torch.bfloat16))]), logits=tensor([[-3.2188, -1.7578, -0.5859,  2.0469,  2.4375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.0312,  2.4219,  0.4570, -2.0781, -2.8594]], dtype=torch.bfloat16))]), logits=tensor([[ 3.0312,  2.4219,  0.4570, -2.0781, -2.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3438,  0.4727,  1.9375,  1.1641, -1.7266]], dtype=torch.bfloat16))]), logits=tensor([[-2.3438,  0.4727,  1.9375,  1.1641, -1.7266]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.1094,  0.1748,  3.1250,  1.7656]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.1094,  0.1748,  3.1250,  1.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5156, -2.8281, -2.4688,  1.8906,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.5156, -2.8281, -2.4688,  1.8906,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9219, -3.0781, -2.4062,  2.3906,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.9219, -3.0781, -2.4062,  2.3906,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -1.3828,  0.6680,  2.4531,  0.9297]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -1.3828,  0.6680,  2.4531,  0.9297]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.8672,  1.1641,  2.3594,  0.6250, -2.5781]], dtype=torch.bfloat16))]), logits=tensor([[-1.8672,  1.1641,  2.3594,  0.6250, -2.5781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -3.0625, -2.2500,  2.6250,  4.7812]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -3.0625, -2.2500,  2.6250,  4.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4219, -2.8906, -2.5156,  1.9062,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.4219, -2.8906, -2.5156,  1.9062,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.4531, -0.4258,  3.2188,  2.5156]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.4531, -0.4258,  3.2188,  2.5156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1562, -3.1562, -2.2188,  2.7344,  4.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.1562, -3.1562, -2.2188,  2.7344,  4.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7031, -2.4844, -0.7617,  3.0312,  2.8906]], dtype=torch.bfloat16))]), logits=tensor([[-3.7031, -2.4844, -0.7617,  3.0312,  2.8906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7031, -1.7188,  0.6953,  2.8281,  1.1953]], dtype=torch.bfloat16))]), logits=tensor([[-3.7031, -1.7188,  0.6953,  2.8281,  1.1953]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -1.8125,  0.7070,  2.9062,  1.1172]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -1.8125,  0.7070,  2.9062,  1.1172]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.9062,  2.1094,  0.2139, -1.9219, -2.4688]], dtype=torch.bfloat16))]), logits=tensor([[ 2.9062,  2.1094,  0.2139, -1.9219, -2.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8906, -2.9844, -2.4062,  2.2500,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.8906, -2.9844, -2.4062,  2.2500,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9688, -3.0625, -2.4062,  2.3906,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.9688, -3.0625, -2.4062,  2.3906,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8750, -0.2305,  1.1797,  1.3438, -0.1855]], dtype=torch.bfloat16))]), logits=tensor([[-2.8750, -0.2305,  1.1797,  1.3438, -0.1855]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7969, -2.6875, -0.5781,  3.1875,  2.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.7969, -2.6875, -0.5781,  3.1875,  2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8125, -1.9141,  0.3750,  3.0469,  1.4688]], dtype=torch.bfloat16))]), logits=tensor([[-3.8125, -1.9141,  0.3750,  3.0469,  1.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3906, -1.9766, -0.6914,  2.0469,  2.7969]], dtype=torch.bfloat16))]), logits=tensor([[-3.3906, -1.9766, -0.6914,  2.0469,  2.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7031, -0.7734,  0.2520,  1.5391,  0.8242]], dtype=torch.bfloat16))]), logits=tensor([[-2.7031, -0.7734,  0.2520,  1.5391,  0.8242]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2188, -1.0625,  0.9922,  2.3906,  0.2285]], dtype=torch.bfloat16))]), logits=tensor([[-3.2188, -1.0625,  0.9922,  2.3906,  0.2285]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -3.0625, -1.9375,  2.9219,  4.4062]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -3.0625, -1.9375,  2.9219,  4.4062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.0234,  2.0156,  1.0859, -1.0078, -2.6719]], dtype=torch.bfloat16))]), logits=tensor([[ 1.0234,  2.0156,  1.0859, -1.0078, -2.6719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.2539,  1.2266,  0.9258, -0.2490, -1.8984]], dtype=torch.bfloat16))]), logits=tensor([[-0.2539,  1.2266,  0.9258, -0.2490, -1.8984]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.5469, -0.4648,  3.2344,  2.5781]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.5469, -0.4648,  3.2344,  2.5781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.9375, -1.7500,  2.8594,  4.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.9375, -1.7500,  2.8594,  4.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.5156,  2.3750,  0.2324, -2.2500, -2.7812]], dtype=torch.bfloat16))]), logits=tensor([[ 3.5156,  2.3750,  0.2324, -2.2500, -2.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -1.8594,  0.3789,  2.8438,  1.4141]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -1.8594,  0.3789,  2.8438,  1.4141]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7812, -2.2188,  0.2266,  3.2656,  1.7656]], dtype=torch.bfloat16))]), logits=tensor([[-3.7812, -2.2188,  0.2266,  3.2656,  1.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2188, -3.1094, -2.1562,  2.5781,  4.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.2188, -3.1094, -2.1562,  2.5781,  4.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7188, -2.8281, -1.2578,  3.0000,  3.6719]], dtype=torch.bfloat16))]), logits=tensor([[-3.7188, -2.8281, -1.2578,  3.0000,  3.6719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6719, -2.4531, -0.5859,  2.7344,  2.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.6719, -2.4531, -0.5859,  2.7344,  2.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6719, -1.7969,  0.4316,  2.8906,  1.2969]], dtype=torch.bfloat16))]), logits=tensor([[-3.6719, -1.7969,  0.4316,  2.8906,  1.2969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.8906,  2.0625, -0.1895, -2.2500, -2.4219]], dtype=torch.bfloat16))]), logits=tensor([[ 3.8906,  2.0625, -0.1895, -2.2500, -2.4219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.9141,  0.5234,  1.1719,  0.8086, -1.0703]], dtype=torch.bfloat16))]), logits=tensor([[-1.9141,  0.5234,  1.1719,  0.8086, -1.0703]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.8867,  1.2812,  1.3672, -0.0033, -1.8672]], dtype=torch.bfloat16))]), logits=tensor([[-0.8867,  1.2812,  1.3672, -0.0033, -1.8672]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2656, -0.9102,  1.3984,  2.3438, -0.2305]], dtype=torch.bfloat16))]), logits=tensor([[-3.2656, -0.9102,  1.3984,  2.3438, -0.2305]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.0625,  2.3125,  0.9297, -1.6406, -2.9062]], dtype=torch.bfloat16))]), logits=tensor([[ 2.0625,  2.3125,  0.9297, -1.6406, -2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3281, -3.1562, -1.9531,  2.6875,  4.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.3281, -3.1562, -1.9531,  2.6875,  4.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5938, -2.8438, -1.4922,  2.7344,  3.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.5938, -2.8438, -1.4922,  2.7344,  3.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7812, -2.3438, -0.3750,  2.9844,  2.5312]], dtype=torch.bfloat16))]), logits=tensor([[-3.7812, -2.3438, -0.3750,  2.9844,  2.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.8984,  0.9258,  1.9844,  0.7227, -2.1250]], dtype=torch.bfloat16))]), logits=tensor([[-1.8984,  0.9258,  1.9844,  0.7227, -2.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.9688,  1.4297, -0.4395, -1.9531, -1.7891]], dtype=torch.bfloat16))]), logits=tensor([[ 3.9688,  1.4297, -0.4395, -1.9531, -1.7891]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9062, -2.9844, -2.4062,  2.2344,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.9062, -2.9844, -2.4062,  2.2344,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5312, -2.8438, -2.4531,  1.9062,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.5312, -2.8438, -2.4531,  1.9062,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9688, -3.0469, -2.3750,  2.3750,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.9688, -3.0469, -2.3750,  2.3750,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.9688,  2.3281,  1.4922, -1.2109, -3.1875]], dtype=torch.bfloat16))]), logits=tensor([[ 0.9688,  2.3281,  1.4922, -1.2109, -3.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7969, -2.9844, -2.4219,  2.2031,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.7969, -2.9844, -2.4219,  2.2031,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.7656,  2.2812,  0.5117, -1.9219, -2.7656]], dtype=torch.bfloat16))]), logits=tensor([[ 2.7656,  2.2812,  0.5117, -1.9219, -2.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.3438, -0.8398,  2.7500,  2.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.3438, -0.8398,  2.7500,  2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -3.0312, -0.9570,  3.5625,  3.3281]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -3.0312, -0.9570,  3.5625,  3.3281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8750, -0.1328,  1.8438,  1.6172, -1.0547]], dtype=torch.bfloat16))]), logits=tensor([[-2.8750, -0.1328,  1.8438,  1.6172, -1.0547]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.5312,  0.6445,  1.0156,  0.4863, -1.0703]], dtype=torch.bfloat16))]), logits=tensor([[-1.5312,  0.6445,  1.0156,  0.4863, -1.0703]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.0781,  2.1406,  0.6758, -1.5625, -2.7344]], dtype=torch.bfloat16))]), logits=tensor([[ 2.0781,  2.1406,  0.6758, -1.5625, -2.7344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7188, -2.8438, -2.4844,  2.0312,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.7188, -2.8438, -2.4844,  2.0312,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.9180,  1.1094, -0.1050, -0.8711, -1.1641]], dtype=torch.bfloat16))]), logits=tensor([[ 0.9180,  1.1094, -0.1050, -0.8711, -1.1641]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3125,  0.2471,  1.4219,  1.1719, -1.0938]], dtype=torch.bfloat16))]), logits=tensor([[-2.3125,  0.2471,  1.4219,  1.1719, -1.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6719, -1.6953,  0.7148,  2.9531,  0.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.6719, -1.6953,  0.7148,  2.9531,  0.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1562, -3.0469, -2.2344,  2.5156,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.1562, -3.0469, -2.2344,  2.5156,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7344, -2.9219, -2.4375,  2.0156,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7344, -2.9219, -2.4375,  2.0156,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -3.1250, -2.3281,  2.6250,  5.0312]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -3.1250, -2.3281,  2.6250,  5.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.9766,  0.7500,  2.3438,  0.7891, -2.3125]], dtype=torch.bfloat16))]), logits=tensor([[-1.9766,  0.7500,  2.3438,  0.7891, -2.3125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6250, -0.0408,  1.8281,  1.5859, -1.2969]], dtype=torch.bfloat16))]), logits=tensor([[-2.6250, -0.0408,  1.8281,  1.5859, -1.2969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.8906,  1.2031, -0.5156, -1.8359, -1.6172]], dtype=torch.bfloat16))]), logits=tensor([[ 3.8906,  1.2031, -0.5156, -1.8359, -1.6172]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.5156,  2.3281,  0.1318, -2.2344, -2.7031]], dtype=torch.bfloat16))]), logits=tensor([[ 3.5156,  2.3281,  0.1318, -2.2344, -2.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -2.9219, -1.9453,  2.7812,  4.3438]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -2.9219, -1.9453,  2.7812,  4.3438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9219, -3.0625, -2.4062,  2.3750,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.9219, -3.0625, -2.4062,  2.3750,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6250, -2.8750, -2.4844,  2.0000,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.6250, -2.8750, -2.4844,  2.0000,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -3.1250, -1.9219,  2.8750,  4.5312]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -3.1250, -1.9219,  2.8750,  4.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8438, -2.5625, -0.3164,  3.4219,  2.4688]], dtype=torch.bfloat16))]), logits=tensor([[-3.8438, -2.5625, -0.3164,  3.4219,  2.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -3.1406, -1.7812,  3.0469,  4.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -3.1406, -1.7812,  3.0469,  4.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.5156,  2.3281,  0.6641, -1.8438, -2.8438]], dtype=torch.bfloat16))]), logits=tensor([[ 2.5156,  2.3281,  0.6641, -1.8438, -2.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.9219,  1.1875, -0.5078, -1.8516, -1.6250]], dtype=torch.bfloat16))]), logits=tensor([[ 3.9219,  1.1875, -0.5078, -1.8516, -1.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4531, -2.7812, -0.8945,  2.9844,  3.1250]], dtype=torch.bfloat16))]), logits=tensor([[-3.4531, -2.7812, -0.8945,  2.9844,  3.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.9375, -1.1094,  3.2656,  3.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.9375, -1.1094,  3.2656,  3.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3906, -3.2344, -1.9219,  3.1250,  4.5312]], dtype=torch.bfloat16))]), logits=tensor([[-3.3906, -3.2344, -1.9219,  3.1250,  4.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.7656, -0.5625,  3.4219,  2.7969]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.7656, -0.5625,  3.4219,  2.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.0703,  2.2031,  1.3047, -1.1406, -3.0000]], dtype=torch.bfloat16))]), logits=tensor([[ 1.0703,  2.2031,  1.3047, -1.1406, -3.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3438, -2.6406, -2.3125,  1.6875,  4.9062]], dtype=torch.bfloat16))]), logits=tensor([[-2.3438, -2.6406, -2.3125,  1.6875,  4.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7031, -2.9531, -2.4844,  2.1094,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7031, -2.9531, -2.4844,  2.1094,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0469, -2.8438, -1.9375,  2.2969,  4.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.0469, -2.8438, -1.9375,  2.2969,  4.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2188, -2.8594, -1.8047,  2.5938,  4.1875]], dtype=torch.bfloat16))]), logits=tensor([[-3.2188, -2.8594, -1.8047,  2.5938,  4.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -1.3828,  1.1016,  2.7969,  0.3926]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -1.3828,  1.1016,  2.7969,  0.3926]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -3.0625, -1.8906,  2.8750,  4.4062]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -3.0625, -1.8906,  2.8750,  4.4062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3125,  0.4199,  2.0469,  1.0938, -1.7578]], dtype=torch.bfloat16))]), logits=tensor([[-2.3125,  0.4199,  2.0469,  1.0938, -1.7578]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6250, -1.7500,  0.8750,  2.9844,  0.8516]], dtype=torch.bfloat16))]), logits=tensor([[-3.6250, -1.7500,  0.8750,  2.9844,  0.8516]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -3.0625, -1.9219,  2.8125,  4.3438]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -3.0625, -1.9219,  2.8125,  4.3438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.9375, -1.5156,  2.8594,  3.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.9375, -1.5156,  2.8594,  3.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.1157,  1.3203,  0.6055, -0.5664, -1.5938]], dtype=torch.bfloat16))]), logits=tensor([[ 0.1157,  1.3203,  0.6055, -0.5664, -1.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8906, -2.3906, -0.4609,  3.0625,  2.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.8906, -2.3906, -0.4609,  3.0625,  2.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.4453,  1.2500,  1.8672,  0.4609, -2.3281]], dtype=torch.bfloat16))]), logits=tensor([[-1.4453,  1.2500,  1.8672,  0.4609, -2.3281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6562, -2.9531, -2.4531,  2.1250,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.6562, -2.9531, -2.4531,  2.1250,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5469, -2.7188, -1.1875,  2.9375,  3.3750]], dtype=torch.bfloat16))]), logits=tensor([[-3.5469, -2.7188, -1.1875,  2.9375,  3.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -3.1406, -2.1094,  2.8281,  4.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -3.1406, -2.1094,  2.8281,  4.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -2.9375, -2.0312,  2.4531,  4.5625]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -2.9375, -2.0312,  2.4531,  4.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -1.3984,  0.8867,  2.6562,  0.7578]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -1.3984,  0.8867,  2.6562,  0.7578]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.2188,  2.4062,  0.3887, -2.1406, -2.8594]], dtype=torch.bfloat16))]), logits=tensor([[ 3.2188,  2.4062,  0.3887, -2.1406, -2.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.2500,  0.6094,  1.7812,  0.9414, -1.5312]], dtype=torch.bfloat16))]), logits=tensor([[-2.2500,  0.6094,  1.7812,  0.9414, -1.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.6875,  2.2031,  0.0056, -2.2344, -2.5625]], dtype=torch.bfloat16))]), logits=tensor([[ 3.6875,  2.2031,  0.0056, -2.2344, -2.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3281,  0.2441,  1.4062,  0.9961, -0.9531]], dtype=torch.bfloat16))]), logits=tensor([[-2.3281,  0.2441,  1.4062,  0.9961, -0.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6719, -2.6094, -0.7969,  2.8438,  3.0156]], dtype=torch.bfloat16))]), logits=tensor([[-3.6719, -2.6094, -0.7969,  2.8438,  3.0156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1094, -1.9922, -1.1094,  1.8594,  3.1719]], dtype=torch.bfloat16))]), logits=tensor([[-3.1094, -1.9922, -1.1094,  1.8594,  3.1719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.4531,  1.2969,  2.0469,  0.3457, -2.4688]], dtype=torch.bfloat16))]), logits=tensor([[-1.4531,  1.2969,  2.0469,  0.3457, -2.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7812, -2.5000, -0.7461,  2.9688,  2.8906]], dtype=torch.bfloat16))]), logits=tensor([[-3.7812, -2.5000, -0.7461,  2.9688,  2.8906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -1.8281, -0.7070,  1.8047,  2.5938]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -1.8281, -0.7070,  1.8047,  2.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9062, -0.6758,  0.9805,  1.9453, -0.0332]], dtype=torch.bfloat16))]), logits=tensor([[-2.9062, -0.6758,  0.9805,  1.9453, -0.0332]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.9453,  1.0859,  2.2656,  0.5938, -2.3750]], dtype=torch.bfloat16))]), logits=tensor([[-1.9453,  1.0859,  2.2656,  0.5938, -2.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.2559,  2.0938,  2.1094, -0.3945, -3.2500]], dtype=torch.bfloat16))]), logits=tensor([[-0.2559,  2.0938,  2.1094, -0.3945, -3.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.4609,  0.7227,  0.9961,  0.5078, -1.2344]], dtype=torch.bfloat16))]), logits=tensor([[-1.4609,  0.7227,  0.9961,  0.5078, -1.2344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1094, -3.0781, -2.2812,  2.4688,  5.0938]], dtype=torch.bfloat16))]), logits=tensor([[-3.1094, -3.0781, -2.2812,  2.4688,  5.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.1904,  2.0469,  1.9453, -0.4316, -3.0312]], dtype=torch.bfloat16))]), logits=tensor([[-0.1904,  2.0469,  1.9453, -0.4316, -3.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -3.0938, -1.5469,  3.1875,  4.0938]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -3.0938, -1.5469,  3.1875,  4.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7656, -0.4941,  1.0781,  1.7969, -0.2969]], dtype=torch.bfloat16))]), logits=tensor([[-2.7656, -0.4941,  1.0781,  1.7969, -0.2969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -1.7734,  0.0258,  2.3750,  1.8594]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -1.7734,  0.0258,  2.3750,  1.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2812, -1.4609,  0.7070,  2.6875,  0.6914]], dtype=torch.bfloat16))]), logits=tensor([[-3.2812, -1.4609,  0.7070,  2.6875,  0.6914]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.4375,  2.3906,  0.2256, -2.2188, -2.7969]], dtype=torch.bfloat16))]), logits=tensor([[ 3.4375,  2.3906,  0.2256, -2.2188, -2.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.8906,  2.3750,  0.4512, -2.0469, -2.8281]], dtype=torch.bfloat16))]), logits=tensor([[ 2.8906,  2.3750,  0.4512, -2.0469, -2.8281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7812, -3.0781, -1.3906,  3.2969,  3.8906]], dtype=torch.bfloat16))]), logits=tensor([[-3.7812, -3.0781, -1.3906,  3.2969,  3.8906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7812, -2.5000, -0.6289,  3.1250,  2.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.7812, -2.5000, -0.6289,  3.1250,  2.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7969, -2.9062, -2.3750,  2.0938,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.7969, -2.9062, -2.3750,  2.0938,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 4.0938,  1.7344, -0.3301, -2.1406, -2.1250]], dtype=torch.bfloat16))]), logits=tensor([[ 4.0938,  1.7344, -0.3301, -2.1406, -2.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.6250,  0.9219, -0.5586, -1.6406, -1.3594]], dtype=torch.bfloat16))]), logits=tensor([[ 3.6250,  0.9219, -0.5586, -1.6406, -1.3594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.1016,  1.5781,  2.0312,  0.1396, -2.6562]], dtype=torch.bfloat16))]), logits=tensor([[-1.1016,  1.5781,  2.0312,  0.1396, -2.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.9844, -1.7031,  2.9375,  4.1250]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.9844, -1.7031,  2.9375,  4.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9531, -2.9531, -2.3906,  2.2969,  5.0938]], dtype=torch.bfloat16))]), logits=tensor([[-2.9531, -2.9531, -2.3906,  2.2969,  5.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -2.1562, -0.3086,  2.9844,  2.2344]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -2.1562, -0.3086,  2.9844,  2.2344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0000, -2.9531, -2.3750,  2.2500,  5.0938]], dtype=torch.bfloat16))]), logits=tensor([[-3.0000, -2.9531, -2.3750,  2.2500,  5.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.1406,  1.8516,  2.5000, -0.1157, -3.2344]], dtype=torch.bfloat16))]), logits=tensor([[-1.1406,  1.8516,  2.5000, -0.1157, -3.2344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5938, -2.3594, -0.5625,  2.8750,  2.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.5938, -2.3594, -0.5625,  2.8750,  2.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6406, -0.2910,  1.6094,  1.7031, -0.9258]], dtype=torch.bfloat16))]), logits=tensor([[-2.6406, -0.2910,  1.6094,  1.7031, -0.9258]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -2.0312, -0.7617,  2.4531,  2.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -2.0312, -0.7617,  2.4531,  2.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4531, -1.1953,  1.1719,  2.5469,  0.2461]], dtype=torch.bfloat16))]), logits=tensor([[-3.4531, -1.1953,  1.1719,  2.5469,  0.2461]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2031, -3.1719, -2.2812,  2.7031,  5.0312]], dtype=torch.bfloat16))]), logits=tensor([[-3.2031, -3.1719, -2.2812,  2.7031,  5.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.2656,  0.4570,  2.3750,  1.0234, -2.1094]], dtype=torch.bfloat16))]), logits=tensor([[-2.2656,  0.4570,  2.3750,  1.0234, -2.1094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.4375,  2.3438,  0.8086, -1.8203, -2.9219]], dtype=torch.bfloat16))]), logits=tensor([[ 2.4375,  2.3438,  0.8086, -1.8203, -2.9219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -1.9062,  0.4883,  3.0000,  1.3516]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -1.9062,  0.4883,  3.0000,  1.3516]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.5312,  2.0469, -0.1045, -2.1094, -2.2969]], dtype=torch.bfloat16))]), logits=tensor([[ 3.5312,  2.0469, -0.1045, -2.1094, -2.2969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -1.2656,  0.8398,  2.5469,  0.6602]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -1.2656,  0.8398,  2.5469,  0.6602]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7812, -2.4375, -0.1001,  3.3125,  2.2031]], dtype=torch.bfloat16))]), logits=tensor([[-3.7812, -2.4375, -0.1001,  3.3125,  2.2031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.6875,  2.4844,  0.7812, -1.9766, -3.0469]], dtype=torch.bfloat16))]), logits=tensor([[ 2.6875,  2.4844,  0.7812, -1.9766, -3.0469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8906, -3.0156, -2.3906,  2.3125,  5.0938]], dtype=torch.bfloat16))]), logits=tensor([[-2.8906, -3.0156, -2.3906,  2.3125,  5.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7969, -1.8828,  0.4707,  3.0156,  1.3906]], dtype=torch.bfloat16))]), logits=tensor([[-3.7969, -1.8828,  0.4707,  3.0156,  1.3906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.3828,  2.4844,  1.5156, -1.4297, -3.3594]], dtype=torch.bfloat16))]), logits=tensor([[ 1.3828,  2.4844,  1.5156, -1.4297, -3.3594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3906, -1.4297,  1.2578,  2.7344,  0.2832]], dtype=torch.bfloat16))]), logits=tensor([[-3.3906, -1.4297,  1.2578,  2.7344,  0.2832]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4531, -1.2031, -0.8867,  1.2188,  2.2500]], dtype=torch.bfloat16))]), logits=tensor([[-2.4531, -1.2031, -0.8867,  1.2188,  2.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8281, -3.1094, -1.2109,  3.3750,  3.7344]], dtype=torch.bfloat16))]), logits=tensor([[-3.8281, -3.1094, -1.2109,  3.3750,  3.7344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8125, -2.5000, -0.2402,  3.3594,  2.3750]], dtype=torch.bfloat16))]), logits=tensor([[-3.8125, -2.5000, -0.2402,  3.3594,  2.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -2.7969, -1.6406,  2.6406,  3.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -2.7969, -1.6406,  2.6406,  3.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -3.0000, -2.0781,  2.4688,  4.7812]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -3.0000, -2.0781,  2.4688,  4.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3125, -2.9531, -1.8828,  2.7656,  4.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.3125, -2.9531, -1.8828,  2.7656,  4.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6719, -0.1084,  1.9922,  1.3516, -1.1094]], dtype=torch.bfloat16))]), logits=tensor([[-2.6719, -0.1084,  1.9922,  1.3516, -1.1094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7344,  1.0234, -0.5195, -1.7188, -1.4766]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7344,  1.0234, -0.5195, -1.7188, -1.4766]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3125, -0.9648,  1.5000,  2.3906, -0.1865]], dtype=torch.bfloat16))]), logits=tensor([[-3.3125, -0.9648,  1.5000,  2.3906, -0.1865]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7031, -0.0830,  1.8984,  1.6406, -1.2812]], dtype=torch.bfloat16))]), logits=tensor([[-2.7031, -0.0830,  1.8984,  1.6406, -1.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2812, -3.1406, -2.1406,  2.7500,  4.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.2812, -3.1406, -2.1406,  2.7500,  4.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4531, -2.5312, -1.0781,  2.5312,  3.3125]], dtype=torch.bfloat16))]), logits=tensor([[-3.4531, -2.5312, -1.0781,  2.5312,  3.3125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8906, -2.9375, -0.9023,  3.4062,  3.3125]], dtype=torch.bfloat16))]), logits=tensor([[-3.8906, -2.9375, -0.9023,  3.4062,  3.3125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9219, -2.6719, -2.3281,  2.0625,  4.7500]], dtype=torch.bfloat16))]), logits=tensor([[-2.9219, -2.6719, -2.3281,  2.0625,  4.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5469, -2.4062, -0.0869,  3.2500,  2.1094]], dtype=torch.bfloat16))]), logits=tensor([[-3.5469, -2.4062, -0.0869,  3.2500,  2.1094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -3.1719, -2.0938,  2.9531,  4.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -3.1719, -2.0938,  2.9531,  4.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -2.7969, -1.4375,  2.8906,  3.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -2.7969, -1.4375,  2.8906,  3.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -3.1406, -2.2344,  2.7031,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -3.1406, -2.2344,  2.7031,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -1.3750,  0.4805,  2.3281,  1.0234]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -1.3750,  0.4805,  2.3281,  1.0234]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7188, -2.7969, -0.8984,  3.3281,  3.0781]], dtype=torch.bfloat16))]), logits=tensor([[-3.7188, -2.7969, -0.8984,  3.3281,  3.0781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.6406,  1.1953,  2.1250,  0.4551, -2.3906]], dtype=torch.bfloat16))]), logits=tensor([[-1.6406,  1.1953,  2.1250,  0.4551, -2.3906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5938, -2.9375, -2.5000,  2.0469,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.5938, -2.9375, -2.5000,  2.0469,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.3965,  2.0000,  1.4688, -0.6758, -2.7812]], dtype=torch.bfloat16))]), logits=tensor([[ 0.3965,  2.0000,  1.4688, -0.6758, -2.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -3.1250, -2.2500,  2.6406,  4.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -3.1250, -2.2500,  2.6406,  4.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8125, -2.8594, -2.4062,  2.1094,  5.0000]], dtype=torch.bfloat16))]), logits=tensor([[-2.8125, -2.8594, -2.4062,  2.1094,  5.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.9141,  2.2656,  0.9141, -1.5625, -2.8438]], dtype=torch.bfloat16))]), logits=tensor([[ 1.9141,  2.2656,  0.9141, -1.5625, -2.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3594, -2.7344, -1.6719,  2.4219,  4.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.3594, -2.7344, -1.6719,  2.4219,  4.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.3281,  1.5234,  2.1562,  0.2061, -2.6719]], dtype=torch.bfloat16))]), logits=tensor([[-1.3281,  1.5234,  2.1562,  0.2061, -2.6719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.2188, -0.1152,  0.4785,  1.0000,  0.0098]], dtype=torch.bfloat16))]), logits=tensor([[-2.2188, -0.1152,  0.4785,  1.0000,  0.0098]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.6562,  2.2656,  0.4414, -1.8672, -2.7031]], dtype=torch.bfloat16))]), logits=tensor([[ 2.6562,  2.2656,  0.4414, -1.8672, -2.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3125, -2.6875, -1.5391,  2.2500,  4.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.3125, -2.6875, -1.5391,  2.2500,  4.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -3.1250, -2.3438,  2.5781,  5.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -3.1250, -2.3438,  2.5781,  5.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.9219, -1.5156,  2.9844,  3.8906]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.9219, -1.5156,  2.9844,  3.8906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6250, -2.8594, -2.4375,  2.0000,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.6250, -2.8594, -2.4375,  2.0000,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3750,  0.5781,  2.0625,  0.9414, -1.7656]], dtype=torch.bfloat16))]), logits=tensor([[-2.3750,  0.5781,  2.0625,  0.9414, -1.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7812, -0.3262,  1.5859,  1.7656, -0.8633]], dtype=torch.bfloat16))]), logits=tensor([[-2.7812, -0.3262,  1.5859,  1.7656, -0.8633]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3906, -1.2188,  0.4492,  2.1250,  1.0703]], dtype=torch.bfloat16))]), logits=tensor([[-3.3906, -1.2188,  0.4492,  2.1250,  1.0703]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.4688,  2.0312,  0.6602, -1.3828, -2.4375]], dtype=torch.bfloat16))]), logits=tensor([[ 1.4688,  2.0312,  0.6602, -1.3828, -2.4375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.0864,  2.0781,  1.8438, -0.5625, -3.0156]], dtype=torch.bfloat16))]), logits=tensor([[ 0.0864,  2.0781,  1.8438, -0.5625, -3.0156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8125, -2.5781, -0.5508,  3.0938,  2.8281]], dtype=torch.bfloat16))]), logits=tensor([[-3.8125, -2.5781, -0.5508,  3.0938,  2.8281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.8359,  2.2031,  1.4297, -1.0078, -3.0000]], dtype=torch.bfloat16))]), logits=tensor([[ 0.8359,  2.2031,  1.4297, -1.0078, -3.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.6992,  1.8203,  1.9844, -0.1543, -2.8281]], dtype=torch.bfloat16))]), logits=tensor([[-0.6992,  1.8203,  1.9844, -0.1543, -2.8281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8750, -3.0312, -2.4219,  2.2969,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.8750, -3.0312, -2.4219,  2.2969,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8281, -0.2637,  1.7656,  1.7969, -1.0234]], dtype=torch.bfloat16))]), logits=tensor([[-2.8281, -0.2637,  1.7656,  1.7969, -1.0234]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7500, -2.9219, -2.4531,  2.1094,  5.2500]], dtype=torch.bfloat16))]), logits=tensor([[-2.7500, -2.9219, -2.4531,  2.1094,  5.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -3.0625, -2.3281,  2.4062,  5.0938]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -3.0625, -2.3281,  2.4062,  5.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -3.2031, -2.0938,  2.9219,  4.7188]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -3.2031, -2.0938,  2.9219,  4.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 4.1875,  1.7500, -0.3184, -2.1875, -2.1719]], dtype=torch.bfloat16))]), logits=tensor([[ 4.1875,  1.7500, -0.3184, -2.1875, -2.1719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9844, -3.0156, -2.3438,  2.2969,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.9844, -3.0156, -2.3438,  2.2969,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8438, -2.5156, -0.2773,  3.1875,  2.4844]], dtype=torch.bfloat16))]), logits=tensor([[-3.8438, -2.5156, -0.2773,  3.1875,  2.4844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -1.0859,  1.4766,  2.5312, -0.1230]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -1.0859,  1.4766,  2.5312, -0.1230]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -2.7969, -1.8047,  2.5000,  4.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -2.7969, -1.8047,  2.5000,  4.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -3.0469, -1.3750,  3.1406,  3.7969]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -3.0469, -1.3750,  3.1406,  3.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -3.1562, -1.8203,  3.0000,  4.4688]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -3.1562, -1.8203,  3.0000,  4.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4219, -2.8281, -2.4688,  1.8438,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.4219, -2.8281, -2.4688,  1.8438,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -2.7500, -1.6328,  2.5938,  4.0938]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -2.7500, -1.6328,  2.5938,  4.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -2.9375, -2.1719,  2.2656,  4.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -2.9375, -2.1719,  2.2656,  4.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.9141,  2.5000,  1.3047, -1.7031, -3.2500]], dtype=torch.bfloat16))]), logits=tensor([[ 1.9141,  2.5000,  1.3047, -1.7031, -3.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8438, -2.6406, -0.3555,  3.3281,  2.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.8438, -2.6406, -0.3555,  3.3281,  2.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.7188, -1.2422,  2.8594,  3.5469]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.7188, -1.2422,  2.8594,  3.5469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.1953,  2.1250,  1.1172, -1.1562, -2.7969]], dtype=torch.bfloat16))]), logits=tensor([[ 1.1953,  2.1250,  1.1172, -1.1562, -2.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -2.5156, -0.5586,  3.1406,  2.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -2.5156, -0.5586,  3.1406,  2.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.3906,  0.8984, -0.5391, -1.5234, -1.2188]], dtype=torch.bfloat16))]), logits=tensor([[ 3.3906,  0.8984, -0.5391, -1.5234, -1.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6562, -2.8125, -2.4375,  1.9609,  5.0938]], dtype=torch.bfloat16))]), logits=tensor([[-2.6562, -2.8125, -2.4375,  1.9609,  5.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9688, -0.4121,  1.7969,  1.9844, -0.9336]], dtype=torch.bfloat16))]), logits=tensor([[-2.9688, -0.4121,  1.7969,  1.9844, -0.9336]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2969, -3.0156, -1.9219,  2.5781,  4.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.2969, -3.0156, -1.9219,  2.5781,  4.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5625, -2.6875, -2.4375,  1.7656,  5.0312]], dtype=torch.bfloat16))]), logits=tensor([[-2.5625, -2.6875, -2.4375,  1.7656,  5.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8594, -0.1416,  1.7656,  1.4141, -0.9414]], dtype=torch.bfloat16))]), logits=tensor([[-2.8594, -0.1416,  1.7656,  1.4141, -0.9414]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7969, -2.9688, -2.4375,  2.1719,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7969, -2.9688, -2.4375,  2.1719,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2344, -2.8750, -2.1094,  2.3906,  4.5938]], dtype=torch.bfloat16))]), logits=tensor([[-3.2344, -2.8750, -2.1094,  2.3906,  4.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6719, -3.2031, -1.5547,  3.2344,  4.1562]], dtype=torch.bfloat16))]), logits=tensor([[-3.6719, -3.2031, -1.5547,  3.2344,  4.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.8594, -0.9180,  3.3125,  3.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.8594, -0.9180,  3.3125,  3.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5156, -0.3965,  0.5234,  1.1562,  0.3750]], dtype=torch.bfloat16))]), logits=tensor([[-2.5156, -0.3965,  0.5234,  1.1562,  0.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -2.0781, -0.3887,  2.6562,  2.3594]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -2.0781, -0.3887,  2.6562,  2.3594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.9258,  2.3438,  1.6328, -1.1484, -3.2969]], dtype=torch.bfloat16))]), logits=tensor([[ 0.9258,  2.3438,  1.6328, -1.1484, -3.2969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -3.1562, -2.1562,  2.6719,  4.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -3.1562, -2.1562,  2.6719,  4.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7969, -2.3594,  0.1021,  3.3594,  1.9766]], dtype=torch.bfloat16))]), logits=tensor([[-3.7969, -2.3594,  0.1021,  3.3594,  1.9766]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8906, -3.0469, -2.3906,  2.3125,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.8906, -3.0469, -2.3906,  2.3125,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -3.2188, -1.9922,  3.0469,  4.5938]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -3.2188, -1.9922,  3.0469,  4.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7344, -2.9219, -2.4219,  2.0156,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.7344, -2.9219, -2.4219,  2.0156,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.8203,  0.8867,  1.9453,  0.6328, -2.0781]], dtype=torch.bfloat16))]), logits=tensor([[-1.8203,  0.8867,  1.9453,  0.6328, -2.0781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1562, -3.1094, -2.2969,  2.5625,  5.0000]], dtype=torch.bfloat16))]), logits=tensor([[-3.1562, -3.1094, -2.2969,  2.5625,  5.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4844,  0.3301,  2.0469,  1.3203, -1.7266]], dtype=torch.bfloat16))]), logits=tensor([[-2.4844,  0.3301,  2.0469,  1.3203, -1.7266]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -3.1875, -1.7422,  3.2500,  4.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -3.1875, -1.7422,  3.2500,  4.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6719, -2.9062, -2.4688,  2.0312,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.6719, -2.9062, -2.4688,  2.0312,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.8906,  2.4219,  0.5586, -2.0469, -2.9062]], dtype=torch.bfloat16))]), logits=tensor([[ 2.8906,  2.4219,  0.5586, -2.0469, -2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2812, -1.2109,  0.9961,  2.5156,  0.3242]], dtype=torch.bfloat16))]), logits=tensor([[-3.2812, -1.2109,  0.9961,  2.5156,  0.3242]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3594, -1.0000,  1.1641,  2.3438,  0.1143]], dtype=torch.bfloat16))]), logits=tensor([[-3.3594, -1.0000,  1.1641,  2.3438,  0.1143]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.8672,  1.0859,  2.1406,  0.5781, -2.2969]], dtype=torch.bfloat16))]), logits=tensor([[-1.8672,  1.0859,  2.1406,  0.5781, -2.2969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.4531,  2.1250,  0.9766, -1.2812, -2.7344]], dtype=torch.bfloat16))]), logits=tensor([[ 1.4531,  2.1250,  0.9766, -1.2812, -2.7344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5156, -2.7969, -2.4531,  1.8594,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.5156, -2.7969, -2.4531,  1.8594,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.2812,  2.4375,  0.2930, -2.2188, -2.8281]], dtype=torch.bfloat16))]), logits=tensor([[ 3.2812,  2.4375,  0.2930, -2.2188, -2.8281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5625, -2.7500, -1.1172,  2.7344,  3.5312]], dtype=torch.bfloat16))]), logits=tensor([[-3.5625, -2.7500, -1.1172,  2.7344,  3.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6250, -2.9062, -2.4688,  2.0312,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.6250, -2.9062, -2.4688,  2.0312,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8125, -2.9688, -2.4219,  2.1875,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.8125, -2.9688, -2.4219,  2.1875,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -3.0938, -2.2656,  2.5781,  5.0000]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -3.0938, -2.2656,  2.5781,  5.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -1.6484,  0.8711,  2.9219,  0.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -1.6484,  0.8711,  2.9219,  0.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.9766,  1.6797,  2.0625,  0.0913, -2.7812]], dtype=torch.bfloat16))]), logits=tensor([[-0.9766,  1.6797,  2.0625,  0.0913, -2.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -1.1953,  0.8750,  2.3281,  0.5898]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -1.1953,  0.8750,  2.3281,  0.5898]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.9688,  2.0781,  1.2109, -0.9453, -2.8906]], dtype=torch.bfloat16))]), logits=tensor([[ 0.9688,  2.0781,  1.2109, -0.9453, -2.8906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -3.0938, -1.5391,  3.2344,  3.9844]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -3.0938, -1.5391,  3.2344,  3.9844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -3.0312, -1.1562,  3.3906,  3.5938]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -3.0312, -1.1562,  3.3906,  3.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -0.8594,  1.0859,  2.1562, -0.0405]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -0.8594,  1.0859,  2.1562, -0.0405]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.2812,  0.4922,  1.8047,  0.9414, -1.4219]], dtype=torch.bfloat16))]), logits=tensor([[-2.2812,  0.4922,  1.8047,  0.9414, -1.4219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8438, -3.0000, -2.4375,  2.2969,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.8438, -3.0000, -2.4375,  2.2969,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8750, -3.0312, -2.4219,  2.2656,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.8750, -3.0312, -2.4219,  2.2656,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7656, -0.9258, -0.0095,  1.5000,  1.2344]], dtype=torch.bfloat16))]), logits=tensor([[-2.7656, -0.9258, -0.0095,  1.5000,  1.2344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.5703,  1.4141,  2.5156,  0.2812, -2.8750]], dtype=torch.bfloat16))]), logits=tensor([[-1.5703,  1.4141,  2.5156,  0.2812, -2.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7188, -2.9375, -1.4688,  3.0469,  3.8906]], dtype=torch.bfloat16))]), logits=tensor([[-3.7188, -2.9375, -1.4688,  3.0469,  3.8906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.3906,  2.3906,  1.4297, -1.3594, -3.2344]], dtype=torch.bfloat16))]), logits=tensor([[ 1.3906,  2.3906,  1.4297, -1.3594, -3.2344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -1.3516,  1.0234,  2.4844,  0.5508]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -1.3516,  1.0234,  2.4844,  0.5508]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.6094,  2.1094,  0.2461, -1.8047, -2.4531]], dtype=torch.bfloat16))]), logits=tensor([[ 2.6094,  2.1094,  0.2461, -1.8047, -2.4531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9219, -2.9844, -2.4219,  2.2188,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.9219, -2.9844, -2.4219,  2.2188,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7344, -2.9531, -2.4531,  2.0938,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7344, -2.9531, -2.4531,  2.0938,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4062, -3.1562, -2.0625,  2.9844,  4.5625]], dtype=torch.bfloat16))]), logits=tensor([[-3.4062, -3.1562, -2.0625,  2.9844,  4.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.8594,  1.9062, -0.2598, -2.1406, -2.2031]], dtype=torch.bfloat16))]), logits=tensor([[ 3.8594,  1.9062, -0.2598, -2.1406, -2.2031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -2.5156, -0.4434,  3.1875,  2.5156]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -2.5156, -0.4434,  3.1875,  2.5156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8594, -2.3906, -0.2314,  3.1406,  2.3906]], dtype=torch.bfloat16))]), logits=tensor([[-3.8594, -2.3906, -0.2314,  3.1406,  2.3906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8281, -3.0156, -1.3125,  3.1406,  3.8594]], dtype=torch.bfloat16))]), logits=tensor([[-3.8281, -3.0156, -1.3125,  3.1406,  3.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -3.1250, -2.3125,  2.6875,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -3.1250, -2.3125,  2.6875,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.4199,  1.9766,  2.1406, -0.3398, -3.3906]], dtype=torch.bfloat16))]), logits=tensor([[-0.4199,  1.9766,  2.1406, -0.3398, -3.3906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6875, -2.9531, -2.5000,  2.1406,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.6875, -2.9531, -2.5000,  2.1406,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.1562,  2.2812,  0.2773, -2.0781, -2.6875]], dtype=torch.bfloat16))]), logits=tensor([[ 3.1562,  2.2812,  0.2773, -2.0781, -2.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3281,  0.1367,  0.7930,  0.8164, -0.2754]], dtype=torch.bfloat16))]), logits=tensor([[-2.3281,  0.1367,  0.7930,  0.8164, -0.2754]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -1.5078,  0.8906,  2.7344,  0.7539]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -1.5078,  0.8906,  2.7344,  0.7539]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7812,  1.1797, -0.5195, -1.7734, -1.5625]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7812,  1.1797, -0.5195, -1.7734, -1.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5156, -2.7969, -2.4219,  1.8047,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.5156, -2.7969, -2.4219,  1.8047,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -1.7656,  0.5547,  2.6094,  1.2109]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -1.7656,  0.5547,  2.6094,  1.2109]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.1719,  2.3750,  0.3242, -2.1406, -2.7656]], dtype=torch.bfloat16))]), logits=tensor([[ 3.1719,  2.3750,  0.3242, -2.1406, -2.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1562, -3.1250, -2.1875,  2.7188,  4.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.1562, -3.1250, -2.1875,  2.7188,  4.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -3.1562, -2.1094,  2.8594,  4.7812]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -3.1562, -2.1094,  2.8594,  4.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.7617,  1.9922,  2.4062, -0.1377, -3.3594]], dtype=torch.bfloat16))]), logits=tensor([[-0.7617,  1.9922,  2.4062, -0.1377, -3.3594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7188, -2.9062, -2.4844,  2.0312,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7188, -2.9062, -2.4844,  2.0312,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -1.8359,  0.4473,  2.9062,  1.3047]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -1.8359,  0.4473,  2.9062,  1.3047]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0000, -0.8320,  1.0234,  2.1250,  0.0190]], dtype=torch.bfloat16))]), logits=tensor([[-3.0000, -0.8320,  1.0234,  2.1250,  0.0190]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2969, -1.4453,  1.0312,  2.5781,  0.5469]], dtype=torch.bfloat16))]), logits=tensor([[-3.2969, -1.4453,  1.0312,  2.5781,  0.5469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -2.9844, -1.5312,  2.9062,  4.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -2.9844, -1.5312,  2.9062,  4.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.8906,  1.0703,  2.3438,  0.5508, -2.4062]], dtype=torch.bfloat16))]), logits=tensor([[-1.8906,  1.0703,  2.3438,  0.5508, -2.4062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.2598,  2.2188,  1.9062, -0.6992, -3.2656]], dtype=torch.bfloat16))]), logits=tensor([[ 0.2598,  2.2188,  1.9062, -0.6992, -3.2656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -3.1875, -1.7266,  3.2188,  4.3750]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -3.1875, -1.7266,  3.2188,  4.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -3.0938, -2.3906,  2.4844,  5.0938]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -3.0938, -2.3906,  2.4844,  5.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.1719,  0.0981,  0.9102,  0.9727, -0.5078]], dtype=torch.bfloat16))]), logits=tensor([[-2.1719,  0.0981,  0.9102,  0.9727, -0.5078]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -3.0781, -1.8672,  2.9219,  4.3750]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -3.0781, -1.8672,  2.9219,  4.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7812, -0.1187,  1.8047,  1.6875, -1.1641]], dtype=torch.bfloat16))]), logits=tensor([[-2.7812, -0.1187,  1.8047,  1.6875, -1.1641]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -3.0625, -1.3672,  3.3906,  3.7188]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -3.0625, -1.3672,  3.3906,  3.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9844, -3.0156, -2.3906,  2.3750,  5.0938]], dtype=torch.bfloat16))]), logits=tensor([[-2.9844, -3.0156, -2.3906,  2.3750,  5.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 4.0312,  1.8281, -0.2793, -2.1406, -2.1875]], dtype=torch.bfloat16))]), logits=tensor([[ 4.0312,  1.8281, -0.2793, -2.1406, -2.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9688, -2.9844, -2.4375,  2.3281,  5.0625]], dtype=torch.bfloat16))]), logits=tensor([[-2.9688, -2.9844, -2.4375,  2.3281,  5.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -2.1562, -0.0679,  3.1562,  1.9531]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -2.1562, -0.0679,  3.1562,  1.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7188,  2.1406, -0.0312, -2.2031, -2.4688]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7188,  2.1406, -0.0312, -2.2031, -2.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5156, -2.7969, -1.5547,  2.7500,  3.8750]], dtype=torch.bfloat16))]), logits=tensor([[-3.5156, -2.7969, -1.5547,  2.7500,  3.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2188, -2.9688, -2.1719,  2.5156,  4.7188]], dtype=torch.bfloat16))]), logits=tensor([[-3.2188, -2.9688, -2.1719,  2.5156,  4.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7812, -2.5312, -0.5273,  3.0312,  2.7656]], dtype=torch.bfloat16))]), logits=tensor([[-3.7812, -2.5312, -0.5273,  3.0312,  2.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4062,  0.3223,  1.8047,  1.2656, -1.4688]], dtype=torch.bfloat16))]), logits=tensor([[-2.4062,  0.3223,  1.8047,  1.2656, -1.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.4004,  1.9766,  1.4453, -0.6953, -2.7031]], dtype=torch.bfloat16))]), logits=tensor([[ 0.4004,  1.9766,  1.4453, -0.6953, -2.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -3.0781, -1.8359,  2.8750,  4.3750]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -3.0781, -1.8359,  2.8750,  4.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -3.0312, -1.4844,  2.9062,  4.1250]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -3.0312, -1.4844,  2.9062,  4.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -2.9375, -1.2578,  3.1406,  3.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -2.9375, -1.2578,  3.1406,  3.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8438, -2.7656, -0.8203,  3.1094,  3.2031]], dtype=torch.bfloat16))]), logits=tensor([[-3.8438, -2.7656, -0.8203,  3.1094,  3.2031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.9844, -2.4531, -0.2461,  3.3438,  2.4688]], dtype=torch.bfloat16))]), logits=tensor([[-3.9844, -2.4531, -0.2461,  3.3438,  2.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7188, -2.9688, -2.4688,  2.1406,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7188, -2.9688, -2.4688,  2.1406,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -3.1250, -1.3984,  3.2969,  3.8906]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -3.1250, -1.3984,  3.2969,  3.8906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6406, -1.9297,  0.2109,  2.9219,  1.5703]], dtype=torch.bfloat16))]), logits=tensor([[-3.6406, -1.9297,  0.2109,  2.9219,  1.5703]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2031, -1.6328, -0.2930,  2.1875,  1.9531]], dtype=torch.bfloat16))]), logits=tensor([[-3.2031, -1.6328, -0.2930,  2.1875,  1.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -1.7109,  0.1279,  2.2031,  1.7656]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -1.7109,  0.1279,  2.2031,  1.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7344, -2.9375, -2.4375,  2.0938,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7344, -2.9375, -2.4375,  2.0938,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.2969,  1.5547,  2.1719,  0.1611, -2.6719]], dtype=torch.bfloat16))]), logits=tensor([[-1.2969,  1.5547,  2.1719,  0.1611, -2.6719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2656, -2.8906, -1.7578,  2.5312,  4.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.2656, -2.8906, -1.7578,  2.5312,  4.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2031, -3.0469, -2.0312,  2.5938,  4.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.2031, -3.0469, -2.0312,  2.5938,  4.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9062, -3.0156, -2.3906,  2.2812,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.9062, -3.0156, -2.3906,  2.2812,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.1562, -0.2471,  3.0000,  2.2344]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.1562, -0.2471,  3.0000,  2.2344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3281,  0.3828,  1.6484,  1.0938, -1.3203]], dtype=torch.bfloat16))]), logits=tensor([[-2.3281,  0.3828,  1.6484,  1.0938, -1.3203]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2812, -2.0938, -1.1797,  2.1094,  3.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.2812, -2.0938, -1.1797,  2.1094,  3.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -3.0938, -1.3984,  3.2188,  3.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -3.0938, -1.3984,  3.2188,  3.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6250, -3.0312, -1.6562,  3.0156,  4.1562]], dtype=torch.bfloat16))]), logits=tensor([[-3.6250, -3.0312, -1.6562,  3.0156,  4.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7500, -2.1875, -1.6094,  1.6953,  3.6094]], dtype=torch.bfloat16))]), logits=tensor([[-2.7500, -2.1875, -1.6094,  1.6953,  3.6094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5156, -2.9062, -1.5547,  2.9688,  3.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.5156, -2.9062, -1.5547,  2.9688,  3.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.0938, -1.8750, -1.4375,  1.1641,  3.2344]], dtype=torch.bfloat16))]), logits=tensor([[-2.0938, -1.8750, -1.4375,  1.1641,  3.2344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2969, -3.1094, -2.1875,  2.7031,  4.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.2969, -3.1094, -2.1875,  2.7031,  4.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -3.0000, -2.3438,  2.4062,  5.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -3.0000, -2.3438,  2.4062,  5.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.3438,  2.2500,  0.6875, -1.7500, -2.7812]], dtype=torch.bfloat16))]), logits=tensor([[ 2.3438,  2.2500,  0.6875, -1.7500, -2.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1562, -2.8594, -1.9688,  2.3906,  4.4375]], dtype=torch.bfloat16))]), logits=tensor([[-3.1562, -2.8594, -1.9688,  2.3906,  4.4375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.3301,  2.3594,  2.0625, -0.8047, -3.5469]], dtype=torch.bfloat16))]), logits=tensor([[ 0.3301,  2.3594,  2.0625, -0.8047, -3.5469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3281, -1.2578,  0.8242,  2.4688,  0.6289]], dtype=torch.bfloat16))]), logits=tensor([[-3.3281, -1.2578,  0.8242,  2.4688,  0.6289]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6719, -1.9375,  0.3613,  3.1094,  1.4062]], dtype=torch.bfloat16))]), logits=tensor([[-3.6719, -1.9375,  0.3613,  3.1094,  1.4062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1562, -3.1094, -2.3125,  2.5625,  5.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.1562, -3.1094, -2.3125,  2.5625,  5.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0156, -1.9453, -1.0156,  1.8594,  2.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.0156, -1.9453, -1.0156,  1.8594,  2.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6406, -0.0442,  1.5469,  1.5234, -1.0234]], dtype=torch.bfloat16))]), logits=tensor([[-2.6406, -0.0442,  1.5469,  1.5234, -1.0234]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -1.7891,  0.5312,  2.8750,  1.1172]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -1.7891,  0.5312,  2.8750,  1.1172]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6875, -0.3027,  1.0547,  1.4141, -0.2793]], dtype=torch.bfloat16))]), logits=tensor([[-2.6875, -0.3027,  1.0547,  1.4141, -0.2793]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -3.0000, -1.6406,  3.0312,  4.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -3.0000, -1.6406,  3.0312,  4.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6250, -2.8750, -2.4062,  1.9531,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.6250, -2.8750, -2.4062,  1.9531,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.6562,  2.5000,  1.4062, -1.5781, -3.2969]], dtype=torch.bfloat16))]), logits=tensor([[ 1.6562,  2.5000,  1.4062, -1.5781, -3.2969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.6094,  1.3594,  2.3438,  0.4180, -2.7188]], dtype=torch.bfloat16))]), logits=tensor([[-1.6094,  1.3594,  2.3438,  0.4180, -2.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.9219,  2.4688,  1.2188, -1.6797, -3.1719]], dtype=torch.bfloat16))]), logits=tensor([[ 1.9219,  2.4688,  1.2188, -1.6797, -3.1719]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.1562, -0.0422,  0.4043,  0.7422,  0.1465]], dtype=torch.bfloat16))]), logits=tensor([[-2.1562, -0.0422,  0.4043,  0.7422,  0.1465]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.0625,  2.2188,  0.2471, -2.0156, -2.5938]], dtype=torch.bfloat16))]), logits=tensor([[ 3.0625,  2.2188,  0.2471, -2.0156, -2.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 4.0625,  1.3438, -0.4707, -1.9766, -1.7891]], dtype=torch.bfloat16))]), logits=tensor([[ 4.0625,  1.3438, -0.4707, -1.9766, -1.7891]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.3750,  2.3281,  0.7539, -1.7812, -2.8594]], dtype=torch.bfloat16))]), logits=tensor([[ 2.3750,  2.3281,  0.7539, -1.7812, -2.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2656, -1.3906,  0.2490,  2.1094,  1.2734]], dtype=torch.bfloat16))]), logits=tensor([[-3.2656, -1.3906,  0.2490,  2.1094,  1.2734]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7969, -2.8750, -2.3594,  2.0781,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.7969, -2.8750, -2.3594,  2.0781,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2031, -3.0469, -2.1875,  2.5000,  4.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.2031, -3.0469, -2.1875,  2.5000,  4.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -3.1875, -2.0312,  2.9219,  4.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -3.1875, -2.0312,  2.9219,  4.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -3.0625, -2.1875,  2.5156,  4.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -3.0625, -2.1875,  2.5156,  4.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8125, -2.6875, -0.5820,  3.2969,  2.9219]], dtype=torch.bfloat16))]), logits=tensor([[-3.8125, -2.6875, -0.5820,  3.2969,  2.9219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5625, -2.4688, -1.0000,  2.8281,  3.1094]], dtype=torch.bfloat16))]), logits=tensor([[-3.5625, -2.4688, -1.0000,  2.8281,  3.1094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0781, -3.0156, -2.3125,  2.4531,  5.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.0781, -3.0156, -2.3125,  2.4531,  5.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7188, -2.6406, -0.6953,  3.2969,  2.7969]], dtype=torch.bfloat16))]), logits=tensor([[-3.7188, -2.6406, -0.6953,  3.2969,  2.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8281, -3.0000, -2.4688,  2.2188,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.8281, -3.0000, -2.4688,  2.2188,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3594, -2.8438, -1.7188,  2.6562,  4.1875]], dtype=torch.bfloat16))]), logits=tensor([[-3.3594, -2.8438, -1.7188,  2.6562,  4.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8281, -3.0000, -2.4531,  2.2656,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.8281, -3.0000, -2.4531,  2.2656,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -3.1406, -1.8516,  2.8906,  4.4688]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -3.1406, -1.8516,  2.8906,  4.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.9922, -0.3887,  0.0208,  0.8828,  0.6016]], dtype=torch.bfloat16))]), logits=tensor([[-1.9922, -0.3887,  0.0208,  0.8828,  0.6016]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7344, -2.8906, -2.4062,  2.0156,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.7344, -2.8906, -2.4062,  2.0156,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.7969,  2.1875,  0.8750, -1.4766, -2.7656]], dtype=torch.bfloat16))]), logits=tensor([[ 1.7969,  2.1875,  0.8750, -1.4766, -2.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.1387,  1.6172,  1.0312, -0.5273, -2.1250]], dtype=torch.bfloat16))]), logits=tensor([[ 0.1387,  1.6172,  1.0312, -0.5273, -2.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5625, -0.1582,  1.1875,  1.3594, -0.5078]], dtype=torch.bfloat16))]), logits=tensor([[-2.5625, -0.1582,  1.1875,  1.3594, -0.5078]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2656, -2.4844, -1.2031,  2.3281,  3.4375]], dtype=torch.bfloat16))]), logits=tensor([[-3.2656, -2.4844, -1.2031,  2.3281,  3.4375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.3906,  1.9922, -0.1299, -2.0469, -2.2656]], dtype=torch.bfloat16))]), logits=tensor([[ 3.3906,  1.9922, -0.1299, -2.0469, -2.2656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8125, -2.9062, -2.3750,  2.1250,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.8125, -2.9062, -2.3750,  2.1250,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.6094,  1.8438,  2.0625, -0.2100, -2.9219]], dtype=torch.bfloat16))]), logits=tensor([[-0.6094,  1.8438,  2.0625, -0.2100, -2.9219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3125,  0.6250,  2.0469,  0.9375, -1.8047]], dtype=torch.bfloat16))]), logits=tensor([[-2.3125,  0.6250,  2.0469,  0.9375, -1.8047]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0781, -3.1250, -2.3125,  2.6094,  5.0312]], dtype=torch.bfloat16))]), logits=tensor([[-3.0781, -3.1250, -2.3125,  2.6094,  5.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.5938,  2.4375,  0.6797, -1.9766, -2.9062]], dtype=torch.bfloat16))]), logits=tensor([[ 2.5938,  2.4375,  0.6797, -1.9766, -2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.2031,  0.6211,  2.2969,  1.0312, -2.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.2031,  0.6211,  2.2969,  1.0312, -2.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6250, -2.2656,  0.1021,  3.2344,  1.8359]], dtype=torch.bfloat16))]), logits=tensor([[-3.6250, -2.2656,  0.1021,  3.2344,  1.8359]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7188, -3.0000, -1.2266,  3.2188,  3.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.7188, -3.0000, -1.2266,  3.2188,  3.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -3.0469, -1.6719,  2.9062,  4.3125]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -3.0469, -1.6719,  2.9062,  4.3125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7656,  2.2500, -0.0100, -2.2812, -2.6094]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7656,  2.2500, -0.0100, -2.2812, -2.6094]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6250, -2.9219, -1.3906,  3.0156,  3.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.6250, -2.9219, -1.3906,  3.0156,  3.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5156, -2.0469, -0.4062,  2.4844,  2.3906]], dtype=torch.bfloat16))]), logits=tensor([[-3.5156, -2.0469, -0.4062,  2.4844,  2.3906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3906, -3.0625, -2.0469,  2.7656,  4.5938]], dtype=torch.bfloat16))]), logits=tensor([[-3.3906, -3.0625, -2.0469,  2.7656,  4.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.8906,  2.4219,  0.5430, -2.0469, -2.9062]], dtype=torch.bfloat16))]), logits=tensor([[ 2.8906,  2.4219,  0.5430, -2.0469, -2.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6719, -2.9844, -1.0469,  3.4062,  3.3594]], dtype=torch.bfloat16))]), logits=tensor([[-3.6719, -2.9844, -1.0469,  3.4062,  3.3594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7031,  0.9688, -0.5508, -1.6875, -1.4219]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7031,  0.9688, -0.5508, -1.6875, -1.4219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8750, -2.7969, -0.7383,  3.3594,  3.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.8750, -2.7969, -0.7383,  3.3594,  3.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.5469,  2.1250, -0.0664, -2.1562, -2.4375]], dtype=torch.bfloat16))]), logits=tensor([[ 3.5469,  2.1250, -0.0664, -2.1562, -2.4375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9375, -3.0469, -2.4219,  2.3906,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.9375, -3.0469, -2.4219,  2.3906,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4219,  0.2695,  1.8594,  1.3516, -1.5547]], dtype=torch.bfloat16))]), logits=tensor([[-2.4219,  0.2695,  1.8594,  1.3516, -1.5547]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5938, -3.0469, -1.7422,  3.0469,  4.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.5938, -3.0469, -1.7422,  3.0469,  4.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -2.9219, -1.1797,  3.1875,  3.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -2.9219, -1.1797,  3.1875,  3.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3906, -3.0625, -1.9609,  2.6875,  4.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.3906, -3.0625, -1.9609,  2.6875,  4.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5469, -3.1250, -1.7734,  3.0781,  4.2812]], dtype=torch.bfloat16))]), logits=tensor([[-3.5469, -3.1250, -1.7734,  3.0781,  4.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4531, -3.0781, -1.8203,  2.8125,  4.4062]], dtype=torch.bfloat16))]), logits=tensor([[-3.4531, -3.0781, -1.8203,  2.8125,  4.4062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -1.1719,  0.6367,  2.1719,  0.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -1.1719,  0.6367,  2.1719,  0.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -3.1875, -2.0000,  2.9219,  4.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -3.1875, -2.0000,  2.9219,  4.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5156, -2.8750, -1.5312,  2.6094,  4.0938]], dtype=torch.bfloat16))]), logits=tensor([[-3.5156, -2.8750, -1.5312,  2.6094,  4.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.5938, -0.3145,  3.2656,  2.5625]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.5938, -0.3145,  3.2656,  2.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -2.4688, -1.3359,  2.2812,  3.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -2.4688, -1.3359,  2.2812,  3.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.4609,  2.2031,  1.0625, -1.3203, -2.7969]], dtype=torch.bfloat16))]), logits=tensor([[ 1.4609,  2.2031,  1.0625, -1.3203, -2.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -0.4062,  1.6797,  1.8828, -0.7383]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -0.4062,  1.6797,  1.8828, -0.7383]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.9922,  0.4531,  1.3516,  0.8203, -1.1875]], dtype=torch.bfloat16))]), logits=tensor([[-1.9922,  0.4531,  1.3516,  0.8203, -1.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.4531,  2.3594,  0.2129, -2.2031, -2.7656]], dtype=torch.bfloat16))]), logits=tensor([[ 3.4531,  2.3594,  0.2129, -2.2031, -2.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6250, -2.1406, -0.3535,  2.4531,  2.4531]], dtype=torch.bfloat16))]), logits=tensor([[-3.6250, -2.1406, -0.3535,  2.4531,  2.4531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1719, -0.6875,  1.4375,  1.9688, -0.2197]], dtype=torch.bfloat16))]), logits=tensor([[-3.1719, -0.6875,  1.4375,  1.9688, -0.2197]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7656, -2.9375, -2.4375,  2.0781,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.7656, -2.9375, -2.4375,  2.0781,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -2.3438, -0.7422,  2.7500,  2.7656]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -2.3438, -0.7422,  2.7500,  2.7656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7812, -2.8750, -2.4062,  2.0781,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.7812, -2.8750, -2.4062,  2.0781,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8750, -2.9844, -2.3906,  2.2500,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.8750, -2.9844, -2.3906,  2.2500,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7188, -2.6875, -1.0469,  3.0000,  3.2812]], dtype=torch.bfloat16))]), logits=tensor([[-3.7188, -2.6875, -1.0469,  3.0000,  3.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -0.4590,  1.6719,  1.9688, -0.7227]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -0.4590,  1.6719,  1.9688, -0.7227]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1094, -2.7500, -2.0938,  2.2969,  4.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.1094, -2.7500, -2.0938,  2.2969,  4.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2656, -3.0781, -2.0469,  2.6094,  4.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.2656, -3.0781, -2.0469,  2.6094,  4.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4688, -2.7969, -2.5156,  1.8203,  5.2500]], dtype=torch.bfloat16))]), logits=tensor([[-2.4688, -2.7969, -2.5156,  1.8203,  5.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -0.8633,  1.3594,  2.0156, -0.1113]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -0.8633,  1.3594,  2.0156, -0.1113]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.6875,  2.1094,  0.2891, -1.8594, -2.4844]], dtype=torch.bfloat16))]), logits=tensor([[ 2.6875,  2.1094,  0.2891, -1.8594, -2.4844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -2.3281, -0.3535,  2.8125,  2.4531]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -2.3281, -0.3535,  2.8125,  2.4531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4375,  0.2500,  1.5625,  1.2656, -1.1484]], dtype=torch.bfloat16))]), logits=tensor([[-2.4375,  0.2500,  1.5625,  1.2656, -1.1484]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.5469, -0.3652,  3.2500,  2.5312]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.5469, -0.3652,  3.2500,  2.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.3594,  1.4375,  2.0469,  0.2773, -2.5156]], dtype=torch.bfloat16))]), logits=tensor([[-1.3594,  1.4375,  2.0469,  0.2773, -2.5156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1250, -2.8594, -2.1250,  2.2656,  4.7812]], dtype=torch.bfloat16))]), logits=tensor([[-3.1250, -2.8594, -2.1250,  2.2656,  4.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0938, -0.8594,  1.1250,  2.1562,  0.0145]], dtype=torch.bfloat16))]), logits=tensor([[-3.0938, -0.8594,  1.1250,  2.1562,  0.0145]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2188, -3.1875, -2.2656,  2.7188,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.2188, -3.1875, -2.2656,  2.7188,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3125, -3.0469, -1.9844,  2.7656,  4.5312]], dtype=torch.bfloat16))]), logits=tensor([[-3.3125, -3.0469, -1.9844,  2.7656,  4.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.9531,  1.2344, -0.4883, -1.8906, -1.6797]], dtype=torch.bfloat16))]), logits=tensor([[ 3.9531,  1.2344, -0.4883, -1.8906, -1.6797]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2031, -3.0938, -2.1719,  2.5781,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.2031, -3.0938, -2.1719,  2.5781,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7188, -2.9375, -2.5000,  2.1094,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7188, -2.9375, -2.5000,  2.1094,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5469, -2.7969, -1.3828,  2.8438,  3.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.5469, -2.7969, -1.3828,  2.8438,  3.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9844, -3.0469, -2.3125,  2.3281,  5.0938]], dtype=torch.bfloat16))]), logits=tensor([[-2.9844, -3.0469, -2.3125,  2.3281,  5.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.2002,  2.1250,  1.7734, -0.6484, -3.0469]], dtype=torch.bfloat16))]), logits=tensor([[ 0.2002,  2.1250,  1.7734, -0.6484, -3.0469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.9375, -2.7344, -0.6797,  3.3906,  3.0000]], dtype=torch.bfloat16))]), logits=tensor([[-3.9375, -2.7344, -0.6797,  3.3906,  3.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -1.3203,  0.8867,  2.5781,  0.5352]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -1.3203,  0.8867,  2.5781,  0.5352]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5625, -2.9375, -1.4531,  3.0000,  3.7812]], dtype=torch.bfloat16))]), logits=tensor([[-3.5625, -2.9375, -1.4531,  3.0000,  3.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6250, -2.8438, -1.4688,  2.8750,  3.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.6250, -2.8438, -1.4688,  2.8750,  3.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5781, -1.0312, -0.4277,  1.3047,  1.6562]], dtype=torch.bfloat16))]), logits=tensor([[-2.5781, -1.0312, -0.4277,  1.3047,  1.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6406, -1.6875, -0.7695,  1.5078,  2.5000]], dtype=torch.bfloat16))]), logits=tensor([[-2.6406, -1.6875, -0.7695,  1.5078,  2.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -3.1094, -1.6016,  3.0625,  4.1562]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -3.1094, -1.6016,  3.0625,  4.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6875, -3.1406, -1.6562,  3.2188,  4.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.6875, -3.1406, -1.6562,  3.2188,  4.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5781, -3.1719, -1.8984,  3.0000,  4.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.5781, -3.1719, -1.8984,  3.0000,  4.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3906, -1.2031,  1.3672,  2.6406,  0.0189]], dtype=torch.bfloat16))]), logits=tensor([[-3.3906, -1.2031,  1.3672,  2.6406,  0.0189]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7969,  2.2188, -0.0679, -2.2812, -2.5625]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7969,  2.2188, -0.0679, -2.2812, -2.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5625, -2.9062, -2.5000,  1.9688,  5.2500]], dtype=torch.bfloat16))]), logits=tensor([[-2.5625, -2.9062, -2.5000,  1.9688,  5.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.2031,  2.2812,  0.8320, -1.6953, -2.8594]], dtype=torch.bfloat16))]), logits=tensor([[ 2.2031,  2.2812,  0.8320, -1.6953, -2.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -3.1562, -1.9375,  2.8594,  4.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -3.1562, -1.9375,  2.8594,  4.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.2188,  0.6406,  2.2500,  0.8945, -1.9844]], dtype=torch.bfloat16))]), logits=tensor([[-2.2188,  0.6406,  2.2500,  0.8945, -1.9844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0469, -3.0938, -2.2656,  2.4219,  5.0312]], dtype=torch.bfloat16))]), logits=tensor([[-3.0469, -3.0938, -2.2656,  2.4219,  5.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -3.0781, -2.1562,  2.7188,  4.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -3.0781, -2.1562,  2.7188,  4.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8125, -2.7969, -1.0859,  3.1719,  3.4375]], dtype=torch.bfloat16))]), logits=tensor([[-3.8125, -2.7969, -1.0859,  3.1719,  3.4375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.2344,  0.1289,  3.2188,  1.8750]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.2344,  0.1289,  3.2188,  1.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8438, -2.6875, -0.5273,  3.3750,  2.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.8438, -2.6875, -0.5273,  3.3750,  2.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5625, -3.1875, -1.8203,  3.1094,  4.4062]], dtype=torch.bfloat16))]), logits=tensor([[-3.5625, -3.1875, -1.8203,  3.1094,  4.4062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.7969, -1.1797,  3.0625,  3.5156]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.7969, -1.1797,  3.0625,  3.5156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -2.2031, -0.5781,  2.3750,  2.7031]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -2.2031, -0.5781,  2.3750,  2.7031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5312,  0.0938,  1.9844,  1.4531, -1.5234]], dtype=torch.bfloat16))]), logits=tensor([[-2.5312,  0.0938,  1.9844,  1.4531, -1.5234]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1250, -3.0312, -2.2656,  2.4375,  5.0312]], dtype=torch.bfloat16))]), logits=tensor([[-3.1250, -3.0312, -2.2656,  2.4375,  5.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3125, -0.8320,  1.7188,  2.2969, -0.4453]], dtype=torch.bfloat16))]), logits=tensor([[-3.3125, -0.8320,  1.7188,  2.2969, -0.4453]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.2969,  0.6328,  2.1719,  0.9727, -1.9609]], dtype=torch.bfloat16))]), logits=tensor([[-2.2969,  0.6328,  2.1719,  0.9727, -1.9609]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.9219,  2.1562, -0.0708, -2.2969, -2.5156]], dtype=torch.bfloat16))]), logits=tensor([[ 3.9219,  2.1562, -0.0708, -2.2969, -2.5156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.1504,  1.1094,  0.1436, -0.5742, -0.9766]], dtype=torch.bfloat16))]), logits=tensor([[ 0.1504,  1.1094,  0.1436, -0.5742, -0.9766]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.4922,  2.3750,  1.4453, -1.3594, -3.2812]], dtype=torch.bfloat16))]), logits=tensor([[ 1.4922,  2.3750,  1.4453, -1.3594, -3.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3125, -2.6406, -1.5938,  2.5156,  3.8125]], dtype=torch.bfloat16))]), logits=tensor([[-3.3125, -2.6406, -1.5938,  2.5156,  3.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4844, -2.6719, -1.2031,  2.7812,  3.4062]], dtype=torch.bfloat16))]), logits=tensor([[-3.4844, -2.6719, -1.2031,  2.7812,  3.4062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.3750,  0.0947,  3.3750,  1.9141]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.3750,  0.0947,  3.3750,  1.9141]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -3.1250, -2.3438,  2.6094,  5.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -3.1250, -2.3438,  2.6094,  5.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -1.4062,  1.0078,  2.8125,  0.5078]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -1.4062,  1.0078,  2.8125,  0.5078]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7969, -2.9062, -1.1641,  3.1875,  3.6250]], dtype=torch.bfloat16))]), logits=tensor([[-3.7969, -2.9062, -1.1641,  3.1875,  3.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7656, -2.9375, -2.4219,  2.1250,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.7656, -2.9375, -2.4219,  2.1250,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6406, -1.7266,  0.7734,  3.0469,  0.9453]], dtype=torch.bfloat16))]), logits=tensor([[-3.6406, -1.7266,  0.7734,  3.0469,  0.9453]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.0625,  2.4375,  1.1094, -1.6953, -3.1250]], dtype=torch.bfloat16))]), logits=tensor([[ 2.0625,  2.4375,  1.1094, -1.6953, -3.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -3.0000, -1.4062,  3.1562,  3.8594]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -3.0000, -1.4062,  3.1562,  3.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2969, -1.1172,  1.0156,  2.3750,  0.2715]], dtype=torch.bfloat16))]), logits=tensor([[-3.2969, -1.1172,  1.0156,  2.3750,  0.2715]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.0000,  0.8438,  2.1094,  0.8359, -2.2031]], dtype=torch.bfloat16))]), logits=tensor([[-2.0000,  0.8438,  2.1094,  0.8359, -2.2031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -3.1562, -2.0469,  2.8438,  4.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -3.1562, -2.0469,  2.8438,  4.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.0361,  1.7734,  1.4062, -0.5156, -2.3750]], dtype=torch.bfloat16))]), logits=tensor([[-0.0361,  1.7734,  1.4062, -0.5156, -2.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5469, -3.1094, -1.4453,  3.2188,  4.0000]], dtype=torch.bfloat16))]), logits=tensor([[-3.5469, -3.1094, -1.4453,  3.2188,  4.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.3750,  2.1406,  0.0649, -2.0781, -2.4688]], dtype=torch.bfloat16))]), logits=tensor([[ 3.3750,  2.1406,  0.0649, -2.0781, -2.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.5859,  2.0312,  0.7891, -1.3594, -2.5469]], dtype=torch.bfloat16))]), logits=tensor([[ 1.5859,  2.0312,  0.7891, -1.3594, -2.5469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -3.1562, -2.2344,  2.7188,  5.0000]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -3.1562, -2.2344,  2.7188,  5.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.2500,  2.4219,  0.3906, -2.1719, -2.8594]], dtype=torch.bfloat16))]), logits=tensor([[ 3.2500,  2.4219,  0.3906, -2.1719, -2.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2031, -3.1562, -2.2500,  2.6875,  5.0312]], dtype=torch.bfloat16))]), logits=tensor([[-3.2031, -3.1562, -2.2500,  2.6875,  5.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2344, -1.9141, -0.6328,  2.0312,  2.5781]], dtype=torch.bfloat16))]), logits=tensor([[-3.2344, -1.9141, -0.6328,  2.0312,  2.5781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1094, -0.7422,  1.2422,  2.0781, -0.1377]], dtype=torch.bfloat16))]), logits=tensor([[-3.1094, -0.7422,  1.2422,  2.0781, -0.1377]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -3.1562, -2.0000,  3.0000,  4.5625]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -3.1562, -2.0000,  3.0000,  4.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3281, -3.0625, -1.9375,  2.8906,  4.5312]], dtype=torch.bfloat16))]), logits=tensor([[-3.3281, -3.0625, -1.9375,  2.8906,  4.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4062, -2.9375, -1.8906,  2.6719,  4.3750]], dtype=torch.bfloat16))]), logits=tensor([[-3.4062, -2.9375, -1.8906,  2.6719,  4.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.6406,  2.4062,  1.3438, -1.5078, -3.2188]], dtype=torch.bfloat16))]), logits=tensor([[ 1.6406,  2.4062,  1.3438, -1.5078, -3.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2656, -3.1094, -2.2188,  2.6406,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.2656, -3.1094, -2.2188,  2.6406,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -0.7344,  0.8867,  1.7812,  0.3066]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -0.7344,  0.8867,  1.7812,  0.3066]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -3.0625, -1.9141,  2.7031,  4.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -3.0625, -1.9141,  2.7031,  4.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -3.0312, -1.8125,  2.8906,  4.3125]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -3.0312, -1.8125,  2.8906,  4.3125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7031, -2.4219,  0.1006,  3.4062,  2.0156]], dtype=torch.bfloat16))]), logits=tensor([[-3.7031, -2.4219,  0.1006,  3.4062,  2.0156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -3.1406, -2.1406,  2.8125,  4.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -3.1406, -2.1406,  2.8125,  4.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -2.7969, -1.6250,  2.6406,  4.0000]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -2.7969, -1.6250,  2.6406,  4.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0469, -3.0469, -2.3594,  2.4062,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-3.0469, -3.0469, -2.3594,  2.4062,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7812, -2.8438, -2.3750,  2.0938,  5.0000]], dtype=torch.bfloat16))]), logits=tensor([[-2.7812, -2.8438, -2.3750,  2.0938,  5.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -2.9375, -2.1719,  2.2656,  4.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -2.9375, -2.1719,  2.2656,  4.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7500, -2.8906, -2.4375,  2.0625,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.7500, -2.8906, -2.4375,  2.0625,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -3.0469, -1.7969,  3.0312,  4.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -3.0469, -1.7969,  3.0312,  4.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 4.1250,  1.8281, -0.2832, -2.1875, -2.2656]], dtype=torch.bfloat16))]), logits=tensor([[ 4.1250,  1.8281, -0.2832, -2.1875, -2.2656]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8750, -3.0156, -2.4219,  2.2656,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.8750, -3.0156, -2.4219,  2.2656,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.6250,  2.3594,  0.6406, -1.8828, -2.8906]], dtype=torch.bfloat16))]), logits=tensor([[ 2.6250,  2.3594,  0.6406, -1.8828, -2.8906]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7188, -2.9062, -0.6680,  3.5938,  2.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.7188, -2.9062, -0.6680,  3.5938,  2.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8281, -2.7812, -0.8594,  3.2500,  3.2031]], dtype=torch.bfloat16))]), logits=tensor([[-3.8281, -2.7812, -0.8594,  3.2500,  3.2031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6250, -0.6016,  0.9023,  1.5625, -0.0069]], dtype=torch.bfloat16))]), logits=tensor([[-2.6250, -0.6016,  0.9023,  1.5625, -0.0069]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4062,  0.5078,  2.1719,  1.1250, -1.8828]], dtype=torch.bfloat16))]), logits=tensor([[-2.4062,  0.5078,  2.1719,  1.1250, -1.8828]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4688,  0.2158,  1.7969,  1.3047, -1.4141]], dtype=torch.bfloat16))]), logits=tensor([[-2.4688,  0.2158,  1.7969,  1.3047, -1.4141]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7656, -2.9219, -2.4062,  2.0938,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7656, -2.9219, -2.4062,  2.0938,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7031, -2.1719,  0.1348,  3.1406,  1.8203]], dtype=torch.bfloat16))]), logits=tensor([[-3.7031, -2.1719,  0.1348,  3.1406,  1.8203]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7656, -2.7031, -0.7344,  3.0938,  3.0312]], dtype=torch.bfloat16))]), logits=tensor([[-3.7656, -2.7031, -0.7344,  3.0938,  3.0312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7500,  1.9375, -0.1963, -2.1250, -2.2188]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7500,  1.9375, -0.1963, -2.1250, -2.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8594, -2.9531, -2.4531,  2.2344,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.8594, -2.9531, -2.4531,  2.2344,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0156, -2.7344, -1.9375,  2.2344,  4.3125]], dtype=torch.bfloat16))]), logits=tensor([[-3.0156, -2.7344, -1.9375,  2.2344,  4.3125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5469, -1.7266,  0.9453,  3.0156,  0.8164]], dtype=torch.bfloat16))]), logits=tensor([[-3.5469, -1.7266,  0.9453,  3.0156,  0.8164]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4531, -3.1719, -1.9219,  3.0625,  4.4375]], dtype=torch.bfloat16))]), logits=tensor([[-3.4531, -3.1719, -1.9219,  3.0625,  4.4375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2969, -1.2031,  0.6992,  2.1406,  0.7969]], dtype=torch.bfloat16))]), logits=tensor([[-3.2969, -1.2031,  0.6992,  2.1406,  0.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4219, -2.8750, -1.5703,  2.8438,  3.9375]], dtype=torch.bfloat16))]), logits=tensor([[-3.4219, -2.8750, -1.5703,  2.8438,  3.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9688, -0.6836,  1.4844,  2.0312, -0.4004]], dtype=torch.bfloat16))]), logits=tensor([[-2.9688, -0.6836,  1.4844,  2.0312, -0.4004]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7812, -2.7656, -0.6133,  3.4219,  2.7969]], dtype=torch.bfloat16))]), logits=tensor([[-3.7812, -2.7656, -0.6133,  3.4219,  2.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1094, -3.0781, -2.2812,  2.5312,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.1094, -3.0781, -2.2812,  2.5312,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3594, -3.0625, -2.0938,  2.6875,  4.7188]], dtype=torch.bfloat16))]), logits=tensor([[-3.3594, -3.0625, -2.0938,  2.6875,  4.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1562, -3.0625, -2.2188,  2.6094,  4.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.1562, -3.0625, -2.2188,  2.6094,  4.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5312, -1.5312,  0.9023,  2.7812,  0.6641]], dtype=torch.bfloat16))]), logits=tensor([[-3.5312, -1.5312,  0.9023,  2.7812,  0.6641]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4219,  0.3555,  2.1094,  1.1953, -1.7344]], dtype=torch.bfloat16))]), logits=tensor([[-2.4219,  0.3555,  2.1094,  1.1953, -1.7344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4531, -2.1875, -0.7852,  2.7031,  2.6875]], dtype=torch.bfloat16))]), logits=tensor([[-3.4531, -2.1875, -0.7852,  2.7031,  2.6875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0938, -0.5312,  1.3594,  2.0938, -0.4043]], dtype=torch.bfloat16))]), logits=tensor([[-3.0938, -0.5312,  1.3594,  2.0938, -0.4043]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -3.0625, -1.4062,  3.1875,  3.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -3.0625, -1.4062,  3.1875,  3.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2031, -3.1250, -2.1875,  2.6875,  4.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.2031, -3.1250, -2.1875,  2.6875,  4.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1562, -3.1250, -2.2969,  2.6875,  4.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.1562, -3.1250, -2.2969,  2.6875,  4.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4375, -3.1875, -1.8750,  3.0312,  4.4375]], dtype=torch.bfloat16))]), logits=tensor([[-3.4375, -3.1875, -1.8750,  3.0312,  4.4375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 2.0469,  2.3906,  1.0469, -1.6406, -3.0469]], dtype=torch.bfloat16))]), logits=tensor([[ 2.0469,  2.3906,  1.0469, -1.6406, -3.0469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9688, -2.9844, -2.3125,  2.2812,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.9688, -2.9844, -2.3125,  2.2812,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -1.2109,  1.1797,  2.5938,  0.2891]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -1.2109,  1.1797,  2.5938,  0.2891]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4844,  0.2207,  1.5469,  1.1484, -1.0625]], dtype=torch.bfloat16))]), logits=tensor([[-2.4844,  0.2207,  1.5469,  1.1484, -1.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7031,  2.1406, -0.0579, -2.1875, -2.4844]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7031,  2.1406, -0.0579, -2.1875, -2.4844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7812, -2.1875,  0.3164,  3.1719,  1.7734]], dtype=torch.bfloat16))]), logits=tensor([[-3.7812, -2.1875,  0.3164,  3.1719,  1.7734]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3594, -1.7500,  0.1611,  2.5000,  1.5312]], dtype=torch.bfloat16))]), logits=tensor([[-3.3594, -1.7500,  0.1611,  2.5000,  1.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -2.4688, -0.8594,  2.9375,  2.9844]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -2.4688, -0.8594,  2.9375,  2.9844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2500, -2.0938, -1.0781,  1.9141,  3.2500]], dtype=torch.bfloat16))]), logits=tensor([[-3.2500, -2.0938, -1.0781,  1.9141,  3.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7656,  2.2031, -0.0352, -2.2500, -2.5312]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7656,  2.2031, -0.0352, -2.2500, -2.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2969, -1.3438,  0.1982,  2.0625,  1.3594]], dtype=torch.bfloat16))]), logits=tensor([[-3.2969, -1.3438,  0.1982,  2.0625,  1.3594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.5781, -2.8438, -2.4688,  1.9141,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.5781, -2.8438, -2.4688,  1.9141,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1406, -3.0938, -2.2500,  2.5312,  4.9688]], dtype=torch.bfloat16))]), logits=tensor([[-3.1406, -3.0938, -2.2500,  2.5312,  4.9688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1875, -1.6641, -0.5742,  2.1719,  2.2188]], dtype=torch.bfloat16))]), logits=tensor([[-3.1875, -1.6641, -0.5742,  2.1719,  2.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7188, -1.8125,  0.4023,  3.0312,  1.3750]], dtype=torch.bfloat16))]), logits=tensor([[-3.7188, -1.8125,  0.4023,  3.0312,  1.3750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.7422,  1.0547,  1.9297,  0.5234, -2.1562]], dtype=torch.bfloat16))]), logits=tensor([[-1.7422,  1.0547,  1.9297,  0.5234, -2.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2188, -1.4453, -0.1934,  1.9062,  1.8359]], dtype=torch.bfloat16))]), logits=tensor([[-3.2188, -1.4453, -0.1934,  1.9062,  1.8359]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.8320,  1.5703,  1.7734,  0.0057, -2.5312]], dtype=torch.bfloat16))]), logits=tensor([[-0.8320,  1.5703,  1.7734,  0.0057, -2.5312]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5469, -1.9141,  0.3906,  2.8281,  1.4141]], dtype=torch.bfloat16))]), logits=tensor([[-3.5469, -1.9141,  0.3906,  2.8281,  1.4141]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0312, -3.0469, -2.3125,  2.3281,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-3.0312, -3.0469, -2.3125,  2.3281,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0469, -3.0312, -2.3750,  2.3750,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-3.0469, -3.0312, -2.3750,  2.3750,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.7969,  2.1562, -0.0913, -2.2500, -2.5000]], dtype=torch.bfloat16))]), logits=tensor([[ 3.7969,  2.1562, -0.0913, -2.2500, -2.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8438, -3.0312, -2.4375,  2.3125,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.8438, -3.0312, -2.4375,  2.3125,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.9688, -1.2500,  3.1406,  3.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.9688, -1.2500,  3.1406,  3.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.0156,  0.9531,  2.3281,  0.6953, -2.3594]], dtype=torch.bfloat16))]), logits=tensor([[-2.0156,  0.9531,  2.3281,  0.6953, -2.3594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.1523,  1.7422,  1.2812, -0.5703, -2.4688]], dtype=torch.bfloat16))]), logits=tensor([[ 0.1523,  1.7422,  1.2812, -0.5703, -2.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.9375, -1.0859,  3.2969,  3.4219]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.9375, -1.0859,  3.2969,  3.4219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.1094, -3.1250, -2.3125,  2.5938,  5.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.1094, -3.1250, -2.3125,  2.5938,  5.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6875, -2.9219, -2.4688,  2.0781,  5.2500]], dtype=torch.bfloat16))]), logits=tensor([[-2.6875, -2.9219, -2.4688,  2.0781,  5.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -3.0156, -1.3906,  3.2344,  3.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -3.0156, -1.3906,  3.2344,  3.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.2188,  0.9922,  1.2188,  0.3594, -1.6250]], dtype=torch.bfloat16))]), logits=tensor([[-1.2188,  0.9922,  1.2188,  0.3594, -1.6250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9688, -3.0312, -2.3906,  2.4062,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.9688, -3.0312, -2.3906,  2.4062,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.3906, -2.7812, -2.4688,  1.7422,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.3906, -2.7812, -2.4688,  1.7422,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8750, -2.9688, -2.1562,  2.3281,  4.8125]], dtype=torch.bfloat16))]), logits=tensor([[-2.8750, -2.9688, -2.1562,  2.3281,  4.8125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7969, -2.9688, -2.4219,  2.1562,  5.2188]], dtype=torch.bfloat16))]), logits=tensor([[-2.7969, -2.9688, -2.4219,  2.1562,  5.2188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7500, -2.8906, -2.4219,  2.0156,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.7500, -2.8906, -2.4219,  2.0156,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7344, -1.8516,  0.3750,  2.8125,  1.5938]], dtype=torch.bfloat16))]), logits=tensor([[-3.7344, -1.8516,  0.3750,  2.8125,  1.5938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.3359,  1.2578,  1.6719,  0.2637, -2.0781]], dtype=torch.bfloat16))]), logits=tensor([[-1.3359,  1.2578,  1.6719,  0.2637, -2.0781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8594, -2.6250, -0.6133,  3.2188,  2.8594]], dtype=torch.bfloat16))]), logits=tensor([[-3.8594, -2.6250, -0.6133,  3.2188,  2.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7188, -3.0000, -1.2422,  3.0938,  3.7969]], dtype=torch.bfloat16))]), logits=tensor([[-3.7188, -3.0000, -1.2422,  3.0938,  3.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.2969,  0.4922,  2.3125,  0.9062, -1.9297]], dtype=torch.bfloat16))]), logits=tensor([[-2.2969,  0.4922,  2.3125,  0.9062, -1.9297]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.4688, -0.0508,  3.4219,  2.1250]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.4688, -0.0508,  3.4219,  2.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.7500, -2.4844, -0.1533,  3.4531,  2.2031]], dtype=torch.bfloat16))]), logits=tensor([[-3.7500, -2.4844, -0.1533,  3.4531,  2.2031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.8281, -2.8906, -0.7656,  3.4219,  3.2031]], dtype=torch.bfloat16))]), logits=tensor([[-3.8281, -2.8906, -0.7656,  3.4219,  3.2031]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5156, -1.7344, -0.1045,  2.5625,  1.7969]], dtype=torch.bfloat16))]), logits=tensor([[-3.5156, -1.7344, -0.1045,  2.5625,  1.7969]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.5781,  1.7422,  1.8438, -0.1289, -2.7188]], dtype=torch.bfloat16))]), logits=tensor([[-0.5781,  1.7422,  1.8438, -0.1289, -2.7188]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9844, -0.4414,  1.4062,  1.8984, -0.5391]], dtype=torch.bfloat16))]), logits=tensor([[-2.9844, -0.4414,  1.4062,  1.8984, -0.5391]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.0859,  2.2031,  1.3203, -1.1484, -3.0000]], dtype=torch.bfloat16))]), logits=tensor([[ 1.0859,  2.2031,  1.3203, -1.1484, -3.0000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.0938,  2.5312,  0.4883, -2.2031, -2.9531]], dtype=torch.bfloat16))]), logits=tensor([[ 3.0938,  2.5312,  0.4883, -2.2031, -2.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0625, -0.6016,  1.3984,  2.0000, -0.3770]], dtype=torch.bfloat16))]), logits=tensor([[-3.0625, -0.6016,  1.3984,  2.0000, -0.3770]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7969, -2.9531, -2.3750,  2.1562,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.7969, -2.9531, -2.3750,  2.1562,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.4375, -0.1147,  0.8438,  1.1797, -0.2295]], dtype=torch.bfloat16))]), logits=tensor([[-2.4375, -0.1147,  0.8438,  1.1797, -0.2295]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5938, -1.9844,  0.2969,  3.0312,  1.5000]], dtype=torch.bfloat16))]), logits=tensor([[-3.5938, -1.9844,  0.2969,  3.0312,  1.5000]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.2812,  2.5469,  0.4648, -2.2500, -2.9844]], dtype=torch.bfloat16))]), logits=tensor([[ 3.2812,  2.5469,  0.4648, -2.2500, -2.9844]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2812, -1.0781,  1.0234,  2.4375,  0.1982]], dtype=torch.bfloat16))]), logits=tensor([[-3.2812, -1.0781,  1.0234,  2.4375,  0.1982]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.1562,  1.4141,  1.8281,  0.2715, -2.4219]], dtype=torch.bfloat16))]), logits=tensor([[-1.1562,  1.4141,  1.8281,  0.2715, -2.4219]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.2031,  1.4062,  0.6797, -0.5938, -1.5781]], dtype=torch.bfloat16))]), logits=tensor([[ 0.2031,  1.4062,  0.6797, -0.5938, -1.5781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5625, -1.6172,  1.0625,  2.9844,  0.5586]], dtype=torch.bfloat16))]), logits=tensor([[-3.5625, -1.6172,  1.0625,  2.9844,  0.5586]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 0.7578,  1.7969,  0.8086, -0.9297, -2.2812]], dtype=torch.bfloat16))]), logits=tensor([[ 0.7578,  1.7969,  0.8086, -0.9297, -2.2812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5156, -3.1875, -1.8984,  2.8438,  4.6562]], dtype=torch.bfloat16))]), logits=tensor([[-3.5156, -3.1875, -1.8984,  2.8438,  4.6562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7812, -0.0289,  2.1562,  1.5156, -1.3672]], dtype=torch.bfloat16))]), logits=tensor([[-2.7812, -0.0289,  2.1562,  1.5156, -1.3672]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.0781,  2.5625,  0.5898, -2.1719, -3.0625]], dtype=torch.bfloat16))]), logits=tensor([[ 3.0781,  2.5625,  0.5898, -2.1719, -3.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.5312,  0.8828, -0.5430, -1.5781, -1.2891]], dtype=torch.bfloat16))]), logits=tensor([[ 3.5312,  0.8828, -0.5430, -1.5781, -1.2891]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -1.8438, -0.1865,  2.2656,  2.0938]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -1.8438, -0.1865,  2.2656,  2.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.6719,  2.2812,  0.0537, -2.2656, -2.6406]], dtype=torch.bfloat16))]), logits=tensor([[ 3.6719,  2.2812,  0.0537, -2.2656, -2.6406]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5469, -3.0625, -1.6250,  3.0625,  4.0938]], dtype=torch.bfloat16))]), logits=tensor([[-3.5469, -3.0625, -1.6250,  3.0625,  4.0938]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3438, -3.2031, -2.1719,  2.8438,  4.8750]], dtype=torch.bfloat16))]), logits=tensor([[-3.3438, -3.2031, -2.1719,  2.8438,  4.8750]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 1.8047,  2.2656,  1.0234, -1.4922, -2.9375]], dtype=torch.bfloat16))]), logits=tensor([[ 1.8047,  2.2656,  1.0234, -1.4922, -2.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8125, -0.1387,  1.9297,  1.7109, -1.2500]], dtype=torch.bfloat16))]), logits=tensor([[-2.8125, -0.1387,  1.9297,  1.7109, -1.2500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.8516,  0.8711,  0.5859,  0.0400, -0.9414]], dtype=torch.bfloat16))]), logits=tensor([[-0.8516,  0.8711,  0.5859,  0.0400, -0.9414]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.5469,  1.0078, -0.5391, -1.6016, -1.3047]], dtype=torch.bfloat16))]), logits=tensor([[ 3.5469,  1.0078, -0.5391, -1.6016, -1.3047]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5938, -1.6172,  0.7578,  2.8281,  0.8789]], dtype=torch.bfloat16))]), logits=tensor([[-3.5938, -1.6172,  0.7578,  2.8281,  0.8789]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.8438,  1.9688, -0.1729, -2.1562, -2.3125]], dtype=torch.bfloat16))]), logits=tensor([[ 3.8438,  1.9688, -0.1729, -2.1562, -2.3125]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -2.7969, -1.4375,  2.8594,  3.7344]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -2.7969, -1.4375,  2.8594,  3.7344]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3750, -1.1328,  0.9258,  2.2812,  0.5469]], dtype=torch.bfloat16))]), logits=tensor([[-3.3750, -1.1328,  0.9258,  2.2812,  0.5469]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -2.1562,  0.3047,  3.2500,  1.5781]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -2.1562,  0.3047,  3.2500,  1.5781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.1250,  1.5312,  2.0625,  0.2734, -2.7812]], dtype=torch.bfloat16))]), logits=tensor([[-1.1250,  1.5312,  2.0625,  0.2734, -2.7812]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.3594, -1.3750,  0.9297,  2.6562,  0.5156]], dtype=torch.bfloat16))]), logits=tensor([[-3.3594, -1.3750,  0.9297,  2.6562,  0.5156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8906, -0.4668,  1.4609,  1.8672, -0.6367]], dtype=torch.bfloat16))]), logits=tensor([[-2.8906, -0.4668,  1.4609,  1.8672, -0.6367]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5938, -1.9922,  0.1807,  3.0156,  1.6172]], dtype=torch.bfloat16))]), logits=tensor([[-3.5938, -1.9922,  0.1807,  3.0156,  1.6172]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.4688, -1.8203, -0.2354,  2.5469,  1.9531]], dtype=torch.bfloat16))]), logits=tensor([[-3.4688, -1.8203, -0.2354,  2.5469,  1.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.7656, -2.9844, -2.4688,  2.1875,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.7656, -2.9844, -2.4688,  2.1875,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.9062, -2.9531, -2.3594,  2.2031,  5.1562]], dtype=torch.bfloat16))]), logits=tensor([[-2.9062, -2.9531, -2.3594,  2.2031,  5.1562]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.8242,  0.5039,  0.2871,  0.0194, -0.4980]], dtype=torch.bfloat16))]), logits=tensor([[-0.8242,  0.5039,  0.2871,  0.0194, -0.4980]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8125, -2.9531, -2.3438,  2.1719,  5.1250]], dtype=torch.bfloat16))]), logits=tensor([[-2.8125, -2.9531, -2.3438,  2.1719,  5.1250]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6094, -3.0469, -1.4688,  3.1562,  3.8594]], dtype=torch.bfloat16))]), logits=tensor([[-3.6094, -3.0469, -1.4688,  3.1562,  3.8594]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-0.7305,  1.8438,  2.1250, -0.0284, -3.0156]], dtype=torch.bfloat16))]), logits=tensor([[-0.7305,  1.8438,  2.1250, -0.0284, -3.0156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2344, -1.9297, -0.7070,  2.1406,  2.5781]], dtype=torch.bfloat16))]), logits=tensor([[-3.2344, -1.9297, -0.7070,  2.1406,  2.5781]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-1.4922,  1.4141,  2.0781,  0.3223, -2.5156]], dtype=torch.bfloat16))]), logits=tensor([[-1.4922,  1.4141,  2.0781,  0.3223, -2.5156]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2188, -3.0312, -2.1250,  2.4844,  4.8438]], dtype=torch.bfloat16))]), logits=tensor([[-3.2188, -3.0312, -2.1250,  2.4844,  4.8438]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.6406, -0.0157,  1.9609,  1.5234, -1.3281]], dtype=torch.bfloat16))]), logits=tensor([[-2.6406, -0.0157,  1.9609,  1.5234, -1.3281]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5938, -2.6719, -1.1641,  2.6875,  3.4688]], dtype=torch.bfloat16))]), logits=tensor([[-3.5938, -2.6719, -1.1641,  2.6875,  3.4688]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -3.1250, -1.9531,  2.9375,  4.5625]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -3.1250, -1.9531,  2.9375,  4.5625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.6562, -3.0625, -1.3281,  3.2812,  3.7500]], dtype=torch.bfloat16))]), logits=tensor([[-3.6562, -3.0625, -1.3281,  3.2812,  3.7500]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 4.0938,  1.5625, -0.4199, -2.0469, -1.9531]], dtype=torch.bfloat16))]), logits=tensor([[ 4.0938,  1.5625, -0.4199, -2.0469, -1.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.9375,  1.5547, -0.3320, -2.0156, -1.9531]], dtype=torch.bfloat16))]), logits=tensor([[ 3.9375,  1.5547, -0.3320, -2.0156, -1.9531]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[ 3.0938,  2.4688,  0.4941, -2.1406, -2.9375]], dtype=torch.bfloat16))]), logits=tensor([[ 3.0938,  2.4688,  0.4941, -2.1406, -2.9375]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.0469, -0.3594,  1.7734,  1.9219, -0.9062]], dtype=torch.bfloat16))]), logits=tensor([[-3.0469, -0.3594,  1.7734,  1.9219, -0.9062]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.5000, -2.8906, -1.6875,  2.7812,  4.0625]], dtype=torch.bfloat16))]), logits=tensor([[-3.5000, -2.8906, -1.6875,  2.7812,  4.0625]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-3.2812, -0.9336,  1.4375,  2.4375, -0.2266]], dtype=torch.bfloat16))]), logits=tensor([[-3.2812, -0.9336,  1.4375,  2.4375, -0.2266]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " SequenceClassifierOutput(loss=OrderedDict([('logits', tensor([[-2.8594, -3.0156, -2.4062,  2.2344,  5.1875]], dtype=torch.bfloat16))]), logits=tensor([[-2.8594, -3.0156, -2.4062,  2.2344,  5.1875]], dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def predict_step(batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any:\n",
    "    with torch.no_grad():\n",
    "        input_ids, input_mask = batch\n",
    "        return model.model(\n",
    "            input_ids, token_type_ids=None, attention_mask=input_mask\n",
    "        )\n",
    "model.predict_step = predict_step\n",
    "trainer.callbacks = [RichProgressBar()]\n",
    "test_logits = trainer.predict(model, datamodule=test_dm)\n",
    "test_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4\n",
       "1       1\n",
       "2       3\n",
       "3       4\n",
       "4       4\n",
       "       ..\n",
       "4094    0\n",
       "4095    4\n",
       "4096    4\n",
       "4097    3\n",
       "4098    0\n",
       "Length: 4099, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df = pd.Series([torch.argmax(out.logits).item() for out in test_logits])\n",
    "preds_df.to_csv('piatek_owienko_schafer.csv', header=None, index=None)\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssne_p3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
