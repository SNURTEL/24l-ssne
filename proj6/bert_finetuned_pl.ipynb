{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from typing import Literal, Callable\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    "    random_split,\n",
    ")\n",
    "from torch.optim import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassConfusionMatrix,\n",
    ")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_t = str | bytes | os.PathLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 123\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK_DIR = Path(\"./nltk\")\n",
    "MODEL_DIR = Path(\"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tomek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\", download_dir=NLTK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.types import TRAIN_DATALOADERS\n",
    "\n",
    "\n",
    "class TextClassificationDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Path_t,\n",
    "        tokenizer: BertTokenizer,\n",
    "        batch_size: int,\n",
    "        val_fraction: float = 0.0,\n",
    "        truncation_strategy: Literal[\"truncate\"] = \"truncate\",\n",
    "        max_seq_len: int = 512,\n",
    "        clean_fn: Callable[[str], str] = lambda x: x,\n",
    "        n_first: int | None = None  # just for testing purposes\n",
    "    ):\n",
    "        super(TextClassificationDataModule, self).__init__()\n",
    "        assert 0 <= val_fraction <= 1\n",
    "\n",
    "        self._data_path = data_path\n",
    "        self._clean_fn = clean_fn\n",
    "        self._tokenizer = tokenizer\n",
    "        self._encode_fn = partial(\n",
    "            tokenizer.encode_plus,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_seq_len,\n",
    "            truncation=True if truncation_strategy == \"truncate\" else False,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        self.val_fraction = val_fraction\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self._n_first = n_first\n",
    "\n",
    "        self._encoded: list[dict[str, Tensor]]\n",
    "        self._train: Dataset\n",
    "        self._val: Dataset\n",
    "        self._test: Dataset\n",
    "\n",
    "    def setup(self, stage: Literal[\"fit\", \"test\"]) -> None:\n",
    "        if stage == \"fit\":\n",
    "            df_raw = pd.read_csv(self._data_path)\n",
    "            ratings = df_raw[\"rating\"].values[:self._n_first]\n",
    "            dataset = TensorDataset(\n",
    "                *(\n",
    "                    torch.cat([item[k] for item in self._encoded], dim=0)\n",
    "                    for k in (\"input_ids\", \"attention_mask\")\n",
    "                ),\n",
    "                torch.tensor(ratings),\n",
    "            )\n",
    "            val_size = int(len(dataset) * self.val_fraction)\n",
    "            train_size = len(dataset) - val_size\n",
    "\n",
    "            self._train, self._val = torch.utils.data.random_split(\n",
    "                dataset, [train_size, val_size]\n",
    "            )\n",
    "\n",
    "        if stage == \"test\":\n",
    "            self._test = TensorDataset(\n",
    "                *(\n",
    "                    torch.cat([item[k] for item in self._encoded], dim=0)\n",
    "                    for k in (\"input_ids\", \"attention_mask\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        df_raw = pd.read_csv(self._data_path)\n",
    "        reviews = df_raw[\"review\"].apply(self._clean_fn).values[:self._n_first]\n",
    "        self._encoded = [self._encode_fn(review) for review in reviews]\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self._train, batch_size=self.batch_size, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self._val, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self._test, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class BertClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BertForSequenceClassification,\n",
    "        lr: float,\n",
    "        weight_decay: float = 0.0,\n",
    "        warmup_steps: int = 0,\n",
    "        freeze_encoder: bool = False,\n",
    "    ):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        num_classes = model.num_labels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.model = model\n",
    "        self._lr = lr\n",
    "        self._weight_decay = weight_decay\n",
    "        self._warmup_steps = warmup_steps\n",
    "        self._freeze_encoder = freeze_encoder\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for param in self.model.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "        self.flat_metrics = {\n",
    "            \"accuracy\": MulticlassAccuracy(num_classes=num_classes, average=\"micro\").to(device),\n",
    "            \"precision\": MulticlassPrecision(num_classes=num_classes, average=\"micro\").to(device),\n",
    "            \"recall\": MulticlassRecall(num_classes=num_classes, average=\"micro\").to(device),\n",
    "            \"f1\": MulticlassF1Score(num_classes=num_classes, average=\"micro\").to(device),\n",
    "        }\n",
    "        self.class_metrics = {\n",
    "            \"accuracy\": MulticlassAccuracy(num_classes=num_classes, average=\"none\").to(device),\n",
    "            \"precision\": MulticlassPrecision(num_classes=num_classes, average=\"none\").to(device),\n",
    "            \"recall\": MulticlassRecall(num_classes=num_classes, average=\"none\").to(device),\n",
    "            \"f1\": MulticlassF1Score(num_classes=num_classes, average=\"none\").to(device),\n",
    "        }\n",
    "        self.confusion_matrix = MulticlassConfusionMatrix(\n",
    "            num_classes=num_classes, normalize=\"pred\"\n",
    "        ).to(device)\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        self.trainer.fit_loop.setup_data()\n",
    "        total_devices = getattr(self.hparams, \"n_gpus\", 1) * getattr(self.hparams, \"n_nodes\", 1)\n",
    "        train_batches = len(self.trainer.train_dataloader) // total_devices\n",
    "        train_steps = (\n",
    "            self.trainer.max_epochs * train_batches\n",
    "        ) // self.trainer.accumulate_grad_batches\n",
    "        # will not work with AdamW (plus it is deprecated anyway)\n",
    "        # https://discuss.huggingface.co/t/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-grad-fn/47965/2\n",
    "        optimizer = Adam(self.model.parameters(), lr=self._lr, eps=1e-8)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=self._warmup_steps,\n",
    "            num_training_steps=train_steps,\n",
    "        )\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> dict[str, Tensor]:\n",
    "        input_ids, input_mask, labels = batch\n",
    "        output = self.model(\n",
    "            input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels\n",
    "        )\n",
    "        loss = output.loss\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> dict[str, Tensor]:\n",
    "        with torch.no_grad():\n",
    "            input_ids, input_mask, labels = batch\n",
    "            output = self.model(\n",
    "                input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels\n",
    "            )\n",
    "            loss = output.loss\n",
    "\n",
    "            preds = output.logits\n",
    "            self.log(\"val_loss\", loss, prog_bar=True)\n",
    "            self.log_dict(\n",
    "                {name: metric(preds, labels) for name, metric in self.flat_metrics.items()},\n",
    "                prog_bar=True,\n",
    "            )\n",
    "            self.log_dict(\n",
    "                {\n",
    "                    f\"{name}_{c}\": v\n",
    "                    for name, metric_values in {\n",
    "                        _name: metric(preds, labels)\n",
    "                        for _name, metric in self.class_metrics.items()\n",
    "                    }.items()\n",
    "                    for c, v in enumerate(metric_values)\n",
    "                },\n",
    "                prog_bar=True,\n",
    "            )\n",
    "            self.confusion_matrix(preds, labels)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        with torch.no_grad():\n",
    "            confusion_matrix = self.confusion_matrix.compute()\n",
    "\n",
    "            df_cm = pd.DataFrame(\n",
    "                confusion_matrix.cpu().numpy(), index=range(5), columns=range(5)\n",
    "            )\n",
    "            fig, ax = plt.subplots(figsize=(10, 7))\n",
    "            sns.heatmap(\n",
    "                df_cm,\n",
    "                ax=ax,\n",
    "                annot=True,\n",
    "                cmap=\"Greens\",\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                annot_kws={\"size\": 6},\n",
    "                fmt=\".3f\",\n",
    "                \n",
    "            )\n",
    "            ax.set_xlabel(\"True class\")\n",
    "            ax.set_ylabel(\"Predicted class\")\n",
    "            self.logger.experiment.add_figure(\"Confusion matrix\", fig, self.current_epoch)\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any:\n",
    "        with torch.no_grad():\n",
    "            input_ids, input_mask, labels = batch\n",
    "            return self.model(\n",
    "                input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fdd91234054f4587562311d2de0340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5687850bb6f84dacb38224a2891c48ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870d2c22f38f42dbb51cbd31649f1af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f5cf87bb204bcd9e9f6ca7c45a9e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True, cache_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ./nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords', download_dir=\"./nltk\")\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TextClassificationDataModule(\n",
    "    data_path=Path(\"data/train_data.csv\"),\n",
    "    tokenizer=tokenizer,\n",
    "    val_fraction=0.2,\n",
    "    batch_size=32,\n",
    "    clean_fn=clean_text\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (model): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  )\n",
       "  (confusion_matrix): MulticlassConfusionMatrix()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertClassifier(\n",
    "    model=BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=5,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        cache_dir=MODEL_DIR\n",
    "    ),\n",
    "    lr=2e-5,\n",
    "    weight_decay=0,\n",
    "    warmup_steps=0,\n",
    "    freeze_encoder=False,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/lightning_fabric/connector.py:555: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/tomek/ssne/proj6/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model            │ BertForSequenceClassification │  109 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ confusion_matrix │ MulticlassConfusionMatrix     │      0 │\n",
       "└───┴──────────────────┴───────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model            │ BertForSequenceClassification │  109 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ confusion_matrix │ MulticlassConfusionMatrix     │      0 │\n",
       "└───┴──────────────────┴───────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 109 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 109 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 437                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 109 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 109 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 437                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d0b888e4264262b78232dd70a06ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connecto\n",
       "r.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a \n",
       "bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on \n",
       "this machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connecto\n",
       "r.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a \n",
       "bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on \n",
       "this machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "20 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "20 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "5 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/miniconda3/envs/ssne_p3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: \n",
       "10 NaN values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=4,\n",
    "    gradient_clip_val=1,\n",
    "    callbacks=[RichProgressBar()],\n",
    "    precision=\"bf16\",\n",
    "    val_check_interval=0.2\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssne_p3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
